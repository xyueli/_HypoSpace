import torch
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model
from pathlib import Path

# ==============================
# âš™ï¸ åŸºç¡€é…ç½®
# ==============================
model_name = "/opt/data/private/Qwen3-4B-Instruct-2507"
output_dir = Path(__file__).parent / "lora_output_qwen3_balanced"  # è¾“å‡ºåˆ°å½“å‰ç›®å½•

# åˆ†å±‚é‡‡æ ·é…ç½®
max_samples_per_node = 100000

# ==============================
# ğŸš€ 1. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
# ==============================
print("ğŸ”¹ Loading model and tokenizer ...")
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# ==============================
# ğŸ’¡ 2. é…ç½® LoRA
# ==============================
print("ğŸ”¹ Setting up LoRA configuration ...")
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ==============================
# ğŸ“š 3. åŠ è½½å¹¶å¹³è¡¡æ•°æ®é›†
# ==============================
print("ğŸ”¹ Loading and balancing dataset ...")

# ä»å½“å‰ç›®å½•åŠ è½½JSONLæ–‡ä»¶
base_dir = Path(__file__).parent
dataset_03 = load_dataset("json", data_files=str(base_dir / "causal_training_node03.jsonl"), split="train")
dataset_04 = load_dataset("json", data_files=str(base_dir / "causal_training_node04.jsonl"), split="train") 
dataset_05 = load_dataset("json", data_files=str(base_dir / "causal_training_node05.jsonl"), split="train")

print(f"ğŸ“Š åŸå§‹æ•°æ®é‡ç»Ÿè®¡:")
print(f"   - Node 03: {len(dataset_03):,} æ ·æœ¬")
print(f"   - Node 04: {len(dataset_04):,} æ ·æœ¬") 
print(f"   - Node 05: {len(dataset_05):,} æ ·æœ¬")

# å¹³è¡¡é‡‡æ ·
dataset_03_balanced = dataset_03
dataset_04_balanced = dataset_04
dataset_05_balanced = dataset_05.shuffle(seed=42).select(
    range(min(len(dataset_05), max_samples_per_node))
)

print(f"ğŸ“Š å¤„ç†åæ•°æ®é‡ç»Ÿè®¡:")
print(f"   - Node 03: {len(dataset_03_balanced):,} æ ·æœ¬")
print(f"   - Node 04: {len(dataset_04_balanced):,} æ ·æœ¬")
print(f"   - Node 05: {len(dataset_05_balanced):,} æ ·æœ¬")

# åˆå¹¶æ•°æ®é›†
dataset = concatenate_datasets([dataset_03_balanced, dataset_04_balanced, dataset_05_balanced])
dataset = dataset.shuffle(seed=42)

print(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒæ•°æ®é›†: {len(dataset):,} ä¸ªæ ·æœ¬")

# ==============================
# ğŸ’¬ 4. æ ¼å¼åŒ–èŠå¤©æ•°æ®
# ==============================
def format_chat(example):
    messages = example["messages"]
    formatted = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"].strip()
        formatted += f"<|im_start|>{role}\n{content}<|im_end|>\n"
    formatted += "<|im_start|>assistant\n"
    return {"text": formatted}

print("ğŸ”¹ Formatting chat data ...")
dataset = dataset.map(format_chat)

# ==============================
# âœ‚ï¸ 5. åˆ†è¯
# ==============================
def tokenize(example):
    return tokenizer(example["text"], truncation=True, max_length=1024)

print("ğŸ”¹ Tokenizing data ...")
tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)

# ==============================
# ğŸ§  6. è®­ç»ƒé…ç½®
# ==============================
print("ğŸ”¹ Preparing training arguments ...")

total_samples = len(tokenized)
total_steps = total_samples // (2 * 2) * 2
warmup_steps = max(100, int(0.05 * total_steps))

args = TrainingArguments(
    output_dir=str(output_dir),
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    num_train_epochs=2,
    learning_rate=1e-4,
    fp16=False,
    bf16=True,
    logging_steps=10, 
    save_steps=500,
    save_total_limit=2,
    warmup_steps=warmup_steps,
    lr_scheduler_type="cosine",
    report_to="none",
    dataloader_pin_memory=True,
)

print(f"ğŸ“ˆ è®­ç»ƒå‚æ•°:")
print(f"   - æ€»æ ·æœ¬æ•°: {total_samples:,}")
print(f"   - æ€»æ­¥æ•°: ~{total_steps}")

# ==============================
# âš™ï¸ 7. Trainer
# ==============================
print("ğŸ”¹ Starting training ...")
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

trainer.train()

# ==============================
# ğŸ’¾ 8. ä¿å­˜ç»“æœ
# ==============================
print("âœ… Saving model ...")
model.save_pretrained(str(output_dir))
tokenizer.save_pretrained(str(output_dir))

print(f"ğŸ‰ LoRA å¾®è°ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")