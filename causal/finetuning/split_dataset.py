import json
import random
from pathlib import Path

def split_dataset(input_path, train_ratio=0.6, seed=42):
    """å°†å› æœæ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
    
    with open(input_path, 'r') as f:
        data = json.load(f)
    
    # è·å–æ‰€æœ‰è§‚æµ‹é›†
    if 'datasets_by_n_observations' in data:
        all_observation_sets = []
        for n_obs, datasets in data['datasets_by_n_observations'].items():
            all_observation_sets.extend(datasets)
    else:
        all_observation_sets = data.get('datasets', [])
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(all_observation_sets)} ä¸ªè§‚æµ‹é›†")
    
    # éšæœºåˆ’åˆ†
    random.seed(seed)
    random.shuffle(all_observation_sets)
    split_idx = int(len(all_observation_sets) * train_ratio)
    
    train_sets = all_observation_sets[:split_idx]
    test_sets = all_observation_sets[split_idx:]
    
    # æ„å»ºè®­ç»ƒé›†æ•°æ®
    train_data = data.copy()
    if 'datasets_by_n_observations' in train_data:
        train_data['datasets_by_n_observations'] = {}
        for obs_set in train_sets:
            n_obs = obs_set['n_observations']
            if n_obs not in train_data['datasets_by_n_observations']:
                train_data['datasets_by_n_observations'][n_obs] = []
            train_data['datasets_by_n_observations'][n_obs].append(obs_set)
    else:
        train_data['datasets'] = train_sets
    
    # æ„å»ºæµ‹è¯•é›†æ•°æ®
    test_data = data.copy()
    if 'datasets_by_n_observations' in test_data:
        test_data['datasets_by_n_observations'] = {}
        for obs_set in test_sets:
            n_obs = obs_set['n_observations']
            if n_obs not in test_data['datasets_by_n_observations']:
                test_data['datasets_by_n_observations'][n_obs] = []
            test_data['datasets_by_n_observations'][n_obs].append(obs_set)
    else:
        test_data['datasets'] = test_sets
    
    # ä¿å­˜æ–‡ä»¶
    input_path = Path(input_path)
    train_output = input_path.parent / f"{input_path.stem}_train.json"
    test_output = input_path.parent / f"{input_path.stem}_test.json"
    
    with open(train_output, 'w') as f:
        json.dump(train_data, f, indent=2)
    with open(test_output, 'w') as f:
        json.dump(test_data, f, indent=2)
    
    print(f"âœ… åˆ’åˆ†å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {len(train_sets)} ä¸ªè§‚æµ‹é›† -> {train_output}")
    print(f"   - æµ‹è¯•é›†: {len(test_sets)} ä¸ªè§‚æµ‹é›† -> {test_output}")
    
    return train_output, test_output

def main():
    """åˆ’åˆ†æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®é›†"""
    # ä» finetuning ç›®å½•å‘ä¸Šåˆ° causalï¼Œç„¶åè¿›å…¥ datasets
    base_dir = Path(__file__).parent.parent / "datasets"
    
    nodes = ["node03", "node04", "node05"]
    
    for node in nodes:
        node_dir = base_dir / node
        json_files = list(node_dir.glob("*.json"))
        
        if not json_files:
            print(f"âš ï¸  åœ¨ {node_dir} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
            continue
            
        input_file = json_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªJSONæ–‡ä»¶
        print(f"\nğŸ”¹ å¤„ç† {node}: {input_file.name}")
        split_dataset(input_file)

if __name__ == "__main__":
    main()