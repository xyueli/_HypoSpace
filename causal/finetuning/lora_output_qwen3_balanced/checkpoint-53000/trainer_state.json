{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9856880596455735,
  "eval_steps": 500,
  "global_step": 53000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00037465812446142895,
      "grad_norm": 2.5399281978607178,
      "learning_rate": 3.3720494567253656e-07,
      "loss": 2.8235,
      "step": 10
    },
    {
      "epoch": 0.0007493162489228579,
      "grad_norm": 2.9085640907287598,
      "learning_rate": 7.118771075309105e-07,
      "loss": 2.7485,
      "step": 20
    },
    {
      "epoch": 0.0011239743733842868,
      "grad_norm": 2.711121082305908,
      "learning_rate": 1.0865492693892845e-06,
      "loss": 2.8072,
      "step": 30
    },
    {
      "epoch": 0.0014986324978457158,
      "grad_norm": 3.0986227989196777,
      "learning_rate": 1.4612214312476585e-06,
      "loss": 2.8669,
      "step": 40
    },
    {
      "epoch": 0.0018732906223071447,
      "grad_norm": 2.9494004249572754,
      "learning_rate": 1.8358935931060322e-06,
      "loss": 2.7398,
      "step": 50
    },
    {
      "epoch": 0.0022479487467685737,
      "grad_norm": 2.834542989730835,
      "learning_rate": 2.210565754964406e-06,
      "loss": 2.8286,
      "step": 60
    },
    {
      "epoch": 0.0026226068712300026,
      "grad_norm": 3.526921510696411,
      "learning_rate": 2.5852379168227804e-06,
      "loss": 2.9065,
      "step": 70
    },
    {
      "epoch": 0.0029972649956914316,
      "grad_norm": 3.8825504779815674,
      "learning_rate": 2.959910078681154e-06,
      "loss": 2.9179,
      "step": 80
    },
    {
      "epoch": 0.0033719231201528605,
      "grad_norm": 3.299936532974243,
      "learning_rate": 3.334582240539528e-06,
      "loss": 2.8843,
      "step": 90
    },
    {
      "epoch": 0.0037465812446142895,
      "grad_norm": 3.1286885738372803,
      "learning_rate": 3.709254402397902e-06,
      "loss": 2.7682,
      "step": 100
    },
    {
      "epoch": 0.004121239369075718,
      "grad_norm": 3.2790770530700684,
      "learning_rate": 4.083926564256276e-06,
      "loss": 2.637,
      "step": 110
    },
    {
      "epoch": 0.004495897493537147,
      "grad_norm": 3.401933431625366,
      "learning_rate": 4.45859872611465e-06,
      "loss": 2.5941,
      "step": 120
    },
    {
      "epoch": 0.004870555617998576,
      "grad_norm": 3.167865753173828,
      "learning_rate": 4.833270887973024e-06,
      "loss": 2.5324,
      "step": 130
    },
    {
      "epoch": 0.005245213742460005,
      "grad_norm": 4.282097339630127,
      "learning_rate": 5.207943049831398e-06,
      "loss": 2.5992,
      "step": 140
    },
    {
      "epoch": 0.005619871866921434,
      "grad_norm": 3.4049630165100098,
      "learning_rate": 5.582615211689772e-06,
      "loss": 2.3131,
      "step": 150
    },
    {
      "epoch": 0.005994529991382863,
      "grad_norm": 3.0753962993621826,
      "learning_rate": 5.957287373548146e-06,
      "loss": 2.2467,
      "step": 160
    },
    {
      "epoch": 0.006369188115844292,
      "grad_norm": 3.036046266555786,
      "learning_rate": 6.331959535406519e-06,
      "loss": 2.1817,
      "step": 170
    },
    {
      "epoch": 0.006743846240305721,
      "grad_norm": 2.544590473175049,
      "learning_rate": 6.706631697264894e-06,
      "loss": 2.0528,
      "step": 180
    },
    {
      "epoch": 0.00711850436476715,
      "grad_norm": 2.494204044342041,
      "learning_rate": 7.081303859123267e-06,
      "loss": 1.9455,
      "step": 190
    },
    {
      "epoch": 0.007493162489228579,
      "grad_norm": 2.1339335441589355,
      "learning_rate": 7.455976020981642e-06,
      "loss": 1.7602,
      "step": 200
    },
    {
      "epoch": 0.007867820613690008,
      "grad_norm": 2.009384870529175,
      "learning_rate": 7.830648182840016e-06,
      "loss": 1.7372,
      "step": 210
    },
    {
      "epoch": 0.008242478738151437,
      "grad_norm": 1.6670546531677246,
      "learning_rate": 8.205320344698388e-06,
      "loss": 1.4695,
      "step": 220
    },
    {
      "epoch": 0.008617136862612866,
      "grad_norm": 1.5467519760131836,
      "learning_rate": 8.579992506556764e-06,
      "loss": 1.454,
      "step": 230
    },
    {
      "epoch": 0.008991794987074295,
      "grad_norm": 1.5021395683288574,
      "learning_rate": 8.954664668415138e-06,
      "loss": 1.3378,
      "step": 240
    },
    {
      "epoch": 0.009366453111535724,
      "grad_norm": 1.6151777505874634,
      "learning_rate": 9.32933683027351e-06,
      "loss": 1.1743,
      "step": 250
    },
    {
      "epoch": 0.009741111235997153,
      "grad_norm": 1.5433719158172607,
      "learning_rate": 9.704008992131886e-06,
      "loss": 1.0777,
      "step": 260
    },
    {
      "epoch": 0.010115769360458582,
      "grad_norm": 1.757885217666626,
      "learning_rate": 1.007868115399026e-05,
      "loss": 0.9363,
      "step": 270
    },
    {
      "epoch": 0.01049042748492001,
      "grad_norm": 1.8126174211502075,
      "learning_rate": 1.0453353315848632e-05,
      "loss": 0.8047,
      "step": 280
    },
    {
      "epoch": 0.01086508560938144,
      "grad_norm": 2.3129656314849854,
      "learning_rate": 1.0828025477707008e-05,
      "loss": 0.6974,
      "step": 290
    },
    {
      "epoch": 0.011239743733842868,
      "grad_norm": 1.689144253730774,
      "learning_rate": 1.120269763956538e-05,
      "loss": 0.5613,
      "step": 300
    },
    {
      "epoch": 0.011614401858304297,
      "grad_norm": 1.2086524963378906,
      "learning_rate": 1.1577369801423754e-05,
      "loss": 0.4155,
      "step": 310
    },
    {
      "epoch": 0.011989059982765726,
      "grad_norm": 0.6790031790733337,
      "learning_rate": 1.195204196328213e-05,
      "loss": 0.3505,
      "step": 320
    },
    {
      "epoch": 0.012363718107227155,
      "grad_norm": 0.6157109141349792,
      "learning_rate": 1.2326714125140502e-05,
      "loss": 0.2983,
      "step": 330
    },
    {
      "epoch": 0.012738376231688584,
      "grad_norm": 0.8505420684814453,
      "learning_rate": 1.2701386286998876e-05,
      "loss": 0.2679,
      "step": 340
    },
    {
      "epoch": 0.013113034356150013,
      "grad_norm": 0.8564262390136719,
      "learning_rate": 1.307605844885725e-05,
      "loss": 0.2369,
      "step": 350
    },
    {
      "epoch": 0.013487692480611442,
      "grad_norm": 0.4318109154701233,
      "learning_rate": 1.3450730610715626e-05,
      "loss": 0.2215,
      "step": 360
    },
    {
      "epoch": 0.013862350605072871,
      "grad_norm": 0.4516194462776184,
      "learning_rate": 1.3825402772573998e-05,
      "loss": 0.2069,
      "step": 370
    },
    {
      "epoch": 0.0142370087295343,
      "grad_norm": 0.5488504767417908,
      "learning_rate": 1.4200074934432372e-05,
      "loss": 0.2081,
      "step": 380
    },
    {
      "epoch": 0.014611666853995729,
      "grad_norm": 0.49234288930892944,
      "learning_rate": 1.4574747096290748e-05,
      "loss": 0.1996,
      "step": 390
    },
    {
      "epoch": 0.014986324978457158,
      "grad_norm": 0.4049762189388275,
      "learning_rate": 1.494941925814912e-05,
      "loss": 0.1856,
      "step": 400
    },
    {
      "epoch": 0.015360983102918587,
      "grad_norm": 0.906822681427002,
      "learning_rate": 1.5324091420007494e-05,
      "loss": 0.1836,
      "step": 410
    },
    {
      "epoch": 0.015735641227380016,
      "grad_norm": 0.5286746025085449,
      "learning_rate": 1.569876358186587e-05,
      "loss": 0.1668,
      "step": 420
    },
    {
      "epoch": 0.016110299351841446,
      "grad_norm": 0.8866917490959167,
      "learning_rate": 1.6073435743724242e-05,
      "loss": 0.1607,
      "step": 430
    },
    {
      "epoch": 0.016484957476302874,
      "grad_norm": 0.4916723370552063,
      "learning_rate": 1.6448107905582618e-05,
      "loss": 0.1433,
      "step": 440
    },
    {
      "epoch": 0.016859615600764304,
      "grad_norm": 0.3888227939605713,
      "learning_rate": 1.682278006744099e-05,
      "loss": 0.1453,
      "step": 450
    },
    {
      "epoch": 0.01723427372522573,
      "grad_norm": 0.4133501946926117,
      "learning_rate": 1.7197452229299362e-05,
      "loss": 0.1393,
      "step": 460
    },
    {
      "epoch": 0.017608931849687162,
      "grad_norm": 0.5256198644638062,
      "learning_rate": 1.7572124391157738e-05,
      "loss": 0.139,
      "step": 470
    },
    {
      "epoch": 0.01798358997414859,
      "grad_norm": 0.3383985161781311,
      "learning_rate": 1.7946796553016113e-05,
      "loss": 0.1408,
      "step": 480
    },
    {
      "epoch": 0.01835824809861002,
      "grad_norm": 0.6364378929138184,
      "learning_rate": 1.8321468714874486e-05,
      "loss": 0.1346,
      "step": 490
    },
    {
      "epoch": 0.018732906223071447,
      "grad_norm": 0.524473249912262,
      "learning_rate": 1.8696140876732858e-05,
      "loss": 0.1352,
      "step": 500
    },
    {
      "epoch": 0.019107564347532878,
      "grad_norm": 0.9111581444740295,
      "learning_rate": 1.9070813038591234e-05,
      "loss": 0.1324,
      "step": 510
    },
    {
      "epoch": 0.019482222471994305,
      "grad_norm": 0.455995112657547,
      "learning_rate": 1.9445485200449606e-05,
      "loss": 0.1297,
      "step": 520
    },
    {
      "epoch": 0.019856880596455736,
      "grad_norm": 0.5032837390899658,
      "learning_rate": 1.9820157362307982e-05,
      "loss": 0.1341,
      "step": 530
    },
    {
      "epoch": 0.020231538720917163,
      "grad_norm": 0.4372953176498413,
      "learning_rate": 2.0194829524166357e-05,
      "loss": 0.1324,
      "step": 540
    },
    {
      "epoch": 0.020606196845378594,
      "grad_norm": 0.5050563812255859,
      "learning_rate": 2.056950168602473e-05,
      "loss": 0.1328,
      "step": 550
    },
    {
      "epoch": 0.02098085496984002,
      "grad_norm": 0.5903928279876709,
      "learning_rate": 2.0944173847883102e-05,
      "loss": 0.131,
      "step": 560
    },
    {
      "epoch": 0.02135551309430145,
      "grad_norm": 0.5245802998542786,
      "learning_rate": 2.1318846009741478e-05,
      "loss": 0.1352,
      "step": 570
    },
    {
      "epoch": 0.02173017121876288,
      "grad_norm": 0.4531751573085785,
      "learning_rate": 2.1693518171599853e-05,
      "loss": 0.1235,
      "step": 580
    },
    {
      "epoch": 0.02210482934322431,
      "grad_norm": 0.6106098294258118,
      "learning_rate": 2.2068190333458226e-05,
      "loss": 0.1297,
      "step": 590
    },
    {
      "epoch": 0.022479487467685737,
      "grad_norm": 0.5247213244438171,
      "learning_rate": 2.2442862495316598e-05,
      "loss": 0.1272,
      "step": 600
    },
    {
      "epoch": 0.022854145592147167,
      "grad_norm": 0.6103293895721436,
      "learning_rate": 2.2817534657174974e-05,
      "loss": 0.1317,
      "step": 610
    },
    {
      "epoch": 0.023228803716608595,
      "grad_norm": 0.5272287726402283,
      "learning_rate": 2.3192206819033346e-05,
      "loss": 0.1285,
      "step": 620
    },
    {
      "epoch": 0.023603461841070025,
      "grad_norm": 0.6170745491981506,
      "learning_rate": 2.356687898089172e-05,
      "loss": 0.1266,
      "step": 630
    },
    {
      "epoch": 0.023978119965531453,
      "grad_norm": 0.4063468277454376,
      "learning_rate": 2.3941551142750097e-05,
      "loss": 0.1297,
      "step": 640
    },
    {
      "epoch": 0.024352778089992883,
      "grad_norm": 0.46480420231819153,
      "learning_rate": 2.431622330460847e-05,
      "loss": 0.1279,
      "step": 650
    },
    {
      "epoch": 0.02472743621445431,
      "grad_norm": 0.49615901708602905,
      "learning_rate": 2.4690895466466842e-05,
      "loss": 0.1282,
      "step": 660
    },
    {
      "epoch": 0.02510209433891574,
      "grad_norm": 0.5959656238555908,
      "learning_rate": 2.5065567628325214e-05,
      "loss": 0.1261,
      "step": 670
    },
    {
      "epoch": 0.02547675246337717,
      "grad_norm": 0.6283664107322693,
      "learning_rate": 2.544023979018359e-05,
      "loss": 0.1231,
      "step": 680
    },
    {
      "epoch": 0.0258514105878386,
      "grad_norm": 0.7185509204864502,
      "learning_rate": 2.5814911952041966e-05,
      "loss": 0.1209,
      "step": 690
    },
    {
      "epoch": 0.026226068712300026,
      "grad_norm": 0.6122183799743652,
      "learning_rate": 2.618958411390034e-05,
      "loss": 0.1252,
      "step": 700
    },
    {
      "epoch": 0.026600726836761457,
      "grad_norm": 0.5793728232383728,
      "learning_rate": 2.6564256275758713e-05,
      "loss": 0.1228,
      "step": 710
    },
    {
      "epoch": 0.026975384961222884,
      "grad_norm": 0.47074899077415466,
      "learning_rate": 2.6938928437617082e-05,
      "loss": 0.1203,
      "step": 720
    },
    {
      "epoch": 0.027350043085684315,
      "grad_norm": 0.5338026285171509,
      "learning_rate": 2.7313600599475458e-05,
      "loss": 0.1204,
      "step": 730
    },
    {
      "epoch": 0.027724701210145742,
      "grad_norm": 0.5857835412025452,
      "learning_rate": 2.7688272761333834e-05,
      "loss": 0.1258,
      "step": 740
    },
    {
      "epoch": 0.028099359334607173,
      "grad_norm": 0.4613674581050873,
      "learning_rate": 2.806294492319221e-05,
      "loss": 0.1236,
      "step": 750
    },
    {
      "epoch": 0.0284740174590686,
      "grad_norm": 0.502277672290802,
      "learning_rate": 2.8437617085050582e-05,
      "loss": 0.1183,
      "step": 760
    },
    {
      "epoch": 0.02884867558353003,
      "grad_norm": 0.5054189562797546,
      "learning_rate": 2.8812289246908957e-05,
      "loss": 0.1181,
      "step": 770
    },
    {
      "epoch": 0.029223333707991458,
      "grad_norm": 0.4957868158817291,
      "learning_rate": 2.9186961408767333e-05,
      "loss": 0.1172,
      "step": 780
    },
    {
      "epoch": 0.02959799183245289,
      "grad_norm": 1.0293748378753662,
      "learning_rate": 2.9561633570625702e-05,
      "loss": 0.1204,
      "step": 790
    },
    {
      "epoch": 0.029972649956914316,
      "grad_norm": 0.6265914440155029,
      "learning_rate": 2.9936305732484078e-05,
      "loss": 0.1219,
      "step": 800
    },
    {
      "epoch": 0.030347308081375746,
      "grad_norm": 0.49287712574005127,
      "learning_rate": 3.031097789434245e-05,
      "loss": 0.1152,
      "step": 810
    },
    {
      "epoch": 0.030721966205837174,
      "grad_norm": 0.5988606214523315,
      "learning_rate": 3.0685650056200826e-05,
      "loss": 0.1218,
      "step": 820
    },
    {
      "epoch": 0.031096624330298604,
      "grad_norm": 0.5358026027679443,
      "learning_rate": 3.10603222180592e-05,
      "loss": 0.1149,
      "step": 830
    },
    {
      "epoch": 0.03147128245476003,
      "grad_norm": 0.5632745623588562,
      "learning_rate": 3.143499437991758e-05,
      "loss": 0.1177,
      "step": 840
    },
    {
      "epoch": 0.03184594057922146,
      "grad_norm": 0.6138742566108704,
      "learning_rate": 3.180966654177594e-05,
      "loss": 0.1143,
      "step": 850
    },
    {
      "epoch": 0.03222059870368289,
      "grad_norm": 0.5595046281814575,
      "learning_rate": 3.218433870363432e-05,
      "loss": 0.1147,
      "step": 860
    },
    {
      "epoch": 0.03259525682814432,
      "grad_norm": 0.4809715747833252,
      "learning_rate": 3.2559010865492694e-05,
      "loss": 0.1125,
      "step": 870
    },
    {
      "epoch": 0.03296991495260575,
      "grad_norm": 0.5406731367111206,
      "learning_rate": 3.293368302735107e-05,
      "loss": 0.1134,
      "step": 880
    },
    {
      "epoch": 0.033344573077067174,
      "grad_norm": 0.48198801279067993,
      "learning_rate": 3.3308355189209445e-05,
      "loss": 0.1121,
      "step": 890
    },
    {
      "epoch": 0.03371923120152861,
      "grad_norm": 0.5611675977706909,
      "learning_rate": 3.368302735106782e-05,
      "loss": 0.1164,
      "step": 900
    },
    {
      "epoch": 0.034093889325990036,
      "grad_norm": 0.4120733141899109,
      "learning_rate": 3.405769951292619e-05,
      "loss": 0.1108,
      "step": 910
    },
    {
      "epoch": 0.03446854745045146,
      "grad_norm": 0.558828592300415,
      "learning_rate": 3.443237167478456e-05,
      "loss": 0.1184,
      "step": 920
    },
    {
      "epoch": 0.03484320557491289,
      "grad_norm": 0.5387363433837891,
      "learning_rate": 3.480704383664294e-05,
      "loss": 0.1124,
      "step": 930
    },
    {
      "epoch": 0.035217863699374324,
      "grad_norm": 0.6203526854515076,
      "learning_rate": 3.5181715998501313e-05,
      "loss": 0.1081,
      "step": 940
    },
    {
      "epoch": 0.03559252182383575,
      "grad_norm": 0.4666767120361328,
      "learning_rate": 3.5556388160359686e-05,
      "loss": 0.1153,
      "step": 950
    },
    {
      "epoch": 0.03596717994829718,
      "grad_norm": 0.5262033343315125,
      "learning_rate": 3.5931060322218065e-05,
      "loss": 0.1117,
      "step": 960
    },
    {
      "epoch": 0.036341838072758606,
      "grad_norm": 0.5392175912857056,
      "learning_rate": 3.630573248407643e-05,
      "loss": 0.1055,
      "step": 970
    },
    {
      "epoch": 0.03671649619722004,
      "grad_norm": 0.5528380274772644,
      "learning_rate": 3.668040464593481e-05,
      "loss": 0.1092,
      "step": 980
    },
    {
      "epoch": 0.03709115432168147,
      "grad_norm": 0.6814393401145935,
      "learning_rate": 3.705507680779318e-05,
      "loss": 0.1065,
      "step": 990
    },
    {
      "epoch": 0.037465812446142895,
      "grad_norm": 0.6619139313697815,
      "learning_rate": 3.7429748969651554e-05,
      "loss": 0.1124,
      "step": 1000
    },
    {
      "epoch": 0.03784047057060432,
      "grad_norm": 0.7300602793693542,
      "learning_rate": 3.780442113150993e-05,
      "loss": 0.1157,
      "step": 1010
    },
    {
      "epoch": 0.038215128695065756,
      "grad_norm": 0.6347424983978271,
      "learning_rate": 3.8179093293368305e-05,
      "loss": 0.1124,
      "step": 1020
    },
    {
      "epoch": 0.03858978681952718,
      "grad_norm": 0.6833394765853882,
      "learning_rate": 3.855376545522668e-05,
      "loss": 0.1139,
      "step": 1030
    },
    {
      "epoch": 0.03896444494398861,
      "grad_norm": 0.5186034440994263,
      "learning_rate": 3.892843761708505e-05,
      "loss": 0.1053,
      "step": 1040
    },
    {
      "epoch": 0.03933910306845004,
      "grad_norm": 0.5300969481468201,
      "learning_rate": 3.930310977894342e-05,
      "loss": 0.1077,
      "step": 1050
    },
    {
      "epoch": 0.03971376119291147,
      "grad_norm": 0.4700556993484497,
      "learning_rate": 3.96777819408018e-05,
      "loss": 0.1118,
      "step": 1060
    },
    {
      "epoch": 0.0400884193173729,
      "grad_norm": 0.541860044002533,
      "learning_rate": 4.0052454102660174e-05,
      "loss": 0.106,
      "step": 1070
    },
    {
      "epoch": 0.040463077441834326,
      "grad_norm": 0.4798400402069092,
      "learning_rate": 4.042712626451855e-05,
      "loss": 0.1087,
      "step": 1080
    },
    {
      "epoch": 0.04083773556629575,
      "grad_norm": 0.5418282747268677,
      "learning_rate": 4.0801798426376925e-05,
      "loss": 0.1029,
      "step": 1090
    },
    {
      "epoch": 0.04121239369075719,
      "grad_norm": 0.6121187210083008,
      "learning_rate": 4.11764705882353e-05,
      "loss": 0.1068,
      "step": 1100
    },
    {
      "epoch": 0.041587051815218615,
      "grad_norm": 0.6400646567344666,
      "learning_rate": 4.155114275009367e-05,
      "loss": 0.1101,
      "step": 1110
    },
    {
      "epoch": 0.04196170993968004,
      "grad_norm": 0.5752801895141602,
      "learning_rate": 4.192581491195204e-05,
      "loss": 0.1046,
      "step": 1120
    },
    {
      "epoch": 0.04233636806414147,
      "grad_norm": 0.44163745641708374,
      "learning_rate": 4.230048707381042e-05,
      "loss": 0.1088,
      "step": 1130
    },
    {
      "epoch": 0.0427110261886029,
      "grad_norm": 0.5187163352966309,
      "learning_rate": 4.267515923566879e-05,
      "loss": 0.107,
      "step": 1140
    },
    {
      "epoch": 0.04308568431306433,
      "grad_norm": 0.6819753646850586,
      "learning_rate": 4.3049831397527166e-05,
      "loss": 0.1066,
      "step": 1150
    },
    {
      "epoch": 0.04346034243752576,
      "grad_norm": 0.5772718787193298,
      "learning_rate": 4.342450355938554e-05,
      "loss": 0.1068,
      "step": 1160
    },
    {
      "epoch": 0.043835000561987185,
      "grad_norm": 0.5182446241378784,
      "learning_rate": 4.379917572124391e-05,
      "loss": 0.1106,
      "step": 1170
    },
    {
      "epoch": 0.04420965868644862,
      "grad_norm": 0.5033270120620728,
      "learning_rate": 4.417384788310229e-05,
      "loss": 0.1074,
      "step": 1180
    },
    {
      "epoch": 0.044584316810910046,
      "grad_norm": 0.6532837152481079,
      "learning_rate": 4.454852004496066e-05,
      "loss": 0.1141,
      "step": 1190
    },
    {
      "epoch": 0.044958974935371473,
      "grad_norm": 0.5484523773193359,
      "learning_rate": 4.4923192206819034e-05,
      "loss": 0.1086,
      "step": 1200
    },
    {
      "epoch": 0.0453336330598329,
      "grad_norm": 0.5059536099433899,
      "learning_rate": 4.529786436867741e-05,
      "loss": 0.1021,
      "step": 1210
    },
    {
      "epoch": 0.045708291184294335,
      "grad_norm": 0.6154226660728455,
      "learning_rate": 4.567253653053578e-05,
      "loss": 0.1065,
      "step": 1220
    },
    {
      "epoch": 0.04608294930875576,
      "grad_norm": 0.6750279068946838,
      "learning_rate": 4.604720869239416e-05,
      "loss": 0.1057,
      "step": 1230
    },
    {
      "epoch": 0.04645760743321719,
      "grad_norm": 0.44952768087387085,
      "learning_rate": 4.642188085425253e-05,
      "loss": 0.106,
      "step": 1240
    },
    {
      "epoch": 0.046832265557678616,
      "grad_norm": 0.4982306957244873,
      "learning_rate": 4.679655301611091e-05,
      "loss": 0.1079,
      "step": 1250
    },
    {
      "epoch": 0.04720692368214005,
      "grad_norm": 0.5516546964645386,
      "learning_rate": 4.717122517796928e-05,
      "loss": 0.1023,
      "step": 1260
    },
    {
      "epoch": 0.04758158180660148,
      "grad_norm": 0.5218448042869568,
      "learning_rate": 4.754589733982765e-05,
      "loss": 0.1007,
      "step": 1270
    },
    {
      "epoch": 0.047956239931062905,
      "grad_norm": 0.4992845058441162,
      "learning_rate": 4.7920569501686026e-05,
      "loss": 0.1019,
      "step": 1280
    },
    {
      "epoch": 0.04833089805552433,
      "grad_norm": 0.4458443522453308,
      "learning_rate": 4.82952416635444e-05,
      "loss": 0.1025,
      "step": 1290
    },
    {
      "epoch": 0.048705556179985766,
      "grad_norm": 0.43037083745002747,
      "learning_rate": 4.866991382540278e-05,
      "loss": 0.1045,
      "step": 1300
    },
    {
      "epoch": 0.049080214304447194,
      "grad_norm": 0.5430207252502441,
      "learning_rate": 4.904458598726115e-05,
      "loss": 0.1014,
      "step": 1310
    },
    {
      "epoch": 0.04945487242890862,
      "grad_norm": 0.4154379069805145,
      "learning_rate": 4.941925814911952e-05,
      "loss": 0.1005,
      "step": 1320
    },
    {
      "epoch": 0.04982953055337005,
      "grad_norm": 0.5767652988433838,
      "learning_rate": 4.97939303109779e-05,
      "loss": 0.1038,
      "step": 1330
    },
    {
      "epoch": 0.05020418867783148,
      "grad_norm": 0.5685900449752808,
      "learning_rate": 5.0168602472836266e-05,
      "loss": 0.1021,
      "step": 1340
    },
    {
      "epoch": 0.05057884680229291,
      "grad_norm": 0.4894859492778778,
      "learning_rate": 5.0543274634694645e-05,
      "loss": 0.1036,
      "step": 1350
    },
    {
      "epoch": 0.05095350492675434,
      "grad_norm": 0.7887095808982849,
      "learning_rate": 5.091794679655302e-05,
      "loss": 0.1026,
      "step": 1360
    },
    {
      "epoch": 0.051328163051215764,
      "grad_norm": 0.623320996761322,
      "learning_rate": 5.129261895841139e-05,
      "loss": 0.1075,
      "step": 1370
    },
    {
      "epoch": 0.0517028211756772,
      "grad_norm": 0.41404989361763,
      "learning_rate": 5.166729112026977e-05,
      "loss": 0.1036,
      "step": 1380
    },
    {
      "epoch": 0.052077479300138625,
      "grad_norm": 0.5470202565193176,
      "learning_rate": 5.2041963282128134e-05,
      "loss": 0.1047,
      "step": 1390
    },
    {
      "epoch": 0.05245213742460005,
      "grad_norm": 0.4603148102760315,
      "learning_rate": 5.241663544398652e-05,
      "loss": 0.1041,
      "step": 1400
    },
    {
      "epoch": 0.05282679554906148,
      "grad_norm": 0.45203524827957153,
      "learning_rate": 5.2791307605844886e-05,
      "loss": 0.1029,
      "step": 1410
    },
    {
      "epoch": 0.053201453673522914,
      "grad_norm": 0.4203925132751465,
      "learning_rate": 5.3165979767703265e-05,
      "loss": 0.0975,
      "step": 1420
    },
    {
      "epoch": 0.05357611179798434,
      "grad_norm": 0.5360749959945679,
      "learning_rate": 5.354065192956164e-05,
      "loss": 0.1048,
      "step": 1430
    },
    {
      "epoch": 0.05395076992244577,
      "grad_norm": 0.5800510048866272,
      "learning_rate": 5.391532409142e-05,
      "loss": 0.1099,
      "step": 1440
    },
    {
      "epoch": 0.054325428046907195,
      "grad_norm": 0.39194542169570923,
      "learning_rate": 5.428999625327839e-05,
      "loss": 0.1105,
      "step": 1450
    },
    {
      "epoch": 0.05470008617136863,
      "grad_norm": 0.49329936504364014,
      "learning_rate": 5.4664668415136754e-05,
      "loss": 0.1091,
      "step": 1460
    },
    {
      "epoch": 0.05507474429583006,
      "grad_norm": 0.4628669321537018,
      "learning_rate": 5.503934057699513e-05,
      "loss": 0.1012,
      "step": 1470
    },
    {
      "epoch": 0.055449402420291484,
      "grad_norm": 0.8474286794662476,
      "learning_rate": 5.5414012738853505e-05,
      "loss": 0.1,
      "step": 1480
    },
    {
      "epoch": 0.05582406054475291,
      "grad_norm": 0.4358626902103424,
      "learning_rate": 5.5788684900711884e-05,
      "loss": 0.099,
      "step": 1490
    },
    {
      "epoch": 0.056198718669214345,
      "grad_norm": 0.5060158371925354,
      "learning_rate": 5.616335706257026e-05,
      "loss": 0.1036,
      "step": 1500
    },
    {
      "epoch": 0.05657337679367577,
      "grad_norm": 0.3694692552089691,
      "learning_rate": 5.653802922442862e-05,
      "loss": 0.0964,
      "step": 1510
    },
    {
      "epoch": 0.0569480349181372,
      "grad_norm": 0.6226645112037659,
      "learning_rate": 5.6912701386287e-05,
      "loss": 0.1024,
      "step": 1520
    },
    {
      "epoch": 0.05732269304259863,
      "grad_norm": 0.5447995066642761,
      "learning_rate": 5.7287373548145374e-05,
      "loss": 0.1042,
      "step": 1530
    },
    {
      "epoch": 0.05769735116706006,
      "grad_norm": 0.3976435959339142,
      "learning_rate": 5.766204571000375e-05,
      "loss": 0.1008,
      "step": 1540
    },
    {
      "epoch": 0.05807200929152149,
      "grad_norm": 0.4239519238471985,
      "learning_rate": 5.8036717871862125e-05,
      "loss": 0.0994,
      "step": 1550
    },
    {
      "epoch": 0.058446667415982916,
      "grad_norm": 0.5805388689041138,
      "learning_rate": 5.841139003372049e-05,
      "loss": 0.106,
      "step": 1560
    },
    {
      "epoch": 0.05882132554044434,
      "grad_norm": 0.5715479850769043,
      "learning_rate": 5.878606219557887e-05,
      "loss": 0.1008,
      "step": 1570
    },
    {
      "epoch": 0.05919598366490578,
      "grad_norm": 0.45648831129074097,
      "learning_rate": 5.916073435743724e-05,
      "loss": 0.0962,
      "step": 1580
    },
    {
      "epoch": 0.059570641789367204,
      "grad_norm": 0.4877936542034149,
      "learning_rate": 5.953540651929562e-05,
      "loss": 0.1038,
      "step": 1590
    },
    {
      "epoch": 0.05994529991382863,
      "grad_norm": 0.4765101671218872,
      "learning_rate": 5.991007868115399e-05,
      "loss": 0.0981,
      "step": 1600
    },
    {
      "epoch": 0.06031995803829006,
      "grad_norm": 0.5164681077003479,
      "learning_rate": 6.028475084301237e-05,
      "loss": 0.1021,
      "step": 1610
    },
    {
      "epoch": 0.06069461616275149,
      "grad_norm": 0.45372918248176575,
      "learning_rate": 6.0659423004870745e-05,
      "loss": 0.0977,
      "step": 1620
    },
    {
      "epoch": 0.06106927428721292,
      "grad_norm": 0.7267744541168213,
      "learning_rate": 6.103409516672911e-05,
      "loss": 0.1002,
      "step": 1630
    },
    {
      "epoch": 0.06144393241167435,
      "grad_norm": 0.4249965250492096,
      "learning_rate": 6.140876732858749e-05,
      "loss": 0.1019,
      "step": 1640
    },
    {
      "epoch": 0.061818590536135774,
      "grad_norm": 0.504743218421936,
      "learning_rate": 6.178343949044585e-05,
      "loss": 0.1037,
      "step": 1650
    },
    {
      "epoch": 0.06219324866059721,
      "grad_norm": 0.5337876677513123,
      "learning_rate": 6.215811165230425e-05,
      "loss": 0.1012,
      "step": 1660
    },
    {
      "epoch": 0.06256790678505864,
      "grad_norm": 0.6010631322860718,
      "learning_rate": 6.253278381416261e-05,
      "loss": 0.0993,
      "step": 1670
    },
    {
      "epoch": 0.06294256490952006,
      "grad_norm": 0.44510164856910706,
      "learning_rate": 6.290745597602098e-05,
      "loss": 0.1004,
      "step": 1680
    },
    {
      "epoch": 0.06331722303398149,
      "grad_norm": 0.4018622636795044,
      "learning_rate": 6.328212813787936e-05,
      "loss": 0.1019,
      "step": 1690
    },
    {
      "epoch": 0.06369188115844292,
      "grad_norm": 0.4208158254623413,
      "learning_rate": 6.365680029973772e-05,
      "loss": 0.0995,
      "step": 1700
    },
    {
      "epoch": 0.06406653928290434,
      "grad_norm": 0.5147934556007385,
      "learning_rate": 6.403147246159612e-05,
      "loss": 0.0999,
      "step": 1710
    },
    {
      "epoch": 0.06444119740736579,
      "grad_norm": 0.4176494777202606,
      "learning_rate": 6.440614462345448e-05,
      "loss": 0.1013,
      "step": 1720
    },
    {
      "epoch": 0.06481585553182721,
      "grad_norm": 0.5126901268959045,
      "learning_rate": 6.478081678531286e-05,
      "loss": 0.1036,
      "step": 1730
    },
    {
      "epoch": 0.06519051365628864,
      "grad_norm": 0.29501667618751526,
      "learning_rate": 6.515548894717123e-05,
      "loss": 0.1018,
      "step": 1740
    },
    {
      "epoch": 0.06556517178075007,
      "grad_norm": 0.4670970141887665,
      "learning_rate": 6.553016110902959e-05,
      "loss": 0.0969,
      "step": 1750
    },
    {
      "epoch": 0.0659398299052115,
      "grad_norm": 0.6290436387062073,
      "learning_rate": 6.590483327088798e-05,
      "loss": 0.1016,
      "step": 1760
    },
    {
      "epoch": 0.06631448802967292,
      "grad_norm": 0.8153673410415649,
      "learning_rate": 6.627950543274635e-05,
      "loss": 0.1045,
      "step": 1770
    },
    {
      "epoch": 0.06668914615413435,
      "grad_norm": 0.4754602611064911,
      "learning_rate": 6.665417759460473e-05,
      "loss": 0.0972,
      "step": 1780
    },
    {
      "epoch": 0.06706380427859578,
      "grad_norm": 0.3270218074321747,
      "learning_rate": 6.70288497564631e-05,
      "loss": 0.0978,
      "step": 1790
    },
    {
      "epoch": 0.06743846240305722,
      "grad_norm": 0.39873194694519043,
      "learning_rate": 6.740352191832147e-05,
      "loss": 0.1028,
      "step": 1800
    },
    {
      "epoch": 0.06781312052751864,
      "grad_norm": 0.4356020987033844,
      "learning_rate": 6.777819408017985e-05,
      "loss": 0.1003,
      "step": 1810
    },
    {
      "epoch": 0.06818777865198007,
      "grad_norm": 0.43105071783065796,
      "learning_rate": 6.815286624203822e-05,
      "loss": 0.0972,
      "step": 1820
    },
    {
      "epoch": 0.0685624367764415,
      "grad_norm": 0.649172842502594,
      "learning_rate": 6.85275384038966e-05,
      "loss": 0.1022,
      "step": 1830
    },
    {
      "epoch": 0.06893709490090293,
      "grad_norm": 0.43867117166519165,
      "learning_rate": 6.890221056575496e-05,
      "loss": 0.0997,
      "step": 1840
    },
    {
      "epoch": 0.06931175302536435,
      "grad_norm": 0.6065885424613953,
      "learning_rate": 6.927688272761334e-05,
      "loss": 0.1024,
      "step": 1850
    },
    {
      "epoch": 0.06968641114982578,
      "grad_norm": 0.42607319355010986,
      "learning_rate": 6.965155488947172e-05,
      "loss": 0.0952,
      "step": 1860
    },
    {
      "epoch": 0.07006106927428721,
      "grad_norm": 0.4849097430706024,
      "learning_rate": 7.002622705133009e-05,
      "loss": 0.0972,
      "step": 1870
    },
    {
      "epoch": 0.07043572739874865,
      "grad_norm": 0.42156529426574707,
      "learning_rate": 7.040089921318846e-05,
      "loss": 0.0974,
      "step": 1880
    },
    {
      "epoch": 0.07081038552321008,
      "grad_norm": 0.39304226636886597,
      "learning_rate": 7.077557137504683e-05,
      "loss": 0.1001,
      "step": 1890
    },
    {
      "epoch": 0.0711850436476715,
      "grad_norm": 0.5315263867378235,
      "learning_rate": 7.115024353690521e-05,
      "loss": 0.0928,
      "step": 1900
    },
    {
      "epoch": 0.07155970177213293,
      "grad_norm": 0.4297293424606323,
      "learning_rate": 7.152491569876359e-05,
      "loss": 0.0969,
      "step": 1910
    },
    {
      "epoch": 0.07193435989659436,
      "grad_norm": 0.456508070230484,
      "learning_rate": 7.189958786062197e-05,
      "loss": 0.0983,
      "step": 1920
    },
    {
      "epoch": 0.07230901802105578,
      "grad_norm": 0.4350598156452179,
      "learning_rate": 7.227426002248033e-05,
      "loss": 0.0979,
      "step": 1930
    },
    {
      "epoch": 0.07268367614551721,
      "grad_norm": 0.3605014383792877,
      "learning_rate": 7.26489321843387e-05,
      "loss": 0.0982,
      "step": 1940
    },
    {
      "epoch": 0.07305833426997864,
      "grad_norm": 0.6053057312965393,
      "learning_rate": 7.302360434619708e-05,
      "loss": 0.0984,
      "step": 1950
    },
    {
      "epoch": 0.07343299239444008,
      "grad_norm": 0.3522905707359314,
      "learning_rate": 7.339827650805546e-05,
      "loss": 0.0947,
      "step": 1960
    },
    {
      "epoch": 0.07380765051890151,
      "grad_norm": 0.4164623022079468,
      "learning_rate": 7.377294866991384e-05,
      "loss": 0.0957,
      "step": 1970
    },
    {
      "epoch": 0.07418230864336293,
      "grad_norm": 0.524608314037323,
      "learning_rate": 7.41476208317722e-05,
      "loss": 0.0983,
      "step": 1980
    },
    {
      "epoch": 0.07455696676782436,
      "grad_norm": 0.4141996204853058,
      "learning_rate": 7.452229299363057e-05,
      "loss": 0.0986,
      "step": 1990
    },
    {
      "epoch": 0.07493162489228579,
      "grad_norm": 0.49057647585868835,
      "learning_rate": 7.489696515548895e-05,
      "loss": 0.0956,
      "step": 2000
    },
    {
      "epoch": 0.07530628301674722,
      "grad_norm": 0.35582810640335083,
      "learning_rate": 7.527163731734733e-05,
      "loss": 0.0975,
      "step": 2010
    },
    {
      "epoch": 0.07568094114120864,
      "grad_norm": 0.3753702938556671,
      "learning_rate": 7.56463094792057e-05,
      "loss": 0.0976,
      "step": 2020
    },
    {
      "epoch": 0.07605559926567007,
      "grad_norm": 0.35277074575424194,
      "learning_rate": 7.602098164106407e-05,
      "loss": 0.099,
      "step": 2030
    },
    {
      "epoch": 0.07643025739013151,
      "grad_norm": 0.3736534118652344,
      "learning_rate": 7.639565380292245e-05,
      "loss": 0.1003,
      "step": 2040
    },
    {
      "epoch": 0.07680491551459294,
      "grad_norm": 0.5058566927909851,
      "learning_rate": 7.677032596478081e-05,
      "loss": 0.1012,
      "step": 2050
    },
    {
      "epoch": 0.07717957363905437,
      "grad_norm": 0.3415970504283905,
      "learning_rate": 7.71449981266392e-05,
      "loss": 0.0977,
      "step": 2060
    },
    {
      "epoch": 0.0775542317635158,
      "grad_norm": 0.6010521054267883,
      "learning_rate": 7.751967028849757e-05,
      "loss": 0.0969,
      "step": 2070
    },
    {
      "epoch": 0.07792888988797722,
      "grad_norm": 0.34673643112182617,
      "learning_rate": 7.789434245035594e-05,
      "loss": 0.0939,
      "step": 2080
    },
    {
      "epoch": 0.07830354801243865,
      "grad_norm": 0.3473828434944153,
      "learning_rate": 7.826901461221432e-05,
      "loss": 0.0985,
      "step": 2090
    },
    {
      "epoch": 0.07867820613690008,
      "grad_norm": 0.5194674134254456,
      "learning_rate": 7.864368677407268e-05,
      "loss": 0.1041,
      "step": 2100
    },
    {
      "epoch": 0.0790528642613615,
      "grad_norm": 0.3601902425289154,
      "learning_rate": 7.901835893593108e-05,
      "loss": 0.099,
      "step": 2110
    },
    {
      "epoch": 0.07942752238582294,
      "grad_norm": 0.47015154361724854,
      "learning_rate": 7.939303109778944e-05,
      "loss": 0.1004,
      "step": 2120
    },
    {
      "epoch": 0.07980218051028437,
      "grad_norm": 0.337278813123703,
      "learning_rate": 7.97677032596478e-05,
      "loss": 0.0978,
      "step": 2130
    },
    {
      "epoch": 0.0801768386347458,
      "grad_norm": 0.4047807455062866,
      "learning_rate": 8.014237542150619e-05,
      "loss": 0.0994,
      "step": 2140
    },
    {
      "epoch": 0.08055149675920723,
      "grad_norm": 0.2842023968696594,
      "learning_rate": 8.051704758336456e-05,
      "loss": 0.0969,
      "step": 2150
    },
    {
      "epoch": 0.08092615488366865,
      "grad_norm": 0.3847450315952301,
      "learning_rate": 8.089171974522294e-05,
      "loss": 0.0987,
      "step": 2160
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 0.38159722089767456,
      "learning_rate": 8.126639190708131e-05,
      "loss": 0.0932,
      "step": 2170
    },
    {
      "epoch": 0.0816754711325915,
      "grad_norm": 0.3318145275115967,
      "learning_rate": 8.164106406893967e-05,
      "loss": 0.0982,
      "step": 2180
    },
    {
      "epoch": 0.08205012925705293,
      "grad_norm": 0.3838846683502197,
      "learning_rate": 8.201573623079805e-05,
      "loss": 0.0939,
      "step": 2190
    },
    {
      "epoch": 0.08242478738151438,
      "grad_norm": 0.42264148592948914,
      "learning_rate": 8.239040839265643e-05,
      "loss": 0.0938,
      "step": 2200
    },
    {
      "epoch": 0.0827994455059758,
      "grad_norm": 0.36476001143455505,
      "learning_rate": 8.276508055451481e-05,
      "loss": 0.0915,
      "step": 2210
    },
    {
      "epoch": 0.08317410363043723,
      "grad_norm": 0.24886305630207062,
      "learning_rate": 8.313975271637318e-05,
      "loss": 0.0996,
      "step": 2220
    },
    {
      "epoch": 0.08354876175489866,
      "grad_norm": 0.4124017357826233,
      "learning_rate": 8.351442487823156e-05,
      "loss": 0.0984,
      "step": 2230
    },
    {
      "epoch": 0.08392341987936008,
      "grad_norm": 0.31412801146507263,
      "learning_rate": 8.388909704008992e-05,
      "loss": 0.0966,
      "step": 2240
    },
    {
      "epoch": 0.08429807800382151,
      "grad_norm": 0.32578325271606445,
      "learning_rate": 8.42637692019483e-05,
      "loss": 0.103,
      "step": 2250
    },
    {
      "epoch": 0.08467273612828294,
      "grad_norm": 0.6227522492408752,
      "learning_rate": 8.463844136380668e-05,
      "loss": 0.1011,
      "step": 2260
    },
    {
      "epoch": 0.08504739425274437,
      "grad_norm": 0.33022159337997437,
      "learning_rate": 8.501311352566505e-05,
      "loss": 0.0967,
      "step": 2270
    },
    {
      "epoch": 0.0854220523772058,
      "grad_norm": 0.31347981095314026,
      "learning_rate": 8.538778568752342e-05,
      "loss": 0.0939,
      "step": 2280
    },
    {
      "epoch": 0.08579671050166723,
      "grad_norm": 0.3572128415107727,
      "learning_rate": 8.576245784938179e-05,
      "loss": 0.0972,
      "step": 2290
    },
    {
      "epoch": 0.08617136862612866,
      "grad_norm": 0.41664594411849976,
      "learning_rate": 8.613713001124017e-05,
      "loss": 0.1006,
      "step": 2300
    },
    {
      "epoch": 0.08654602675059009,
      "grad_norm": 0.2385951280593872,
      "learning_rate": 8.651180217309855e-05,
      "loss": 0.1028,
      "step": 2310
    },
    {
      "epoch": 0.08692068487505152,
      "grad_norm": 0.5450841188430786,
      "learning_rate": 8.688647433495691e-05,
      "loss": 0.0987,
      "step": 2320
    },
    {
      "epoch": 0.08729534299951294,
      "grad_norm": 0.3119923174381256,
      "learning_rate": 8.726114649681529e-05,
      "loss": 0.1015,
      "step": 2330
    },
    {
      "epoch": 0.08767000112397437,
      "grad_norm": 0.348233699798584,
      "learning_rate": 8.763581865867366e-05,
      "loss": 0.0963,
      "step": 2340
    },
    {
      "epoch": 0.0880446592484358,
      "grad_norm": 0.46695461869239807,
      "learning_rate": 8.801049082053204e-05,
      "loss": 0.0996,
      "step": 2350
    },
    {
      "epoch": 0.08841931737289724,
      "grad_norm": 0.3203594386577606,
      "learning_rate": 8.838516298239042e-05,
      "loss": 0.0975,
      "step": 2360
    },
    {
      "epoch": 0.08879397549735867,
      "grad_norm": 0.4215434491634369,
      "learning_rate": 8.875983514424878e-05,
      "loss": 0.0993,
      "step": 2370
    },
    {
      "epoch": 0.08916863362182009,
      "grad_norm": 0.30988284945487976,
      "learning_rate": 8.913450730610716e-05,
      "loss": 0.0966,
      "step": 2380
    },
    {
      "epoch": 0.08954329174628152,
      "grad_norm": 0.36293381452560425,
      "learning_rate": 8.950917946796553e-05,
      "loss": 0.0962,
      "step": 2390
    },
    {
      "epoch": 0.08991794987074295,
      "grad_norm": 0.5710891485214233,
      "learning_rate": 8.98838516298239e-05,
      "loss": 0.0978,
      "step": 2400
    },
    {
      "epoch": 0.09029260799520437,
      "grad_norm": 0.28789058327674866,
      "learning_rate": 9.025852379168228e-05,
      "loss": 0.1012,
      "step": 2410
    },
    {
      "epoch": 0.0906672661196658,
      "grad_norm": 0.2834298312664032,
      "learning_rate": 9.063319595354066e-05,
      "loss": 0.097,
      "step": 2420
    },
    {
      "epoch": 0.09104192424412723,
      "grad_norm": 0.2698240578174591,
      "learning_rate": 9.100786811539903e-05,
      "loss": 0.1002,
      "step": 2430
    },
    {
      "epoch": 0.09141658236858867,
      "grad_norm": 0.3063575327396393,
      "learning_rate": 9.13825402772574e-05,
      "loss": 0.0946,
      "step": 2440
    },
    {
      "epoch": 0.0917912404930501,
      "grad_norm": 0.3712059557437897,
      "learning_rate": 9.175721243911579e-05,
      "loss": 0.0975,
      "step": 2450
    },
    {
      "epoch": 0.09216589861751152,
      "grad_norm": 0.35214105248451233,
      "learning_rate": 9.213188460097415e-05,
      "loss": 0.0978,
      "step": 2460
    },
    {
      "epoch": 0.09254055674197295,
      "grad_norm": 0.30043935775756836,
      "learning_rate": 9.250655676283253e-05,
      "loss": 0.0944,
      "step": 2470
    },
    {
      "epoch": 0.09291521486643438,
      "grad_norm": 0.31111276149749756,
      "learning_rate": 9.28812289246909e-05,
      "loss": 0.0979,
      "step": 2480
    },
    {
      "epoch": 0.0932898729908958,
      "grad_norm": 0.25429767370224,
      "learning_rate": 9.325590108654926e-05,
      "loss": 0.0987,
      "step": 2490
    },
    {
      "epoch": 0.09366453111535723,
      "grad_norm": 0.32392624020576477,
      "learning_rate": 9.363057324840766e-05,
      "loss": 0.0943,
      "step": 2500
    },
    {
      "epoch": 0.09403918923981866,
      "grad_norm": 0.34050366282463074,
      "learning_rate": 9.400524541026602e-05,
      "loss": 0.1034,
      "step": 2510
    },
    {
      "epoch": 0.0944138473642801,
      "grad_norm": 0.35210222005844116,
      "learning_rate": 9.43799175721244e-05,
      "loss": 0.098,
      "step": 2520
    },
    {
      "epoch": 0.09478850548874153,
      "grad_norm": 0.3488994240760803,
      "learning_rate": 9.475458973398277e-05,
      "loss": 0.0931,
      "step": 2530
    },
    {
      "epoch": 0.09516316361320296,
      "grad_norm": 0.33771413564682007,
      "learning_rate": 9.512926189584114e-05,
      "loss": 0.0958,
      "step": 2540
    },
    {
      "epoch": 0.09553782173766438,
      "grad_norm": 0.26579123735427856,
      "learning_rate": 9.550393405769952e-05,
      "loss": 0.0973,
      "step": 2550
    },
    {
      "epoch": 0.09591247986212581,
      "grad_norm": 0.42147040367126465,
      "learning_rate": 9.587860621955789e-05,
      "loss": 0.098,
      "step": 2560
    },
    {
      "epoch": 0.09628713798658724,
      "grad_norm": 0.530762791633606,
      "learning_rate": 9.625327838141627e-05,
      "loss": 0.0975,
      "step": 2570
    },
    {
      "epoch": 0.09666179611104866,
      "grad_norm": 0.33061257004737854,
      "learning_rate": 9.662795054327463e-05,
      "loss": 0.0985,
      "step": 2580
    },
    {
      "epoch": 0.09703645423551009,
      "grad_norm": 0.2500254213809967,
      "learning_rate": 9.700262270513301e-05,
      "loss": 0.0991,
      "step": 2590
    },
    {
      "epoch": 0.09741111235997153,
      "grad_norm": 0.31447750329971313,
      "learning_rate": 9.737729486699139e-05,
      "loss": 0.1015,
      "step": 2600
    },
    {
      "epoch": 0.09778577048443296,
      "grad_norm": 0.28801485896110535,
      "learning_rate": 9.775196702884976e-05,
      "loss": 0.0968,
      "step": 2610
    },
    {
      "epoch": 0.09816042860889439,
      "grad_norm": 0.304465115070343,
      "learning_rate": 9.812663919070814e-05,
      "loss": 0.1008,
      "step": 2620
    },
    {
      "epoch": 0.09853508673335581,
      "grad_norm": 0.34185469150543213,
      "learning_rate": 9.85013113525665e-05,
      "loss": 0.0989,
      "step": 2630
    },
    {
      "epoch": 0.09890974485781724,
      "grad_norm": 0.36728763580322266,
      "learning_rate": 9.887598351442488e-05,
      "loss": 0.0968,
      "step": 2640
    },
    {
      "epoch": 0.09928440298227867,
      "grad_norm": 0.40772491693496704,
      "learning_rate": 9.925065567628326e-05,
      "loss": 0.1021,
      "step": 2650
    },
    {
      "epoch": 0.0996590611067401,
      "grad_norm": 0.2727093994617462,
      "learning_rate": 9.962532783814164e-05,
      "loss": 0.0981,
      "step": 2660
    },
    {
      "epoch": 0.10003371923120152,
      "grad_norm": 0.28590869903564453,
      "learning_rate": 0.0001,
      "loss": 0.0992,
      "step": 2670
    },
    {
      "epoch": 0.10040837735566296,
      "grad_norm": 0.3011067807674408,
      "learning_rate": 9.999999040596861e-05,
      "loss": 0.1005,
      "step": 2680
    },
    {
      "epoch": 0.10078303548012439,
      "grad_norm": 0.27448415756225586,
      "learning_rate": 9.999996162387814e-05,
      "loss": 0.099,
      "step": 2690
    },
    {
      "epoch": 0.10115769360458582,
      "grad_norm": 0.27235835790634155,
      "learning_rate": 9.999991365373962e-05,
      "loss": 0.0984,
      "step": 2700
    },
    {
      "epoch": 0.10153235172904725,
      "grad_norm": 0.47901859879493713,
      "learning_rate": 9.999984649557145e-05,
      "loss": 0.0985,
      "step": 2710
    },
    {
      "epoch": 0.10190700985350867,
      "grad_norm": 0.24537622928619385,
      "learning_rate": 9.999976014939943e-05,
      "loss": 0.0953,
      "step": 2720
    },
    {
      "epoch": 0.1022816679779701,
      "grad_norm": 0.3133217990398407,
      "learning_rate": 9.999965461525667e-05,
      "loss": 0.1016,
      "step": 2730
    },
    {
      "epoch": 0.10265632610243153,
      "grad_norm": 0.2675808370113373,
      "learning_rate": 9.999952989318367e-05,
      "loss": 0.0981,
      "step": 2740
    },
    {
      "epoch": 0.10303098422689295,
      "grad_norm": 0.2895553410053253,
      "learning_rate": 9.999938598322832e-05,
      "loss": 0.0956,
      "step": 2750
    },
    {
      "epoch": 0.1034056423513544,
      "grad_norm": 0.22171324491500854,
      "learning_rate": 9.999922288544583e-05,
      "loss": 0.0993,
      "step": 2760
    },
    {
      "epoch": 0.10378030047581582,
      "grad_norm": 0.30060097575187683,
      "learning_rate": 9.99990405998988e-05,
      "loss": 0.0942,
      "step": 2770
    },
    {
      "epoch": 0.10415495860027725,
      "grad_norm": 0.27011215686798096,
      "learning_rate": 9.999883912665717e-05,
      "loss": 0.0986,
      "step": 2780
    },
    {
      "epoch": 0.10452961672473868,
      "grad_norm": 0.34187957644462585,
      "learning_rate": 9.999861846579826e-05,
      "loss": 0.0977,
      "step": 2790
    },
    {
      "epoch": 0.1049042748492001,
      "grad_norm": 0.38574182987213135,
      "learning_rate": 9.999837861740675e-05,
      "loss": 0.098,
      "step": 2800
    },
    {
      "epoch": 0.10527893297366153,
      "grad_norm": 0.2283720225095749,
      "learning_rate": 9.99981195815747e-05,
      "loss": 0.0952,
      "step": 2810
    },
    {
      "epoch": 0.10565359109812296,
      "grad_norm": 0.34110864996910095,
      "learning_rate": 9.999784135840151e-05,
      "loss": 0.0981,
      "step": 2820
    },
    {
      "epoch": 0.10602824922258439,
      "grad_norm": 0.3678636848926544,
      "learning_rate": 9.999754394799395e-05,
      "loss": 0.0973,
      "step": 2830
    },
    {
      "epoch": 0.10640290734704583,
      "grad_norm": 0.2573375701904297,
      "learning_rate": 9.999722735046616e-05,
      "loss": 0.0987,
      "step": 2840
    },
    {
      "epoch": 0.10677756547150725,
      "grad_norm": 0.2554686665534973,
      "learning_rate": 9.999689156593962e-05,
      "loss": 0.1002,
      "step": 2850
    },
    {
      "epoch": 0.10715222359596868,
      "grad_norm": 0.24972398579120636,
      "learning_rate": 9.999653659454318e-05,
      "loss": 0.0938,
      "step": 2860
    },
    {
      "epoch": 0.10752688172043011,
      "grad_norm": 0.32702165842056274,
      "learning_rate": 9.999616243641311e-05,
      "loss": 0.0964,
      "step": 2870
    },
    {
      "epoch": 0.10790153984489154,
      "grad_norm": 0.3330904245376587,
      "learning_rate": 9.999576909169297e-05,
      "loss": 0.096,
      "step": 2880
    },
    {
      "epoch": 0.10827619796935296,
      "grad_norm": 0.296191930770874,
      "learning_rate": 9.999535656053371e-05,
      "loss": 0.0976,
      "step": 2890
    },
    {
      "epoch": 0.10865085609381439,
      "grad_norm": 0.2971838414669037,
      "learning_rate": 9.999492484309367e-05,
      "loss": 0.095,
      "step": 2900
    },
    {
      "epoch": 0.10902551421827582,
      "grad_norm": 0.3286822438240051,
      "learning_rate": 9.999447393953849e-05,
      "loss": 0.103,
      "step": 2910
    },
    {
      "epoch": 0.10940017234273726,
      "grad_norm": 0.2523081600666046,
      "learning_rate": 9.999400385004124e-05,
      "loss": 0.0956,
      "step": 2920
    },
    {
      "epoch": 0.10977483046719869,
      "grad_norm": 0.3315621316432953,
      "learning_rate": 9.999351457478228e-05,
      "loss": 0.0953,
      "step": 2930
    },
    {
      "epoch": 0.11014948859166011,
      "grad_norm": 0.23905962705612183,
      "learning_rate": 9.99930061139494e-05,
      "loss": 0.0964,
      "step": 2940
    },
    {
      "epoch": 0.11052414671612154,
      "grad_norm": 0.28065410256385803,
      "learning_rate": 9.999247846773773e-05,
      "loss": 0.09,
      "step": 2950
    },
    {
      "epoch": 0.11089880484058297,
      "grad_norm": 0.22375501692295074,
      "learning_rate": 9.999193163634978e-05,
      "loss": 0.0959,
      "step": 2960
    },
    {
      "epoch": 0.1112734629650444,
      "grad_norm": 0.3251866400241852,
      "learning_rate": 9.999136561999536e-05,
      "loss": 0.0969,
      "step": 2970
    },
    {
      "epoch": 0.11164812108950582,
      "grad_norm": 0.2493991106748581,
      "learning_rate": 9.999078041889171e-05,
      "loss": 0.0953,
      "step": 2980
    },
    {
      "epoch": 0.11202277921396725,
      "grad_norm": 0.2461528778076172,
      "learning_rate": 9.999017603326342e-05,
      "loss": 0.0952,
      "step": 2990
    },
    {
      "epoch": 0.11239743733842869,
      "grad_norm": 0.26015937328338623,
      "learning_rate": 9.99895524633424e-05,
      "loss": 0.0928,
      "step": 3000
    },
    {
      "epoch": 0.11277209546289012,
      "grad_norm": 0.2858801782131195,
      "learning_rate": 9.998890970936797e-05,
      "loss": 0.098,
      "step": 3010
    },
    {
      "epoch": 0.11314675358735155,
      "grad_norm": 0.437483012676239,
      "learning_rate": 9.99882477715868e-05,
      "loss": 0.0979,
      "step": 3020
    },
    {
      "epoch": 0.11352141171181297,
      "grad_norm": 0.24918630719184875,
      "learning_rate": 9.998756665025289e-05,
      "loss": 0.0924,
      "step": 3030
    },
    {
      "epoch": 0.1138960698362744,
      "grad_norm": 0.24750803411006927,
      "learning_rate": 9.998686634562764e-05,
      "loss": 0.0963,
      "step": 3040
    },
    {
      "epoch": 0.11427072796073583,
      "grad_norm": 0.27339044213294983,
      "learning_rate": 9.998614685797983e-05,
      "loss": 0.0973,
      "step": 3050
    },
    {
      "epoch": 0.11464538608519725,
      "grad_norm": 0.2551179528236389,
      "learning_rate": 9.998540818758551e-05,
      "loss": 0.1004,
      "step": 3060
    },
    {
      "epoch": 0.11502004420965868,
      "grad_norm": 0.2386505901813507,
      "learning_rate": 9.998465033472822e-05,
      "loss": 0.0997,
      "step": 3070
    },
    {
      "epoch": 0.11539470233412012,
      "grad_norm": 0.35627469420433044,
      "learning_rate": 9.998387329969873e-05,
      "loss": 0.0969,
      "step": 3080
    },
    {
      "epoch": 0.11576936045858155,
      "grad_norm": 0.3024165630340576,
      "learning_rate": 9.998307708279529e-05,
      "loss": 0.0937,
      "step": 3090
    },
    {
      "epoch": 0.11614401858304298,
      "grad_norm": 0.32826289534568787,
      "learning_rate": 9.998226168432343e-05,
      "loss": 0.0962,
      "step": 3100
    },
    {
      "epoch": 0.1165186767075044,
      "grad_norm": 0.2324276715517044,
      "learning_rate": 9.998142710459608e-05,
      "loss": 0.0928,
      "step": 3110
    },
    {
      "epoch": 0.11689333483196583,
      "grad_norm": 0.2805304229259491,
      "learning_rate": 9.998057334393352e-05,
      "loss": 0.0944,
      "step": 3120
    },
    {
      "epoch": 0.11726799295642726,
      "grad_norm": 0.22668518126010895,
      "learning_rate": 9.997970040266336e-05,
      "loss": 0.096,
      "step": 3130
    },
    {
      "epoch": 0.11764265108088869,
      "grad_norm": 0.3128175437450409,
      "learning_rate": 9.997880828112065e-05,
      "loss": 0.0951,
      "step": 3140
    },
    {
      "epoch": 0.11801730920535011,
      "grad_norm": 0.30733323097229004,
      "learning_rate": 9.99778969796477e-05,
      "loss": 0.0976,
      "step": 3150
    },
    {
      "epoch": 0.11839196732981155,
      "grad_norm": 0.2244632989168167,
      "learning_rate": 9.997696649859428e-05,
      "loss": 0.0988,
      "step": 3160
    },
    {
      "epoch": 0.11876662545427298,
      "grad_norm": 0.21705029904842377,
      "learning_rate": 9.997601683831745e-05,
      "loss": 0.0974,
      "step": 3170
    },
    {
      "epoch": 0.11914128357873441,
      "grad_norm": 0.20545898377895355,
      "learning_rate": 9.997504799918165e-05,
      "loss": 0.0949,
      "step": 3180
    },
    {
      "epoch": 0.11951594170319584,
      "grad_norm": 0.20884816348552704,
      "learning_rate": 9.997405998155869e-05,
      "loss": 0.0955,
      "step": 3190
    },
    {
      "epoch": 0.11989059982765726,
      "grad_norm": 0.20882882177829742,
      "learning_rate": 9.997305278582771e-05,
      "loss": 0.097,
      "step": 3200
    },
    {
      "epoch": 0.12026525795211869,
      "grad_norm": 0.2651112973690033,
      "learning_rate": 9.997202641237528e-05,
      "loss": 0.0937,
      "step": 3210
    },
    {
      "epoch": 0.12063991607658012,
      "grad_norm": 0.240952730178833,
      "learning_rate": 9.997098086159524e-05,
      "loss": 0.0912,
      "step": 3220
    },
    {
      "epoch": 0.12101457420104154,
      "grad_norm": 0.2734330892562866,
      "learning_rate": 9.996991613388885e-05,
      "loss": 0.093,
      "step": 3230
    },
    {
      "epoch": 0.12138923232550299,
      "grad_norm": 0.2987852394580841,
      "learning_rate": 9.996883222966471e-05,
      "loss": 0.0946,
      "step": 3240
    },
    {
      "epoch": 0.12176389044996441,
      "grad_norm": 0.25641345977783203,
      "learning_rate": 9.996772914933878e-05,
      "loss": 0.0986,
      "step": 3250
    },
    {
      "epoch": 0.12213854857442584,
      "grad_norm": 0.1910734921693802,
      "learning_rate": 9.996660689333437e-05,
      "loss": 0.0968,
      "step": 3260
    },
    {
      "epoch": 0.12251320669888727,
      "grad_norm": 0.1889266073703766,
      "learning_rate": 9.996546546208219e-05,
      "loss": 0.0956,
      "step": 3270
    },
    {
      "epoch": 0.1228878648233487,
      "grad_norm": 0.2160186767578125,
      "learning_rate": 9.996430485602022e-05,
      "loss": 0.0946,
      "step": 3280
    },
    {
      "epoch": 0.12326252294781012,
      "grad_norm": 0.3249324560165405,
      "learning_rate": 9.996312507559392e-05,
      "loss": 0.097,
      "step": 3290
    },
    {
      "epoch": 0.12363718107227155,
      "grad_norm": 0.21200016140937805,
      "learning_rate": 9.996192612125599e-05,
      "loss": 0.0949,
      "step": 3300
    },
    {
      "epoch": 0.12401183919673298,
      "grad_norm": 0.33620789647102356,
      "learning_rate": 9.996070799346657e-05,
      "loss": 0.0964,
      "step": 3310
    },
    {
      "epoch": 0.12438649732119442,
      "grad_norm": 0.26908621191978455,
      "learning_rate": 9.995947069269314e-05,
      "loss": 0.0954,
      "step": 3320
    },
    {
      "epoch": 0.12476115544565584,
      "grad_norm": 0.259211003780365,
      "learning_rate": 9.99582142194105e-05,
      "loss": 0.0963,
      "step": 3330
    },
    {
      "epoch": 0.12513581357011727,
      "grad_norm": 0.3098854422569275,
      "learning_rate": 9.995693857410085e-05,
      "loss": 0.0993,
      "step": 3340
    },
    {
      "epoch": 0.12551047169457868,
      "grad_norm": 0.25434988737106323,
      "learning_rate": 9.995564375725372e-05,
      "loss": 0.0945,
      "step": 3350
    },
    {
      "epoch": 0.12588512981904013,
      "grad_norm": 0.2197299599647522,
      "learning_rate": 9.995432976936606e-05,
      "loss": 0.0949,
      "step": 3360
    },
    {
      "epoch": 0.12625978794350157,
      "grad_norm": 0.23805902898311615,
      "learning_rate": 9.995299661094207e-05,
      "loss": 0.0989,
      "step": 3370
    },
    {
      "epoch": 0.12663444606796298,
      "grad_norm": 0.21332140266895294,
      "learning_rate": 9.995164428249337e-05,
      "loss": 0.0919,
      "step": 3380
    },
    {
      "epoch": 0.12700910419242442,
      "grad_norm": 0.2365451604127884,
      "learning_rate": 9.995027278453897e-05,
      "loss": 0.0967,
      "step": 3390
    },
    {
      "epoch": 0.12738376231688583,
      "grad_norm": 0.18906249105930328,
      "learning_rate": 9.994888211760516e-05,
      "loss": 0.0983,
      "step": 3400
    },
    {
      "epoch": 0.12775842044134728,
      "grad_norm": 0.28411585092544556,
      "learning_rate": 9.994747228222565e-05,
      "loss": 0.0963,
      "step": 3410
    },
    {
      "epoch": 0.1281330785658087,
      "grad_norm": 0.2907366454601288,
      "learning_rate": 9.994604327894146e-05,
      "loss": 0.094,
      "step": 3420
    },
    {
      "epoch": 0.12850773669027013,
      "grad_norm": 0.25462913513183594,
      "learning_rate": 9.994459510830101e-05,
      "loss": 0.0984,
      "step": 3430
    },
    {
      "epoch": 0.12888239481473157,
      "grad_norm": 0.252985417842865,
      "learning_rate": 9.994312777086002e-05,
      "loss": 0.0954,
      "step": 3440
    },
    {
      "epoch": 0.12925705293919298,
      "grad_norm": 0.20608653128147125,
      "learning_rate": 9.994164126718162e-05,
      "loss": 0.0948,
      "step": 3450
    },
    {
      "epoch": 0.12963171106365443,
      "grad_norm": 0.2601601183414459,
      "learning_rate": 9.994013559783626e-05,
      "loss": 0.0972,
      "step": 3460
    },
    {
      "epoch": 0.13000636918811584,
      "grad_norm": 0.31558749079704285,
      "learning_rate": 9.993861076340177e-05,
      "loss": 0.0915,
      "step": 3470
    },
    {
      "epoch": 0.13038102731257728,
      "grad_norm": 0.2225625067949295,
      "learning_rate": 9.993706676446333e-05,
      "loss": 0.0958,
      "step": 3480
    },
    {
      "epoch": 0.1307556854370387,
      "grad_norm": 0.21597246825695038,
      "learning_rate": 9.993550360161344e-05,
      "loss": 0.0966,
      "step": 3490
    },
    {
      "epoch": 0.13113034356150013,
      "grad_norm": 0.23312437534332275,
      "learning_rate": 9.993392127545198e-05,
      "loss": 0.0972,
      "step": 3500
    },
    {
      "epoch": 0.13150500168596155,
      "grad_norm": 0.20277875661849976,
      "learning_rate": 9.993231978658621e-05,
      "loss": 0.0981,
      "step": 3510
    },
    {
      "epoch": 0.131879659810423,
      "grad_norm": 0.2636268436908722,
      "learning_rate": 9.993069913563072e-05,
      "loss": 0.0954,
      "step": 3520
    },
    {
      "epoch": 0.13225431793488443,
      "grad_norm": 0.1425308883190155,
      "learning_rate": 9.992905932320744e-05,
      "loss": 0.096,
      "step": 3530
    },
    {
      "epoch": 0.13262897605934584,
      "grad_norm": 0.2612065374851227,
      "learning_rate": 9.992740034994568e-05,
      "loss": 0.0971,
      "step": 3540
    },
    {
      "epoch": 0.13300363418380728,
      "grad_norm": 0.36205172538757324,
      "learning_rate": 9.992572221648206e-05,
      "loss": 0.094,
      "step": 3550
    },
    {
      "epoch": 0.1333782923082687,
      "grad_norm": 0.3181149661540985,
      "learning_rate": 9.992402492346061e-05,
      "loss": 0.091,
      "step": 3560
    },
    {
      "epoch": 0.13375295043273014,
      "grad_norm": 0.19723089039325714,
      "learning_rate": 9.992230847153266e-05,
      "loss": 0.0981,
      "step": 3570
    },
    {
      "epoch": 0.13412760855719155,
      "grad_norm": 0.2138357162475586,
      "learning_rate": 9.992057286135695e-05,
      "loss": 0.0942,
      "step": 3580
    },
    {
      "epoch": 0.134502266681653,
      "grad_norm": 0.22544240951538086,
      "learning_rate": 9.991881809359952e-05,
      "loss": 0.0972,
      "step": 3590
    },
    {
      "epoch": 0.13487692480611443,
      "grad_norm": 0.2241850644350052,
      "learning_rate": 9.99170441689338e-05,
      "loss": 0.0942,
      "step": 3600
    },
    {
      "epoch": 0.13525158293057585,
      "grad_norm": 0.2094305455684662,
      "learning_rate": 9.991525108804052e-05,
      "loss": 0.0944,
      "step": 3610
    },
    {
      "epoch": 0.1356262410550373,
      "grad_norm": 0.23740644752979279,
      "learning_rate": 9.991343885160782e-05,
      "loss": 0.0929,
      "step": 3620
    },
    {
      "epoch": 0.1360008991794987,
      "grad_norm": 0.25853049755096436,
      "learning_rate": 9.991160746033117e-05,
      "loss": 0.0971,
      "step": 3630
    },
    {
      "epoch": 0.13637555730396014,
      "grad_norm": 0.23937802016735077,
      "learning_rate": 9.990975691491336e-05,
      "loss": 0.0947,
      "step": 3640
    },
    {
      "epoch": 0.13675021542842156,
      "grad_norm": 0.24867863953113556,
      "learning_rate": 9.990788721606459e-05,
      "loss": 0.0991,
      "step": 3650
    },
    {
      "epoch": 0.137124873552883,
      "grad_norm": 0.181392103433609,
      "learning_rate": 9.990599836450236e-05,
      "loss": 0.0916,
      "step": 3660
    },
    {
      "epoch": 0.1374995316773444,
      "grad_norm": 0.23092125356197357,
      "learning_rate": 9.990409036095155e-05,
      "loss": 0.0945,
      "step": 3670
    },
    {
      "epoch": 0.13787418980180585,
      "grad_norm": 0.23612767457962036,
      "learning_rate": 9.990216320614437e-05,
      "loss": 0.0931,
      "step": 3680
    },
    {
      "epoch": 0.1382488479262673,
      "grad_norm": 0.2815536558628082,
      "learning_rate": 9.990021690082036e-05,
      "loss": 0.0949,
      "step": 3690
    },
    {
      "epoch": 0.1386235060507287,
      "grad_norm": 0.2301383912563324,
      "learning_rate": 9.989825144572649e-05,
      "loss": 0.0956,
      "step": 3700
    },
    {
      "epoch": 0.13899816417519015,
      "grad_norm": 0.4586840569972992,
      "learning_rate": 9.989626684161699e-05,
      "loss": 0.094,
      "step": 3710
    },
    {
      "epoch": 0.13937282229965156,
      "grad_norm": 0.2402627319097519,
      "learning_rate": 9.989426308925348e-05,
      "loss": 0.0914,
      "step": 3720
    },
    {
      "epoch": 0.139747480424113,
      "grad_norm": 0.38582295179367065,
      "learning_rate": 9.989224018940493e-05,
      "loss": 0.0937,
      "step": 3730
    },
    {
      "epoch": 0.14012213854857442,
      "grad_norm": 0.21832618117332458,
      "learning_rate": 9.989019814284765e-05,
      "loss": 0.0948,
      "step": 3740
    },
    {
      "epoch": 0.14049679667303586,
      "grad_norm": 0.1784723401069641,
      "learning_rate": 9.988813695036529e-05,
      "loss": 0.0969,
      "step": 3750
    },
    {
      "epoch": 0.1408714547974973,
      "grad_norm": 0.3391157388687134,
      "learning_rate": 9.988605661274886e-05,
      "loss": 0.0925,
      "step": 3760
    },
    {
      "epoch": 0.1412461129219587,
      "grad_norm": 0.20074333250522614,
      "learning_rate": 9.98839571307967e-05,
      "loss": 0.0967,
      "step": 3770
    },
    {
      "epoch": 0.14162077104642015,
      "grad_norm": 0.24879704415798187,
      "learning_rate": 9.988183850531454e-05,
      "loss": 0.0946,
      "step": 3780
    },
    {
      "epoch": 0.14199542917088157,
      "grad_norm": 0.24398647248744965,
      "learning_rate": 9.987970073711541e-05,
      "loss": 0.0949,
      "step": 3790
    },
    {
      "epoch": 0.142370087295343,
      "grad_norm": 0.22194218635559082,
      "learning_rate": 9.98775438270197e-05,
      "loss": 0.0942,
      "step": 3800
    },
    {
      "epoch": 0.14274474541980442,
      "grad_norm": 0.1988087296485901,
      "learning_rate": 9.987536777585514e-05,
      "loss": 0.0894,
      "step": 3810
    },
    {
      "epoch": 0.14311940354426586,
      "grad_norm": 0.21199938654899597,
      "learning_rate": 9.987317258445683e-05,
      "loss": 0.0921,
      "step": 3820
    },
    {
      "epoch": 0.14349406166872727,
      "grad_norm": 0.17694111168384552,
      "learning_rate": 9.987095825366722e-05,
      "loss": 0.0958,
      "step": 3830
    },
    {
      "epoch": 0.14386871979318872,
      "grad_norm": 0.2788679599761963,
      "learning_rate": 9.986872478433602e-05,
      "loss": 0.0958,
      "step": 3840
    },
    {
      "epoch": 0.14424337791765016,
      "grad_norm": 0.2630191445350647,
      "learning_rate": 9.98664721773204e-05,
      "loss": 0.0986,
      "step": 3850
    },
    {
      "epoch": 0.14461803604211157,
      "grad_norm": 0.23642908036708832,
      "learning_rate": 9.986420043348481e-05,
      "loss": 0.0964,
      "step": 3860
    },
    {
      "epoch": 0.144992694166573,
      "grad_norm": 0.22116273641586304,
      "learning_rate": 9.986190955370107e-05,
      "loss": 0.0945,
      "step": 3870
    },
    {
      "epoch": 0.14536735229103442,
      "grad_norm": 0.24780823290348053,
      "learning_rate": 9.985959953884832e-05,
      "loss": 0.0947,
      "step": 3880
    },
    {
      "epoch": 0.14574201041549587,
      "grad_norm": 0.19767040014266968,
      "learning_rate": 9.985727038981303e-05,
      "loss": 0.096,
      "step": 3890
    },
    {
      "epoch": 0.14611666853995728,
      "grad_norm": 0.21976687014102936,
      "learning_rate": 9.985492210748908e-05,
      "loss": 0.0975,
      "step": 3900
    },
    {
      "epoch": 0.14649132666441872,
      "grad_norm": 0.18232566118240356,
      "learning_rate": 9.985255469277762e-05,
      "loss": 0.0925,
      "step": 3910
    },
    {
      "epoch": 0.14686598478888016,
      "grad_norm": 0.27010267972946167,
      "learning_rate": 9.985016814658719e-05,
      "loss": 0.0962,
      "step": 3920
    },
    {
      "epoch": 0.14724064291334157,
      "grad_norm": 0.2952735722064972,
      "learning_rate": 9.984776246983364e-05,
      "loss": 0.0896,
      "step": 3930
    },
    {
      "epoch": 0.14761530103780302,
      "grad_norm": 0.23909670114517212,
      "learning_rate": 9.984533766344018e-05,
      "loss": 0.0962,
      "step": 3940
    },
    {
      "epoch": 0.14798995916226443,
      "grad_norm": 0.1722402274608612,
      "learning_rate": 9.984289372833737e-05,
      "loss": 0.1023,
      "step": 3950
    },
    {
      "epoch": 0.14836461728672587,
      "grad_norm": 0.13328610360622406,
      "learning_rate": 9.984043066546308e-05,
      "loss": 0.0982,
      "step": 3960
    },
    {
      "epoch": 0.14873927541118728,
      "grad_norm": 0.29250338673591614,
      "learning_rate": 9.983794847576254e-05,
      "loss": 0.0983,
      "step": 3970
    },
    {
      "epoch": 0.14911393353564872,
      "grad_norm": 0.3080466091632843,
      "learning_rate": 9.983544716018833e-05,
      "loss": 0.0982,
      "step": 3980
    },
    {
      "epoch": 0.14948859166011014,
      "grad_norm": 0.22011174261569977,
      "learning_rate": 9.983292671970034e-05,
      "loss": 0.0954,
      "step": 3990
    },
    {
      "epoch": 0.14986324978457158,
      "grad_norm": 0.21367430686950684,
      "learning_rate": 9.983038715526584e-05,
      "loss": 0.0956,
      "step": 4000
    },
    {
      "epoch": 0.15023790790903302,
      "grad_norm": 0.4171431362628937,
      "learning_rate": 9.982782846785938e-05,
      "loss": 0.0938,
      "step": 4010
    },
    {
      "epoch": 0.15061256603349443,
      "grad_norm": 0.20087669789791107,
      "learning_rate": 9.982525065846293e-05,
      "loss": 0.0988,
      "step": 4020
    },
    {
      "epoch": 0.15098722415795587,
      "grad_norm": 0.35355183482170105,
      "learning_rate": 9.982265372806573e-05,
      "loss": 0.0961,
      "step": 4030
    },
    {
      "epoch": 0.1513618822824173,
      "grad_norm": 0.2550514340400696,
      "learning_rate": 9.982003767766439e-05,
      "loss": 0.095,
      "step": 4040
    },
    {
      "epoch": 0.15173654040687873,
      "grad_norm": 0.24407541751861572,
      "learning_rate": 9.981740250826284e-05,
      "loss": 0.0983,
      "step": 4050
    },
    {
      "epoch": 0.15211119853134014,
      "grad_norm": 0.13835372030735016,
      "learning_rate": 9.981474822087236e-05,
      "loss": 0.0937,
      "step": 4060
    },
    {
      "epoch": 0.15248585665580158,
      "grad_norm": 0.2176474779844284,
      "learning_rate": 9.981207481651155e-05,
      "loss": 0.0941,
      "step": 4070
    },
    {
      "epoch": 0.15286051478026302,
      "grad_norm": 0.21907280385494232,
      "learning_rate": 9.980938229620638e-05,
      "loss": 0.0956,
      "step": 4080
    },
    {
      "epoch": 0.15323517290472444,
      "grad_norm": 0.27800315618515015,
      "learning_rate": 9.980667066099012e-05,
      "loss": 0.0955,
      "step": 4090
    },
    {
      "epoch": 0.15360983102918588,
      "grad_norm": 0.2280300110578537,
      "learning_rate": 9.98039399119034e-05,
      "loss": 0.0962,
      "step": 4100
    },
    {
      "epoch": 0.1539844891536473,
      "grad_norm": 0.31159767508506775,
      "learning_rate": 9.980119004999415e-05,
      "loss": 0.0938,
      "step": 4110
    },
    {
      "epoch": 0.15435914727810873,
      "grad_norm": 0.17876224219799042,
      "learning_rate": 9.979842107631772e-05,
      "loss": 0.0941,
      "step": 4120
    },
    {
      "epoch": 0.15473380540257015,
      "grad_norm": 0.22506274282932281,
      "learning_rate": 9.979563299193666e-05,
      "loss": 0.0969,
      "step": 4130
    },
    {
      "epoch": 0.1551084635270316,
      "grad_norm": 0.22907578945159912,
      "learning_rate": 9.9792825797921e-05,
      "loss": 0.0991,
      "step": 4140
    },
    {
      "epoch": 0.155483121651493,
      "grad_norm": 0.2896173894405365,
      "learning_rate": 9.978999949534798e-05,
      "loss": 0.0954,
      "step": 4150
    },
    {
      "epoch": 0.15585777977595444,
      "grad_norm": 0.22088977694511414,
      "learning_rate": 9.978715408530224e-05,
      "loss": 0.0983,
      "step": 4160
    },
    {
      "epoch": 0.15623243790041588,
      "grad_norm": 0.26051196455955505,
      "learning_rate": 9.978428956887573e-05,
      "loss": 0.0987,
      "step": 4170
    },
    {
      "epoch": 0.1566070960248773,
      "grad_norm": 0.2504218816757202,
      "learning_rate": 9.978140594716777e-05,
      "loss": 0.0975,
      "step": 4180
    },
    {
      "epoch": 0.15698175414933874,
      "grad_norm": 0.18310077488422394,
      "learning_rate": 9.977850322128497e-05,
      "loss": 0.095,
      "step": 4190
    },
    {
      "epoch": 0.15735641227380015,
      "grad_norm": 0.2264014482498169,
      "learning_rate": 9.977558139234127e-05,
      "loss": 0.0982,
      "step": 4200
    },
    {
      "epoch": 0.1577310703982616,
      "grad_norm": 0.16664916276931763,
      "learning_rate": 9.977264046145795e-05,
      "loss": 0.0949,
      "step": 4210
    },
    {
      "epoch": 0.158105728522723,
      "grad_norm": 0.1742296814918518,
      "learning_rate": 9.976968042976364e-05,
      "loss": 0.0965,
      "step": 4220
    },
    {
      "epoch": 0.15848038664718445,
      "grad_norm": 0.3183697760105133,
      "learning_rate": 9.976670129839431e-05,
      "loss": 0.0948,
      "step": 4230
    },
    {
      "epoch": 0.1588550447716459,
      "grad_norm": 0.2125307023525238,
      "learning_rate": 9.976370306849318e-05,
      "loss": 0.0927,
      "step": 4240
    },
    {
      "epoch": 0.1592297028961073,
      "grad_norm": 0.19568604230880737,
      "learning_rate": 9.97606857412109e-05,
      "loss": 0.0959,
      "step": 4250
    },
    {
      "epoch": 0.15960436102056874,
      "grad_norm": 0.19341252744197845,
      "learning_rate": 9.975764931770538e-05,
      "loss": 0.0979,
      "step": 4260
    },
    {
      "epoch": 0.15997901914503015,
      "grad_norm": 0.25130826234817505,
      "learning_rate": 9.975459379914189e-05,
      "loss": 0.0925,
      "step": 4270
    },
    {
      "epoch": 0.1603536772694916,
      "grad_norm": 0.20334209501743317,
      "learning_rate": 9.975151918669301e-05,
      "loss": 0.0916,
      "step": 4280
    },
    {
      "epoch": 0.160728335393953,
      "grad_norm": 0.3236646354198456,
      "learning_rate": 9.974842548153868e-05,
      "loss": 0.0933,
      "step": 4290
    },
    {
      "epoch": 0.16110299351841445,
      "grad_norm": 0.1601685881614685,
      "learning_rate": 9.97453126848661e-05,
      "loss": 0.0947,
      "step": 4300
    },
    {
      "epoch": 0.16147765164287586,
      "grad_norm": 0.4431193470954895,
      "learning_rate": 9.97421807978699e-05,
      "loss": 0.0965,
      "step": 4310
    },
    {
      "epoch": 0.1618523097673373,
      "grad_norm": 0.2095891386270523,
      "learning_rate": 9.973902982175194e-05,
      "loss": 0.0949,
      "step": 4320
    },
    {
      "epoch": 0.16222696789179875,
      "grad_norm": 0.22640034556388855,
      "learning_rate": 9.973585975772144e-05,
      "loss": 0.0975,
      "step": 4330
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 0.18507196009159088,
      "learning_rate": 9.973267060699496e-05,
      "loss": 0.0946,
      "step": 4340
    },
    {
      "epoch": 0.1629762841407216,
      "grad_norm": 0.16464491188526154,
      "learning_rate": 9.972946237079637e-05,
      "loss": 0.094,
      "step": 4350
    },
    {
      "epoch": 0.163350942265183,
      "grad_norm": 0.17620131373405457,
      "learning_rate": 9.972623505035688e-05,
      "loss": 0.0905,
      "step": 4360
    },
    {
      "epoch": 0.16372560038964445,
      "grad_norm": 0.1991976797580719,
      "learning_rate": 9.972298864691498e-05,
      "loss": 0.0972,
      "step": 4370
    },
    {
      "epoch": 0.16410025851410587,
      "grad_norm": 0.17422524094581604,
      "learning_rate": 9.971972316171654e-05,
      "loss": 0.0957,
      "step": 4380
    },
    {
      "epoch": 0.1644749166385673,
      "grad_norm": 0.26547473669052124,
      "learning_rate": 9.971643859601471e-05,
      "loss": 0.0911,
      "step": 4390
    },
    {
      "epoch": 0.16484957476302875,
      "grad_norm": 0.2882302403450012,
      "learning_rate": 9.971313495106998e-05,
      "loss": 0.0927,
      "step": 4400
    },
    {
      "epoch": 0.16522423288749016,
      "grad_norm": 0.14766955375671387,
      "learning_rate": 9.970981222815018e-05,
      "loss": 0.0941,
      "step": 4410
    },
    {
      "epoch": 0.1655988910119516,
      "grad_norm": 0.17675134539604187,
      "learning_rate": 9.970647042853042e-05,
      "loss": 0.0922,
      "step": 4420
    },
    {
      "epoch": 0.16597354913641302,
      "grad_norm": 0.18215861916542053,
      "learning_rate": 9.970310955349318e-05,
      "loss": 0.0891,
      "step": 4430
    },
    {
      "epoch": 0.16634820726087446,
      "grad_norm": 0.18046808242797852,
      "learning_rate": 9.96997296043282e-05,
      "loss": 0.0928,
      "step": 4440
    },
    {
      "epoch": 0.16672286538533587,
      "grad_norm": 0.2915896475315094,
      "learning_rate": 9.969633058233259e-05,
      "loss": 0.0935,
      "step": 4450
    },
    {
      "epoch": 0.1670975235097973,
      "grad_norm": 0.22637785971164703,
      "learning_rate": 9.969291248881077e-05,
      "loss": 0.0972,
      "step": 4460
    },
    {
      "epoch": 0.16747218163425873,
      "grad_norm": 0.20737065374851227,
      "learning_rate": 9.968947532507447e-05,
      "loss": 0.0928,
      "step": 4470
    },
    {
      "epoch": 0.16784683975872017,
      "grad_norm": 0.18964722752571106,
      "learning_rate": 9.968601909244272e-05,
      "loss": 0.0967,
      "step": 4480
    },
    {
      "epoch": 0.1682214978831816,
      "grad_norm": 0.3041734993457794,
      "learning_rate": 9.968254379224191e-05,
      "loss": 0.095,
      "step": 4490
    },
    {
      "epoch": 0.16859615600764302,
      "grad_norm": 0.1910841166973114,
      "learning_rate": 9.967904942580573e-05,
      "loss": 0.0949,
      "step": 4500
    },
    {
      "epoch": 0.16897081413210446,
      "grad_norm": 0.2402835190296173,
      "learning_rate": 9.967553599447515e-05,
      "loss": 0.094,
      "step": 4510
    },
    {
      "epoch": 0.16934547225656588,
      "grad_norm": 0.1605314463376999,
      "learning_rate": 9.967200349959852e-05,
      "loss": 0.0924,
      "step": 4520
    },
    {
      "epoch": 0.16972013038102732,
      "grad_norm": 0.14229685068130493,
      "learning_rate": 9.966845194253148e-05,
      "loss": 0.0911,
      "step": 4530
    },
    {
      "epoch": 0.17009478850548873,
      "grad_norm": 0.22614628076553345,
      "learning_rate": 9.966488132463694e-05,
      "loss": 0.0955,
      "step": 4540
    },
    {
      "epoch": 0.17046944662995017,
      "grad_norm": 0.21714606881141663,
      "learning_rate": 9.966129164728519e-05,
      "loss": 0.0972,
      "step": 4550
    },
    {
      "epoch": 0.1708441047544116,
      "grad_norm": 0.1849604994058609,
      "learning_rate": 9.965768291185383e-05,
      "loss": 0.094,
      "step": 4560
    },
    {
      "epoch": 0.17121876287887303,
      "grad_norm": 0.15327146649360657,
      "learning_rate": 9.96540551197277e-05,
      "loss": 0.092,
      "step": 4570
    },
    {
      "epoch": 0.17159342100333447,
      "grad_norm": 0.23274967074394226,
      "learning_rate": 9.965040827229906e-05,
      "loss": 0.0942,
      "step": 4580
    },
    {
      "epoch": 0.17196807912779588,
      "grad_norm": 0.13703660666942596,
      "learning_rate": 9.964674237096741e-05,
      "loss": 0.0966,
      "step": 4590
    },
    {
      "epoch": 0.17234273725225732,
      "grad_norm": 0.15780121088027954,
      "learning_rate": 9.964305741713957e-05,
      "loss": 0.0913,
      "step": 4600
    },
    {
      "epoch": 0.17271739537671874,
      "grad_norm": 0.2397664189338684,
      "learning_rate": 9.963935341222968e-05,
      "loss": 0.0952,
      "step": 4610
    },
    {
      "epoch": 0.17309205350118018,
      "grad_norm": 0.17698298394680023,
      "learning_rate": 9.963563035765921e-05,
      "loss": 0.0933,
      "step": 4620
    },
    {
      "epoch": 0.1734667116256416,
      "grad_norm": 0.14908252656459808,
      "learning_rate": 9.963188825485692e-05,
      "loss": 0.0967,
      "step": 4630
    },
    {
      "epoch": 0.17384136975010303,
      "grad_norm": 0.18836164474487305,
      "learning_rate": 9.96281271052589e-05,
      "loss": 0.0935,
      "step": 4640
    },
    {
      "epoch": 0.17421602787456447,
      "grad_norm": 0.15560750663280487,
      "learning_rate": 9.962434691030848e-05,
      "loss": 0.0955,
      "step": 4650
    },
    {
      "epoch": 0.17459068599902589,
      "grad_norm": 0.20780177414417267,
      "learning_rate": 9.962054767145642e-05,
      "loss": 0.091,
      "step": 4660
    },
    {
      "epoch": 0.17496534412348733,
      "grad_norm": 0.17905780673027039,
      "learning_rate": 9.961672939016067e-05,
      "loss": 0.0906,
      "step": 4670
    },
    {
      "epoch": 0.17534000224794874,
      "grad_norm": 0.1868622750043869,
      "learning_rate": 9.961289206788655e-05,
      "loss": 0.094,
      "step": 4680
    },
    {
      "epoch": 0.17571466037241018,
      "grad_norm": 0.17872639000415802,
      "learning_rate": 9.960903570610669e-05,
      "loss": 0.0982,
      "step": 4690
    },
    {
      "epoch": 0.1760893184968716,
      "grad_norm": 0.1935497224330902,
      "learning_rate": 9.960516030630101e-05,
      "loss": 0.0969,
      "step": 4700
    },
    {
      "epoch": 0.17646397662133304,
      "grad_norm": 0.19436129927635193,
      "learning_rate": 9.960126586995673e-05,
      "loss": 0.0929,
      "step": 4710
    },
    {
      "epoch": 0.17683863474579448,
      "grad_norm": 0.22946061193943024,
      "learning_rate": 9.959735239856838e-05,
      "loss": 0.0939,
      "step": 4720
    },
    {
      "epoch": 0.1772132928702559,
      "grad_norm": 0.16682003438472748,
      "learning_rate": 9.95934198936378e-05,
      "loss": 0.0969,
      "step": 4730
    },
    {
      "epoch": 0.17758795099471733,
      "grad_norm": 0.20713481307029724,
      "learning_rate": 9.958946835667415e-05,
      "loss": 0.0942,
      "step": 4740
    },
    {
      "epoch": 0.17796260911917874,
      "grad_norm": 0.19114550948143005,
      "learning_rate": 9.958549778919386e-05,
      "loss": 0.0964,
      "step": 4750
    },
    {
      "epoch": 0.17833726724364019,
      "grad_norm": 0.19562365114688873,
      "learning_rate": 9.958150819272069e-05,
      "loss": 0.0952,
      "step": 4760
    },
    {
      "epoch": 0.1787119253681016,
      "grad_norm": 0.17341040074825287,
      "learning_rate": 9.957749956878568e-05,
      "loss": 0.0922,
      "step": 4770
    },
    {
      "epoch": 0.17908658349256304,
      "grad_norm": 0.1868043839931488,
      "learning_rate": 9.957347191892718e-05,
      "loss": 0.0944,
      "step": 4780
    },
    {
      "epoch": 0.17946124161702445,
      "grad_norm": 0.18035829067230225,
      "learning_rate": 9.956942524469088e-05,
      "loss": 0.0959,
      "step": 4790
    },
    {
      "epoch": 0.1798358997414859,
      "grad_norm": 0.24253347516059875,
      "learning_rate": 9.95653595476297e-05,
      "loss": 0.0958,
      "step": 4800
    },
    {
      "epoch": 0.18021055786594734,
      "grad_norm": 0.1858159452676773,
      "learning_rate": 9.956127482930393e-05,
      "loss": 0.0974,
      "step": 4810
    },
    {
      "epoch": 0.18058521599040875,
      "grad_norm": 0.31554022431373596,
      "learning_rate": 9.95571710912811e-05,
      "loss": 0.0955,
      "step": 4820
    },
    {
      "epoch": 0.1809598741148702,
      "grad_norm": 0.19111956655979156,
      "learning_rate": 9.955304833513606e-05,
      "loss": 0.0926,
      "step": 4830
    },
    {
      "epoch": 0.1813345322393316,
      "grad_norm": 0.16183829307556152,
      "learning_rate": 9.954890656245099e-05,
      "loss": 0.0947,
      "step": 4840
    },
    {
      "epoch": 0.18170919036379304,
      "grad_norm": 0.1833571046590805,
      "learning_rate": 9.954474577481534e-05,
      "loss": 0.0942,
      "step": 4850
    },
    {
      "epoch": 0.18208384848825446,
      "grad_norm": 0.17420992255210876,
      "learning_rate": 9.954056597382583e-05,
      "loss": 0.0933,
      "step": 4860
    },
    {
      "epoch": 0.1824585066127159,
      "grad_norm": 0.17595426738262177,
      "learning_rate": 9.953636716108653e-05,
      "loss": 0.0975,
      "step": 4870
    },
    {
      "epoch": 0.18283316473717734,
      "grad_norm": 0.19996656477451324,
      "learning_rate": 9.953214933820879e-05,
      "loss": 0.0954,
      "step": 4880
    },
    {
      "epoch": 0.18320782286163875,
      "grad_norm": 0.15664762258529663,
      "learning_rate": 9.952791250681121e-05,
      "loss": 0.093,
      "step": 4890
    },
    {
      "epoch": 0.1835824809861002,
      "grad_norm": 0.1789017766714096,
      "learning_rate": 9.952365666851976e-05,
      "loss": 0.0958,
      "step": 4900
    },
    {
      "epoch": 0.1839571391105616,
      "grad_norm": 0.15755344927310944,
      "learning_rate": 9.951938182496765e-05,
      "loss": 0.0939,
      "step": 4910
    },
    {
      "epoch": 0.18433179723502305,
      "grad_norm": 0.1466655731201172,
      "learning_rate": 9.951508797779539e-05,
      "loss": 0.0882,
      "step": 4920
    },
    {
      "epoch": 0.18470645535948446,
      "grad_norm": 0.19768773019313812,
      "learning_rate": 9.951077512865081e-05,
      "loss": 0.0923,
      "step": 4930
    },
    {
      "epoch": 0.1850811134839459,
      "grad_norm": 0.2086498737335205,
      "learning_rate": 9.950644327918901e-05,
      "loss": 0.0961,
      "step": 4940
    },
    {
      "epoch": 0.18545577160840732,
      "grad_norm": 0.18579596281051636,
      "learning_rate": 9.950209243107238e-05,
      "loss": 0.0924,
      "step": 4950
    },
    {
      "epoch": 0.18583042973286876,
      "grad_norm": 0.22241130471229553,
      "learning_rate": 9.94977225859706e-05,
      "loss": 0.0953,
      "step": 4960
    },
    {
      "epoch": 0.1862050878573302,
      "grad_norm": 0.15781638026237488,
      "learning_rate": 9.949333374556067e-05,
      "loss": 0.091,
      "step": 4970
    },
    {
      "epoch": 0.1865797459817916,
      "grad_norm": 0.19272065162658691,
      "learning_rate": 9.948892591152685e-05,
      "loss": 0.0942,
      "step": 4980
    },
    {
      "epoch": 0.18695440410625305,
      "grad_norm": 0.3036370277404785,
      "learning_rate": 9.948449908556068e-05,
      "loss": 0.0952,
      "step": 4990
    },
    {
      "epoch": 0.18732906223071447,
      "grad_norm": 0.1351524442434311,
      "learning_rate": 9.948005326936101e-05,
      "loss": 0.0943,
      "step": 5000
    },
    {
      "epoch": 0.1877037203551759,
      "grad_norm": 0.23445913195610046,
      "learning_rate": 9.947558846463397e-05,
      "loss": 0.0957,
      "step": 5010
    },
    {
      "epoch": 0.18807837847963732,
      "grad_norm": 0.16807302832603455,
      "learning_rate": 9.9471104673093e-05,
      "loss": 0.0939,
      "step": 5020
    },
    {
      "epoch": 0.18845303660409876,
      "grad_norm": 0.17305664718151093,
      "learning_rate": 9.946660189645878e-05,
      "loss": 0.094,
      "step": 5030
    },
    {
      "epoch": 0.1888276947285602,
      "grad_norm": 0.14046330749988556,
      "learning_rate": 9.946208013645932e-05,
      "loss": 0.0921,
      "step": 5040
    },
    {
      "epoch": 0.18920235285302162,
      "grad_norm": 0.21541398763656616,
      "learning_rate": 9.945753939482989e-05,
      "loss": 0.0976,
      "step": 5050
    },
    {
      "epoch": 0.18957701097748306,
      "grad_norm": 0.1683279573917389,
      "learning_rate": 9.945297967331304e-05,
      "loss": 0.0945,
      "step": 5060
    },
    {
      "epoch": 0.18995166910194447,
      "grad_norm": 0.13954313099384308,
      "learning_rate": 9.944840097365863e-05,
      "loss": 0.0914,
      "step": 5070
    },
    {
      "epoch": 0.1903263272264059,
      "grad_norm": 0.18491676449775696,
      "learning_rate": 9.944380329762378e-05,
      "loss": 0.0945,
      "step": 5080
    },
    {
      "epoch": 0.19070098535086732,
      "grad_norm": 0.2248256802558899,
      "learning_rate": 9.94391866469729e-05,
      "loss": 0.0942,
      "step": 5090
    },
    {
      "epoch": 0.19107564347532877,
      "grad_norm": 0.1895461529493332,
      "learning_rate": 9.943455102347767e-05,
      "loss": 0.0946,
      "step": 5100
    },
    {
      "epoch": 0.19145030159979018,
      "grad_norm": 0.18287238478660583,
      "learning_rate": 9.942989642891709e-05,
      "loss": 0.0964,
      "step": 5110
    },
    {
      "epoch": 0.19182495972425162,
      "grad_norm": 0.29392209649086,
      "learning_rate": 9.942522286507739e-05,
      "loss": 0.0949,
      "step": 5120
    },
    {
      "epoch": 0.19219961784871306,
      "grad_norm": 0.15843382477760315,
      "learning_rate": 9.942053033375211e-05,
      "loss": 0.0952,
      "step": 5130
    },
    {
      "epoch": 0.19257427597317447,
      "grad_norm": 0.24085353314876556,
      "learning_rate": 9.941581883674209e-05,
      "loss": 0.0971,
      "step": 5140
    },
    {
      "epoch": 0.19294893409763592,
      "grad_norm": 0.16553843021392822,
      "learning_rate": 9.941108837585535e-05,
      "loss": 0.0955,
      "step": 5150
    },
    {
      "epoch": 0.19332359222209733,
      "grad_norm": 0.16055777668952942,
      "learning_rate": 9.940633895290733e-05,
      "loss": 0.095,
      "step": 5160
    },
    {
      "epoch": 0.19369825034655877,
      "grad_norm": 0.23454059660434723,
      "learning_rate": 9.940157056972064e-05,
      "loss": 0.0923,
      "step": 5170
    },
    {
      "epoch": 0.19407290847102018,
      "grad_norm": 0.19810181856155396,
      "learning_rate": 9.93967832281252e-05,
      "loss": 0.092,
      "step": 5180
    },
    {
      "epoch": 0.19444756659548162,
      "grad_norm": 0.22484001517295837,
      "learning_rate": 9.93919769299582e-05,
      "loss": 0.0957,
      "step": 5190
    },
    {
      "epoch": 0.19482222471994307,
      "grad_norm": 0.1725151687860489,
      "learning_rate": 9.938715167706414e-05,
      "loss": 0.0942,
      "step": 5200
    },
    {
      "epoch": 0.19519688284440448,
      "grad_norm": 0.2650492489337921,
      "learning_rate": 9.938230747129474e-05,
      "loss": 0.0939,
      "step": 5210
    },
    {
      "epoch": 0.19557154096886592,
      "grad_norm": 0.1980568766593933,
      "learning_rate": 9.937744431450903e-05,
      "loss": 0.0922,
      "step": 5220
    },
    {
      "epoch": 0.19594619909332733,
      "grad_norm": 0.1504083275794983,
      "learning_rate": 9.93725622085733e-05,
      "loss": 0.091,
      "step": 5230
    },
    {
      "epoch": 0.19632085721778877,
      "grad_norm": 0.269411563873291,
      "learning_rate": 9.93676611553611e-05,
      "loss": 0.0935,
      "step": 5240
    },
    {
      "epoch": 0.1966955153422502,
      "grad_norm": 0.1597176194190979,
      "learning_rate": 9.936274115675328e-05,
      "loss": 0.0943,
      "step": 5250
    },
    {
      "epoch": 0.19707017346671163,
      "grad_norm": 0.17509576678276062,
      "learning_rate": 9.935780221463796e-05,
      "loss": 0.0934,
      "step": 5260
    },
    {
      "epoch": 0.19744483159117304,
      "grad_norm": 0.141645610332489,
      "learning_rate": 9.935284433091045e-05,
      "loss": 0.0944,
      "step": 5270
    },
    {
      "epoch": 0.19781948971563448,
      "grad_norm": 0.20614786446094513,
      "learning_rate": 9.934786750747347e-05,
      "loss": 0.0966,
      "step": 5280
    },
    {
      "epoch": 0.19819414784009592,
      "grad_norm": 0.6800267100334167,
      "learning_rate": 9.93428717462369e-05,
      "loss": 0.0916,
      "step": 5290
    },
    {
      "epoch": 0.19856880596455734,
      "grad_norm": 0.18767592310905457,
      "learning_rate": 9.933785704911791e-05,
      "loss": 0.0921,
      "step": 5300
    },
    {
      "epoch": 0.19894346408901878,
      "grad_norm": 0.22966237366199493,
      "learning_rate": 9.933282341804095e-05,
      "loss": 0.0932,
      "step": 5310
    },
    {
      "epoch": 0.1993181222134802,
      "grad_norm": 0.22239802777767181,
      "learning_rate": 9.932777085493776e-05,
      "loss": 0.095,
      "step": 5320
    },
    {
      "epoch": 0.19969278033794163,
      "grad_norm": 0.29621562361717224,
      "learning_rate": 9.932269936174727e-05,
      "loss": 0.0994,
      "step": 5330
    },
    {
      "epoch": 0.20006743846240305,
      "grad_norm": 0.16044119000434875,
      "learning_rate": 9.931760894041578e-05,
      "loss": 0.0972,
      "step": 5340
    },
    {
      "epoch": 0.2004420965868645,
      "grad_norm": 0.1813359260559082,
      "learning_rate": 9.931249959289674e-05,
      "loss": 0.0975,
      "step": 5350
    },
    {
      "epoch": 0.20081675471132593,
      "grad_norm": 0.19430217146873474,
      "learning_rate": 9.930737132115095e-05,
      "loss": 0.0949,
      "step": 5360
    },
    {
      "epoch": 0.20119141283578734,
      "grad_norm": 0.14565223455429077,
      "learning_rate": 9.930222412714645e-05,
      "loss": 0.0927,
      "step": 5370
    },
    {
      "epoch": 0.20156607096024878,
      "grad_norm": 0.16697674989700317,
      "learning_rate": 9.929705801285851e-05,
      "loss": 0.0951,
      "step": 5380
    },
    {
      "epoch": 0.2019407290847102,
      "grad_norm": 0.11193056404590607,
      "learning_rate": 9.929187298026969e-05,
      "loss": 0.0949,
      "step": 5390
    },
    {
      "epoch": 0.20231538720917164,
      "grad_norm": 0.22950783371925354,
      "learning_rate": 9.928666903136983e-05,
      "loss": 0.0944,
      "step": 5400
    },
    {
      "epoch": 0.20269004533363305,
      "grad_norm": 0.1758977770805359,
      "learning_rate": 9.928144616815597e-05,
      "loss": 0.099,
      "step": 5410
    },
    {
      "epoch": 0.2030647034580945,
      "grad_norm": 0.188322514295578,
      "learning_rate": 9.927620439263246e-05,
      "loss": 0.0933,
      "step": 5420
    },
    {
      "epoch": 0.2034393615825559,
      "grad_norm": 0.1919732391834259,
      "learning_rate": 9.927094370681089e-05,
      "loss": 0.0934,
      "step": 5430
    },
    {
      "epoch": 0.20381401970701735,
      "grad_norm": 0.1478557288646698,
      "learning_rate": 9.92656641127101e-05,
      "loss": 0.0955,
      "step": 5440
    },
    {
      "epoch": 0.2041886778314788,
      "grad_norm": 0.12366266548633575,
      "learning_rate": 9.926036561235618e-05,
      "loss": 0.0965,
      "step": 5450
    },
    {
      "epoch": 0.2045633359559402,
      "grad_norm": 0.2263043075799942,
      "learning_rate": 9.925504820778253e-05,
      "loss": 0.095,
      "step": 5460
    },
    {
      "epoch": 0.20493799408040164,
      "grad_norm": 0.15023984014987946,
      "learning_rate": 9.924971190102974e-05,
      "loss": 0.0937,
      "step": 5470
    },
    {
      "epoch": 0.20531265220486306,
      "grad_norm": 0.1632002890110016,
      "learning_rate": 9.924435669414567e-05,
      "loss": 0.092,
      "step": 5480
    },
    {
      "epoch": 0.2056873103293245,
      "grad_norm": 0.19217227399349213,
      "learning_rate": 9.923898258918546e-05,
      "loss": 0.0905,
      "step": 5490
    },
    {
      "epoch": 0.2060619684537859,
      "grad_norm": 0.1819583624601364,
      "learning_rate": 9.923358958821147e-05,
      "loss": 0.0994,
      "step": 5500
    },
    {
      "epoch": 0.20643662657824735,
      "grad_norm": 0.1417621225118637,
      "learning_rate": 9.922817769329333e-05,
      "loss": 0.0935,
      "step": 5510
    },
    {
      "epoch": 0.2068112847027088,
      "grad_norm": 0.17016355693340302,
      "learning_rate": 9.922274690650791e-05,
      "loss": 0.0955,
      "step": 5520
    },
    {
      "epoch": 0.2071859428271702,
      "grad_norm": 0.41832417249679565,
      "learning_rate": 9.921729722993933e-05,
      "loss": 0.0885,
      "step": 5530
    },
    {
      "epoch": 0.20756060095163165,
      "grad_norm": 0.1725090742111206,
      "learning_rate": 9.921182866567897e-05,
      "loss": 0.0917,
      "step": 5540
    },
    {
      "epoch": 0.20793525907609306,
      "grad_norm": 0.22291068732738495,
      "learning_rate": 9.920634121582548e-05,
      "loss": 0.0936,
      "step": 5550
    },
    {
      "epoch": 0.2083099172005545,
      "grad_norm": 0.17559373378753662,
      "learning_rate": 9.920083488248468e-05,
      "loss": 0.0949,
      "step": 5560
    },
    {
      "epoch": 0.20868457532501591,
      "grad_norm": 0.16367162764072418,
      "learning_rate": 9.919530966776973e-05,
      "loss": 0.094,
      "step": 5570
    },
    {
      "epoch": 0.20905923344947736,
      "grad_norm": 0.16214586794376373,
      "learning_rate": 9.918976557380098e-05,
      "loss": 0.0951,
      "step": 5580
    },
    {
      "epoch": 0.20943389157393877,
      "grad_norm": 0.14417338371276855,
      "learning_rate": 9.918420260270605e-05,
      "loss": 0.0996,
      "step": 5590
    },
    {
      "epoch": 0.2098085496984002,
      "grad_norm": 0.1396809071302414,
      "learning_rate": 9.917862075661975e-05,
      "loss": 0.0962,
      "step": 5600
    },
    {
      "epoch": 0.21018320782286165,
      "grad_norm": 0.21852031350135803,
      "learning_rate": 9.917302003768423e-05,
      "loss": 0.0937,
      "step": 5610
    },
    {
      "epoch": 0.21055786594732306,
      "grad_norm": 0.1674123853445053,
      "learning_rate": 9.916740044804879e-05,
      "loss": 0.0929,
      "step": 5620
    },
    {
      "epoch": 0.2109325240717845,
      "grad_norm": 0.19793911278247833,
      "learning_rate": 9.916176198987003e-05,
      "loss": 0.093,
      "step": 5630
    },
    {
      "epoch": 0.21130718219624592,
      "grad_norm": 0.11135517805814743,
      "learning_rate": 9.915610466531176e-05,
      "loss": 0.0929,
      "step": 5640
    },
    {
      "epoch": 0.21168184032070736,
      "grad_norm": 0.1561625748872757,
      "learning_rate": 9.915042847654505e-05,
      "loss": 0.0905,
      "step": 5650
    },
    {
      "epoch": 0.21205649844516877,
      "grad_norm": 0.13059759140014648,
      "learning_rate": 9.914473342574822e-05,
      "loss": 0.0921,
      "step": 5660
    },
    {
      "epoch": 0.21243115656963021,
      "grad_norm": 0.125606968998909,
      "learning_rate": 9.913901951510676e-05,
      "loss": 0.0957,
      "step": 5670
    },
    {
      "epoch": 0.21280581469409166,
      "grad_norm": 0.14682433009147644,
      "learning_rate": 9.91332867468135e-05,
      "loss": 0.0898,
      "step": 5680
    },
    {
      "epoch": 0.21318047281855307,
      "grad_norm": 0.19051294028759003,
      "learning_rate": 9.912753512306843e-05,
      "loss": 0.0947,
      "step": 5690
    },
    {
      "epoch": 0.2135551309430145,
      "grad_norm": 0.14682789146900177,
      "learning_rate": 9.91217646460788e-05,
      "loss": 0.0943,
      "step": 5700
    },
    {
      "epoch": 0.21392978906747592,
      "grad_norm": 0.20472073554992676,
      "learning_rate": 9.911597531805909e-05,
      "loss": 0.0966,
      "step": 5710
    },
    {
      "epoch": 0.21430444719193736,
      "grad_norm": 0.12853965163230896,
      "learning_rate": 9.911016714123104e-05,
      "loss": 0.0943,
      "step": 5720
    },
    {
      "epoch": 0.21467910531639878,
      "grad_norm": 0.153652161359787,
      "learning_rate": 9.910434011782359e-05,
      "loss": 0.0944,
      "step": 5730
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 0.1422375589609146,
      "learning_rate": 9.909849425007292e-05,
      "loss": 0.0959,
      "step": 5740
    },
    {
      "epoch": 0.21542842156532163,
      "grad_norm": 0.1355072259902954,
      "learning_rate": 9.909262954022245e-05,
      "loss": 0.0932,
      "step": 5750
    },
    {
      "epoch": 0.21580307968978307,
      "grad_norm": 0.17511999607086182,
      "learning_rate": 9.908674599052283e-05,
      "loss": 0.095,
      "step": 5760
    },
    {
      "epoch": 0.2161777378142445,
      "grad_norm": 0.18434551358222961,
      "learning_rate": 9.908084360323195e-05,
      "loss": 0.0925,
      "step": 5770
    },
    {
      "epoch": 0.21655239593870593,
      "grad_norm": 0.1666508913040161,
      "learning_rate": 9.907492238061491e-05,
      "loss": 0.092,
      "step": 5780
    },
    {
      "epoch": 0.21692705406316737,
      "grad_norm": 0.11853310465812683,
      "learning_rate": 9.906898232494404e-05,
      "loss": 0.0922,
      "step": 5790
    },
    {
      "epoch": 0.21730171218762878,
      "grad_norm": 0.15457004308700562,
      "learning_rate": 9.906302343849891e-05,
      "loss": 0.0943,
      "step": 5800
    },
    {
      "epoch": 0.21767637031209022,
      "grad_norm": 0.1424666941165924,
      "learning_rate": 9.905704572356631e-05,
      "loss": 0.0964,
      "step": 5810
    },
    {
      "epoch": 0.21805102843655164,
      "grad_norm": 0.13700826466083527,
      "learning_rate": 9.905104918244026e-05,
      "loss": 0.0943,
      "step": 5820
    },
    {
      "epoch": 0.21842568656101308,
      "grad_norm": 0.18385952711105347,
      "learning_rate": 9.904503381742198e-05,
      "loss": 0.0932,
      "step": 5830
    },
    {
      "epoch": 0.21880034468547452,
      "grad_norm": 0.20842669904232025,
      "learning_rate": 9.903899963081996e-05,
      "loss": 0.0922,
      "step": 5840
    },
    {
      "epoch": 0.21917500280993593,
      "grad_norm": 0.406687468290329,
      "learning_rate": 9.903294662494985e-05,
      "loss": 0.0932,
      "step": 5850
    },
    {
      "epoch": 0.21954966093439737,
      "grad_norm": 0.19540004432201385,
      "learning_rate": 9.902687480213459e-05,
      "loss": 0.0927,
      "step": 5860
    },
    {
      "epoch": 0.21992431905885879,
      "grad_norm": 0.17186319828033447,
      "learning_rate": 9.902078416470433e-05,
      "loss": 0.0928,
      "step": 5870
    },
    {
      "epoch": 0.22029897718332023,
      "grad_norm": 0.14942100644111633,
      "learning_rate": 9.901467471499637e-05,
      "loss": 0.0963,
      "step": 5880
    },
    {
      "epoch": 0.22067363530778164,
      "grad_norm": 0.16977903246879578,
      "learning_rate": 9.900854645535532e-05,
      "loss": 0.0957,
      "step": 5890
    },
    {
      "epoch": 0.22104829343224308,
      "grad_norm": 0.11789559572935104,
      "learning_rate": 9.900239938813294e-05,
      "loss": 0.09,
      "step": 5900
    },
    {
      "epoch": 0.2214229515567045,
      "grad_norm": 0.19987106323242188,
      "learning_rate": 9.899623351568826e-05,
      "loss": 0.0954,
      "step": 5910
    },
    {
      "epoch": 0.22179760968116594,
      "grad_norm": 0.24673183262348175,
      "learning_rate": 9.899004884038748e-05,
      "loss": 0.0907,
      "step": 5920
    },
    {
      "epoch": 0.22217226780562738,
      "grad_norm": 0.20166179537773132,
      "learning_rate": 9.898384536460405e-05,
      "loss": 0.0952,
      "step": 5930
    },
    {
      "epoch": 0.2225469259300888,
      "grad_norm": 0.16718633472919464,
      "learning_rate": 9.897762309071863e-05,
      "loss": 0.0926,
      "step": 5940
    },
    {
      "epoch": 0.22292158405455023,
      "grad_norm": 0.21219803392887115,
      "learning_rate": 9.897138202111908e-05,
      "loss": 0.0945,
      "step": 5950
    },
    {
      "epoch": 0.22329624217901164,
      "grad_norm": 0.1569046825170517,
      "learning_rate": 9.896512215820049e-05,
      "loss": 0.0935,
      "step": 5960
    },
    {
      "epoch": 0.22367090030347309,
      "grad_norm": 0.18658721446990967,
      "learning_rate": 9.895884350436513e-05,
      "loss": 0.0945,
      "step": 5970
    },
    {
      "epoch": 0.2240455584279345,
      "grad_norm": 0.1873861849308014,
      "learning_rate": 9.895254606202253e-05,
      "loss": 0.0942,
      "step": 5980
    },
    {
      "epoch": 0.22442021655239594,
      "grad_norm": 0.17025652527809143,
      "learning_rate": 9.894622983358941e-05,
      "loss": 0.0943,
      "step": 5990
    },
    {
      "epoch": 0.22479487467685738,
      "grad_norm": 0.21656359732151031,
      "learning_rate": 9.893989482148966e-05,
      "loss": 0.0911,
      "step": 6000
    },
    {
      "epoch": 0.2251695328013188,
      "grad_norm": 0.21678270399570465,
      "learning_rate": 9.893354102815443e-05,
      "loss": 0.0952,
      "step": 6010
    },
    {
      "epoch": 0.22554419092578024,
      "grad_norm": 0.1438109576702118,
      "learning_rate": 9.892716845602205e-05,
      "loss": 0.0913,
      "step": 6020
    },
    {
      "epoch": 0.22591884905024165,
      "grad_norm": 0.17189723253250122,
      "learning_rate": 9.89207771075381e-05,
      "loss": 0.0944,
      "step": 6030
    },
    {
      "epoch": 0.2262935071747031,
      "grad_norm": 0.2056134045124054,
      "learning_rate": 9.89143669851553e-05,
      "loss": 0.0953,
      "step": 6040
    },
    {
      "epoch": 0.2266681652991645,
      "grad_norm": 0.14469122886657715,
      "learning_rate": 9.890793809133361e-05,
      "loss": 0.0929,
      "step": 6050
    },
    {
      "epoch": 0.22704282342362594,
      "grad_norm": 0.16850940883159637,
      "learning_rate": 9.89014904285402e-05,
      "loss": 0.0946,
      "step": 6060
    },
    {
      "epoch": 0.22741748154808736,
      "grad_norm": 0.22098875045776367,
      "learning_rate": 9.889502399924943e-05,
      "loss": 0.0963,
      "step": 6070
    },
    {
      "epoch": 0.2277921396725488,
      "grad_norm": 0.13220429420471191,
      "learning_rate": 9.888853880594285e-05,
      "loss": 0.0909,
      "step": 6080
    },
    {
      "epoch": 0.22816679779701024,
      "grad_norm": 0.15544383227825165,
      "learning_rate": 9.888203485110926e-05,
      "loss": 0.0952,
      "step": 6090
    },
    {
      "epoch": 0.22854145592147165,
      "grad_norm": 0.16454310715198517,
      "learning_rate": 9.887551213724459e-05,
      "loss": 0.096,
      "step": 6100
    },
    {
      "epoch": 0.2289161140459331,
      "grad_norm": 0.10997170954942703,
      "learning_rate": 9.8868970666852e-05,
      "loss": 0.0919,
      "step": 6110
    },
    {
      "epoch": 0.2292907721703945,
      "grad_norm": 0.14850899577140808,
      "learning_rate": 9.88624104424419e-05,
      "loss": 0.0908,
      "step": 6120
    },
    {
      "epoch": 0.22966543029485595,
      "grad_norm": 0.16440510749816895,
      "learning_rate": 9.885583146653183e-05,
      "loss": 0.0988,
      "step": 6130
    },
    {
      "epoch": 0.23004008841931736,
      "grad_norm": 0.16879498958587646,
      "learning_rate": 9.884923374164652e-05,
      "loss": 0.0907,
      "step": 6140
    },
    {
      "epoch": 0.2304147465437788,
      "grad_norm": 0.23416629433631897,
      "learning_rate": 9.884261727031796e-05,
      "loss": 0.0921,
      "step": 6150
    },
    {
      "epoch": 0.23078940466824024,
      "grad_norm": 0.15915806591510773,
      "learning_rate": 9.883598205508527e-05,
      "loss": 0.0931,
      "step": 6160
    },
    {
      "epoch": 0.23116406279270166,
      "grad_norm": 0.1790154129266739,
      "learning_rate": 9.882932809849478e-05,
      "loss": 0.097,
      "step": 6170
    },
    {
      "epoch": 0.2315387209171631,
      "grad_norm": 0.19424690306186676,
      "learning_rate": 9.882265540310003e-05,
      "loss": 0.0947,
      "step": 6180
    },
    {
      "epoch": 0.2319133790416245,
      "grad_norm": 0.16586048901081085,
      "learning_rate": 9.881596397146176e-05,
      "loss": 0.0906,
      "step": 6190
    },
    {
      "epoch": 0.23228803716608595,
      "grad_norm": 0.15634338557720184,
      "learning_rate": 9.880925380614787e-05,
      "loss": 0.0921,
      "step": 6200
    },
    {
      "epoch": 0.23266269529054737,
      "grad_norm": 0.1534351408481598,
      "learning_rate": 9.880252490973346e-05,
      "loss": 0.0932,
      "step": 6210
    },
    {
      "epoch": 0.2330373534150088,
      "grad_norm": 0.15173587203025818,
      "learning_rate": 9.87957772848008e-05,
      "loss": 0.0914,
      "step": 6220
    },
    {
      "epoch": 0.23341201153947022,
      "grad_norm": 0.13728412985801697,
      "learning_rate": 9.878901093393941e-05,
      "loss": 0.0977,
      "step": 6230
    },
    {
      "epoch": 0.23378666966393166,
      "grad_norm": 0.11052738130092621,
      "learning_rate": 9.878222585974592e-05,
      "loss": 0.095,
      "step": 6240
    },
    {
      "epoch": 0.2341613277883931,
      "grad_norm": 0.15350748598575592,
      "learning_rate": 9.87754220648242e-05,
      "loss": 0.0935,
      "step": 6250
    },
    {
      "epoch": 0.23453598591285452,
      "grad_norm": 0.1444738358259201,
      "learning_rate": 9.876859955178527e-05,
      "loss": 0.0927,
      "step": 6260
    },
    {
      "epoch": 0.23491064403731596,
      "grad_norm": 0.15240271389484406,
      "learning_rate": 9.876175832324735e-05,
      "loss": 0.0937,
      "step": 6270
    },
    {
      "epoch": 0.23528530216177737,
      "grad_norm": 0.11235090345144272,
      "learning_rate": 9.875489838183581e-05,
      "loss": 0.0976,
      "step": 6280
    },
    {
      "epoch": 0.2356599602862388,
      "grad_norm": 0.1673414409160614,
      "learning_rate": 9.874801973018328e-05,
      "loss": 0.0971,
      "step": 6290
    },
    {
      "epoch": 0.23603461841070023,
      "grad_norm": 0.1784062534570694,
      "learning_rate": 9.874112237092949e-05,
      "loss": 0.0945,
      "step": 6300
    },
    {
      "epoch": 0.23640927653516167,
      "grad_norm": 0.24328432977199554,
      "learning_rate": 9.873420630672139e-05,
      "loss": 0.0927,
      "step": 6310
    },
    {
      "epoch": 0.2367839346596231,
      "grad_norm": 0.2238391786813736,
      "learning_rate": 9.872727154021309e-05,
      "loss": 0.0907,
      "step": 6320
    },
    {
      "epoch": 0.23715859278408452,
      "grad_norm": 0.25319069623947144,
      "learning_rate": 9.872031807406588e-05,
      "loss": 0.095,
      "step": 6330
    },
    {
      "epoch": 0.23753325090854596,
      "grad_norm": 0.1778222620487213,
      "learning_rate": 9.871334591094826e-05,
      "loss": 0.0956,
      "step": 6340
    },
    {
      "epoch": 0.23790790903300738,
      "grad_norm": 0.14090777933597565,
      "learning_rate": 9.870635505353582e-05,
      "loss": 0.0932,
      "step": 6350
    },
    {
      "epoch": 0.23828256715746882,
      "grad_norm": 0.1449834108352661,
      "learning_rate": 9.869934550451145e-05,
      "loss": 0.0939,
      "step": 6360
    },
    {
      "epoch": 0.23865722528193023,
      "grad_norm": 0.16361244022846222,
      "learning_rate": 9.869231726656508e-05,
      "loss": 0.0947,
      "step": 6370
    },
    {
      "epoch": 0.23903188340639167,
      "grad_norm": 0.12007420510053635,
      "learning_rate": 9.868527034239392e-05,
      "loss": 0.0902,
      "step": 6380
    },
    {
      "epoch": 0.23940654153085308,
      "grad_norm": 0.1296408325433731,
      "learning_rate": 9.867820473470229e-05,
      "loss": 0.097,
      "step": 6390
    },
    {
      "epoch": 0.23978119965531453,
      "grad_norm": 0.13839814066886902,
      "learning_rate": 9.867112044620168e-05,
      "loss": 0.0974,
      "step": 6400
    },
    {
      "epoch": 0.24015585777977597,
      "grad_norm": 0.12162594497203827,
      "learning_rate": 9.86640174796108e-05,
      "loss": 0.0957,
      "step": 6410
    },
    {
      "epoch": 0.24053051590423738,
      "grad_norm": 0.10749471187591553,
      "learning_rate": 9.865689583765547e-05,
      "loss": 0.0946,
      "step": 6420
    },
    {
      "epoch": 0.24090517402869882,
      "grad_norm": 0.10311616212129593,
      "learning_rate": 9.86497555230687e-05,
      "loss": 0.09,
      "step": 6430
    },
    {
      "epoch": 0.24127983215316023,
      "grad_norm": 0.21061602234840393,
      "learning_rate": 9.864259653859068e-05,
      "loss": 0.0916,
      "step": 6440
    },
    {
      "epoch": 0.24165449027762168,
      "grad_norm": 0.12086043506860733,
      "learning_rate": 9.863541888696874e-05,
      "loss": 0.0914,
      "step": 6450
    },
    {
      "epoch": 0.2420291484020831,
      "grad_norm": 0.14024969935417175,
      "learning_rate": 9.862822257095738e-05,
      "loss": 0.0969,
      "step": 6460
    },
    {
      "epoch": 0.24240380652654453,
      "grad_norm": 0.3900333344936371,
      "learning_rate": 9.86210075933183e-05,
      "loss": 0.0935,
      "step": 6470
    },
    {
      "epoch": 0.24277846465100597,
      "grad_norm": 0.17224101722240448,
      "learning_rate": 9.861377395682028e-05,
      "loss": 0.0942,
      "step": 6480
    },
    {
      "epoch": 0.24315312277546738,
      "grad_norm": 0.1385391801595688,
      "learning_rate": 9.860652166423932e-05,
      "loss": 0.0942,
      "step": 6490
    },
    {
      "epoch": 0.24352778089992883,
      "grad_norm": 0.11051100492477417,
      "learning_rate": 9.85992507183586e-05,
      "loss": 0.0895,
      "step": 6500
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 0.1621033251285553,
      "learning_rate": 9.85919611219684e-05,
      "loss": 0.0922,
      "step": 6510
    },
    {
      "epoch": 0.24427709714885168,
      "grad_norm": 0.1036863848567009,
      "learning_rate": 9.858465287786621e-05,
      "loss": 0.0947,
      "step": 6520
    },
    {
      "epoch": 0.2446517552733131,
      "grad_norm": 0.17263026535511017,
      "learning_rate": 9.857732598885662e-05,
      "loss": 0.093,
      "step": 6530
    },
    {
      "epoch": 0.24502641339777453,
      "grad_norm": 0.1013098657131195,
      "learning_rate": 9.856998045775143e-05,
      "loss": 0.089,
      "step": 6540
    },
    {
      "epoch": 0.24540107152223595,
      "grad_norm": 0.1864103227853775,
      "learning_rate": 9.856261628736956e-05,
      "loss": 0.0945,
      "step": 6550
    },
    {
      "epoch": 0.2457757296466974,
      "grad_norm": 0.13600587844848633,
      "learning_rate": 9.855523348053708e-05,
      "loss": 0.0933,
      "step": 6560
    },
    {
      "epoch": 0.24615038777115883,
      "grad_norm": 0.10915543884038925,
      "learning_rate": 9.854783204008725e-05,
      "loss": 0.089,
      "step": 6570
    },
    {
      "epoch": 0.24652504589562024,
      "grad_norm": 0.17229242622852325,
      "learning_rate": 9.854041196886045e-05,
      "loss": 0.0914,
      "step": 6580
    },
    {
      "epoch": 0.24689970402008168,
      "grad_norm": 0.1073223203420639,
      "learning_rate": 9.85329732697042e-05,
      "loss": 0.095,
      "step": 6590
    },
    {
      "epoch": 0.2472743621445431,
      "grad_norm": 0.15327045321464539,
      "learning_rate": 9.85255159454732e-05,
      "loss": 0.0942,
      "step": 6600
    },
    {
      "epoch": 0.24764902026900454,
      "grad_norm": 0.17277733981609344,
      "learning_rate": 9.851803999902928e-05,
      "loss": 0.0958,
      "step": 6610
    },
    {
      "epoch": 0.24802367839346595,
      "grad_norm": 0.1145314872264862,
      "learning_rate": 9.851054543324142e-05,
      "loss": 0.0933,
      "step": 6620
    },
    {
      "epoch": 0.2483983365179274,
      "grad_norm": 0.1108253076672554,
      "learning_rate": 9.850303225098574e-05,
      "loss": 0.0926,
      "step": 6630
    },
    {
      "epoch": 0.24877299464238883,
      "grad_norm": 0.13909533619880676,
      "learning_rate": 9.84955004551455e-05,
      "loss": 0.0926,
      "step": 6640
    },
    {
      "epoch": 0.24914765276685025,
      "grad_norm": 0.18839764595031738,
      "learning_rate": 9.848795004861112e-05,
      "loss": 0.0931,
      "step": 6650
    },
    {
      "epoch": 0.2495223108913117,
      "grad_norm": 0.12438805401325226,
      "learning_rate": 9.848038103428017e-05,
      "loss": 0.0914,
      "step": 6660
    },
    {
      "epoch": 0.2498969690157731,
      "grad_norm": 0.11024869233369827,
      "learning_rate": 9.84727934150573e-05,
      "loss": 0.0931,
      "step": 6670
    },
    {
      "epoch": 0.25027162714023454,
      "grad_norm": 0.22180306911468506,
      "learning_rate": 9.84651871938544e-05,
      "loss": 0.0904,
      "step": 6680
    },
    {
      "epoch": 0.25064628526469596,
      "grad_norm": 0.19306819140911102,
      "learning_rate": 9.84575623735904e-05,
      "loss": 0.0917,
      "step": 6690
    },
    {
      "epoch": 0.25102094338915737,
      "grad_norm": 0.15431010723114014,
      "learning_rate": 9.844991895719144e-05,
      "loss": 0.097,
      "step": 6700
    },
    {
      "epoch": 0.25139560151361884,
      "grad_norm": 0.10965581238269806,
      "learning_rate": 9.844225694759073e-05,
      "loss": 0.0906,
      "step": 6710
    },
    {
      "epoch": 0.25177025963808025,
      "grad_norm": 0.13244210183620453,
      "learning_rate": 9.84345763477287e-05,
      "loss": 0.0921,
      "step": 6720
    },
    {
      "epoch": 0.25214491776254166,
      "grad_norm": 0.11832816898822784,
      "learning_rate": 9.842687716055282e-05,
      "loss": 0.0919,
      "step": 6730
    },
    {
      "epoch": 0.25251957588700313,
      "grad_norm": 0.20697499811649323,
      "learning_rate": 9.841915938901778e-05,
      "loss": 0.0929,
      "step": 6740
    },
    {
      "epoch": 0.25289423401146455,
      "grad_norm": 0.16526684165000916,
      "learning_rate": 9.841142303608533e-05,
      "loss": 0.0933,
      "step": 6750
    },
    {
      "epoch": 0.25326889213592596,
      "grad_norm": 0.1501825451850891,
      "learning_rate": 9.840366810472442e-05,
      "loss": 0.0934,
      "step": 6760
    },
    {
      "epoch": 0.2536435502603874,
      "grad_norm": 0.1738421618938446,
      "learning_rate": 9.839589459791103e-05,
      "loss": 0.0915,
      "step": 6770
    },
    {
      "epoch": 0.25401820838484884,
      "grad_norm": 0.15137390792369843,
      "learning_rate": 9.83881025186284e-05,
      "loss": 0.0916,
      "step": 6780
    },
    {
      "epoch": 0.25439286650931026,
      "grad_norm": 0.19923122227191925,
      "learning_rate": 9.838029186986678e-05,
      "loss": 0.0946,
      "step": 6790
    },
    {
      "epoch": 0.25476752463377167,
      "grad_norm": 0.13870421051979065,
      "learning_rate": 9.837246265462361e-05,
      "loss": 0.0967,
      "step": 6800
    },
    {
      "epoch": 0.25514218275823314,
      "grad_norm": 0.22334936261177063,
      "learning_rate": 9.836461487590344e-05,
      "loss": 0.0935,
      "step": 6810
    },
    {
      "epoch": 0.25551684088269455,
      "grad_norm": 0.12730668485164642,
      "learning_rate": 9.835674853671797e-05,
      "loss": 0.0914,
      "step": 6820
    },
    {
      "epoch": 0.25589149900715596,
      "grad_norm": 0.16258865594863892,
      "learning_rate": 9.834886364008595e-05,
      "loss": 0.0948,
      "step": 6830
    },
    {
      "epoch": 0.2562661571316174,
      "grad_norm": 0.14554595947265625,
      "learning_rate": 9.834096018903333e-05,
      "loss": 0.0951,
      "step": 6840
    },
    {
      "epoch": 0.25664081525607885,
      "grad_norm": 0.13845399022102356,
      "learning_rate": 9.833303818659312e-05,
      "loss": 0.0969,
      "step": 6850
    },
    {
      "epoch": 0.25701547338054026,
      "grad_norm": 0.140767440199852,
      "learning_rate": 9.83250976358055e-05,
      "loss": 0.0893,
      "step": 6860
    },
    {
      "epoch": 0.2573901315050017,
      "grad_norm": 0.13696111738681793,
      "learning_rate": 9.831713853971774e-05,
      "loss": 0.0955,
      "step": 6870
    },
    {
      "epoch": 0.25776478962946314,
      "grad_norm": 0.16873674094676971,
      "learning_rate": 9.830916090138424e-05,
      "loss": 0.0916,
      "step": 6880
    },
    {
      "epoch": 0.25813944775392456,
      "grad_norm": 0.16681715846061707,
      "learning_rate": 9.83011647238665e-05,
      "loss": 0.0949,
      "step": 6890
    },
    {
      "epoch": 0.25851410587838597,
      "grad_norm": 0.1565077006816864,
      "learning_rate": 9.829315001023313e-05,
      "loss": 0.0951,
      "step": 6900
    },
    {
      "epoch": 0.2588887640028474,
      "grad_norm": 0.143308624625206,
      "learning_rate": 9.82851167635599e-05,
      "loss": 0.0936,
      "step": 6910
    },
    {
      "epoch": 0.25926342212730885,
      "grad_norm": 0.16069906949996948,
      "learning_rate": 9.827706498692964e-05,
      "loss": 0.0949,
      "step": 6920
    },
    {
      "epoch": 0.25963808025177026,
      "grad_norm": 0.2015884816646576,
      "learning_rate": 9.826899468343229e-05,
      "loss": 0.092,
      "step": 6930
    },
    {
      "epoch": 0.2600127383762317,
      "grad_norm": 0.12330088764429092,
      "learning_rate": 9.826090585616497e-05,
      "loss": 0.0945,
      "step": 6940
    },
    {
      "epoch": 0.2603873965006931,
      "grad_norm": 0.15032106637954712,
      "learning_rate": 9.825279850823179e-05,
      "loss": 0.094,
      "step": 6950
    },
    {
      "epoch": 0.26076205462515456,
      "grad_norm": 0.15191829204559326,
      "learning_rate": 9.824467264274409e-05,
      "loss": 0.0981,
      "step": 6960
    },
    {
      "epoch": 0.261136712749616,
      "grad_norm": 0.1014278382062912,
      "learning_rate": 9.823652826282026e-05,
      "loss": 0.0926,
      "step": 6970
    },
    {
      "epoch": 0.2615113708740774,
      "grad_norm": 0.17270131409168243,
      "learning_rate": 9.822836537158578e-05,
      "loss": 0.0952,
      "step": 6980
    },
    {
      "epoch": 0.26188602899853886,
      "grad_norm": 0.11906971037387848,
      "learning_rate": 9.822018397217323e-05,
      "loss": 0.0903,
      "step": 6990
    },
    {
      "epoch": 0.26226068712300027,
      "grad_norm": 0.14271943271160126,
      "learning_rate": 9.821198406772237e-05,
      "loss": 0.0929,
      "step": 7000
    },
    {
      "epoch": 0.2626353452474617,
      "grad_norm": 0.14282990992069244,
      "learning_rate": 9.820376566137995e-05,
      "loss": 0.0922,
      "step": 7010
    },
    {
      "epoch": 0.2630100033719231,
      "grad_norm": 0.13479405641555786,
      "learning_rate": 9.819552875629991e-05,
      "loss": 0.0914,
      "step": 7020
    },
    {
      "epoch": 0.26338466149638456,
      "grad_norm": 0.12594762444496155,
      "learning_rate": 9.818727335564326e-05,
      "loss": 0.0949,
      "step": 7030
    },
    {
      "epoch": 0.263759319620846,
      "grad_norm": 0.1566241979598999,
      "learning_rate": 9.817899946257807e-05,
      "loss": 0.0949,
      "step": 7040
    },
    {
      "epoch": 0.2641339777453074,
      "grad_norm": 0.15729470551013947,
      "learning_rate": 9.817070708027958e-05,
      "loss": 0.0933,
      "step": 7050
    },
    {
      "epoch": 0.26450863586976886,
      "grad_norm": 0.13461193442344666,
      "learning_rate": 9.816239621193005e-05,
      "loss": 0.0908,
      "step": 7060
    },
    {
      "epoch": 0.2648832939942303,
      "grad_norm": 0.12099846452474594,
      "learning_rate": 9.815406686071887e-05,
      "loss": 0.0939,
      "step": 7070
    },
    {
      "epoch": 0.2652579521186917,
      "grad_norm": 0.20308315753936768,
      "learning_rate": 9.814571902984256e-05,
      "loss": 0.093,
      "step": 7080
    },
    {
      "epoch": 0.2656326102431531,
      "grad_norm": 0.1571432203054428,
      "learning_rate": 9.813735272250465e-05,
      "loss": 0.0933,
      "step": 7090
    },
    {
      "epoch": 0.26600726836761457,
      "grad_norm": 0.14829021692276,
      "learning_rate": 9.812896794191583e-05,
      "loss": 0.0962,
      "step": 7100
    },
    {
      "epoch": 0.266381926492076,
      "grad_norm": 0.14098034799098969,
      "learning_rate": 9.812056469129385e-05,
      "loss": 0.0932,
      "step": 7110
    },
    {
      "epoch": 0.2667565846165374,
      "grad_norm": 0.1451452523469925,
      "learning_rate": 9.811214297386354e-05,
      "loss": 0.0931,
      "step": 7120
    },
    {
      "epoch": 0.26713124274099886,
      "grad_norm": 0.12920978665351868,
      "learning_rate": 9.810370279285685e-05,
      "loss": 0.0935,
      "step": 7130
    },
    {
      "epoch": 0.2675059008654603,
      "grad_norm": 0.1670100837945938,
      "learning_rate": 9.809524415151278e-05,
      "loss": 0.0911,
      "step": 7140
    },
    {
      "epoch": 0.2678805589899217,
      "grad_norm": 0.1402149647474289,
      "learning_rate": 9.808676705307742e-05,
      "loss": 0.0919,
      "step": 7150
    },
    {
      "epoch": 0.2682552171143831,
      "grad_norm": 0.14027123153209686,
      "learning_rate": 9.807827150080398e-05,
      "loss": 0.0912,
      "step": 7160
    },
    {
      "epoch": 0.2686298752388446,
      "grad_norm": 0.14009253680706024,
      "learning_rate": 9.80697574979527e-05,
      "loss": 0.0911,
      "step": 7170
    },
    {
      "epoch": 0.269004533363306,
      "grad_norm": 0.12006916105747223,
      "learning_rate": 9.806122504779094e-05,
      "loss": 0.0944,
      "step": 7180
    },
    {
      "epoch": 0.2693791914877674,
      "grad_norm": 0.1719730645418167,
      "learning_rate": 9.80526741535931e-05,
      "loss": 0.0949,
      "step": 7190
    },
    {
      "epoch": 0.26975384961222887,
      "grad_norm": 0.1921900510787964,
      "learning_rate": 9.80441048186407e-05,
      "loss": 0.0947,
      "step": 7200
    },
    {
      "epoch": 0.2701285077366903,
      "grad_norm": 0.16392651200294495,
      "learning_rate": 9.803551704622233e-05,
      "loss": 0.0962,
      "step": 7210
    },
    {
      "epoch": 0.2705031658611517,
      "grad_norm": 0.13350602984428406,
      "learning_rate": 9.802691083963363e-05,
      "loss": 0.0914,
      "step": 7220
    },
    {
      "epoch": 0.2708778239856131,
      "grad_norm": 0.11060172319412231,
      "learning_rate": 9.801828620217732e-05,
      "loss": 0.0928,
      "step": 7230
    },
    {
      "epoch": 0.2712524821100746,
      "grad_norm": 0.23223662376403809,
      "learning_rate": 9.800964313716321e-05,
      "loss": 0.0932,
      "step": 7240
    },
    {
      "epoch": 0.271627140234536,
      "grad_norm": 0.1147831454873085,
      "learning_rate": 9.800098164790818e-05,
      "loss": 0.0928,
      "step": 7250
    },
    {
      "epoch": 0.2720017983589974,
      "grad_norm": 0.14101803302764893,
      "learning_rate": 9.799230173773618e-05,
      "loss": 0.0935,
      "step": 7260
    },
    {
      "epoch": 0.2723764564834588,
      "grad_norm": 0.12928244471549988,
      "learning_rate": 9.798360340997818e-05,
      "loss": 0.0924,
      "step": 7270
    },
    {
      "epoch": 0.2727511146079203,
      "grad_norm": 0.11798913031816483,
      "learning_rate": 9.797488666797231e-05,
      "loss": 0.0941,
      "step": 7280
    },
    {
      "epoch": 0.2731257727323817,
      "grad_norm": 0.15879663825035095,
      "learning_rate": 9.79661515150637e-05,
      "loss": 0.092,
      "step": 7290
    },
    {
      "epoch": 0.2735004308568431,
      "grad_norm": 0.15203402936458588,
      "learning_rate": 9.795739795460457e-05,
      "loss": 0.0966,
      "step": 7300
    },
    {
      "epoch": 0.2738750889813046,
      "grad_norm": 0.12370128184556961,
      "learning_rate": 9.794862598995417e-05,
      "loss": 0.0925,
      "step": 7310
    },
    {
      "epoch": 0.274249747105766,
      "grad_norm": 0.103239506483078,
      "learning_rate": 9.79398356244789e-05,
      "loss": 0.0906,
      "step": 7320
    },
    {
      "epoch": 0.2746244052302274,
      "grad_norm": 0.1454005092382431,
      "learning_rate": 9.79310268615521e-05,
      "loss": 0.0907,
      "step": 7330
    },
    {
      "epoch": 0.2749990633546888,
      "grad_norm": 0.18687750399112701,
      "learning_rate": 9.792219970455425e-05,
      "loss": 0.0919,
      "step": 7340
    },
    {
      "epoch": 0.2753737214791503,
      "grad_norm": 0.12414173036813736,
      "learning_rate": 9.791335415687288e-05,
      "loss": 0.0926,
      "step": 7350
    },
    {
      "epoch": 0.2757483796036117,
      "grad_norm": 0.12147238105535507,
      "learning_rate": 9.790449022190258e-05,
      "loss": 0.0901,
      "step": 7360
    },
    {
      "epoch": 0.2761230377280731,
      "grad_norm": 0.3325405716896057,
      "learning_rate": 9.789560790304496e-05,
      "loss": 0.095,
      "step": 7370
    },
    {
      "epoch": 0.2764976958525346,
      "grad_norm": 0.13168932497501373,
      "learning_rate": 9.78867072037087e-05,
      "loss": 0.0929,
      "step": 7380
    },
    {
      "epoch": 0.276872353976996,
      "grad_norm": 0.09191998094320297,
      "learning_rate": 9.787778812730957e-05,
      "loss": 0.0966,
      "step": 7390
    },
    {
      "epoch": 0.2772470121014574,
      "grad_norm": 0.15152038633823395,
      "learning_rate": 9.786885067727038e-05,
      "loss": 0.0931,
      "step": 7400
    },
    {
      "epoch": 0.2776216702259188,
      "grad_norm": 0.12267857789993286,
      "learning_rate": 9.785989485702093e-05,
      "loss": 0.0959,
      "step": 7410
    },
    {
      "epoch": 0.2779963283503803,
      "grad_norm": 0.10816400498151779,
      "learning_rate": 9.785092066999817e-05,
      "loss": 0.0939,
      "step": 7420
    },
    {
      "epoch": 0.2783709864748417,
      "grad_norm": 0.1500183492898941,
      "learning_rate": 9.784192811964598e-05,
      "loss": 0.0962,
      "step": 7430
    },
    {
      "epoch": 0.2787456445993031,
      "grad_norm": 0.11446116864681244,
      "learning_rate": 9.783291720941543e-05,
      "loss": 0.0931,
      "step": 7440
    },
    {
      "epoch": 0.2791203027237646,
      "grad_norm": 0.260780394077301,
      "learning_rate": 9.78238879427645e-05,
      "loss": 0.0971,
      "step": 7450
    },
    {
      "epoch": 0.279494960848226,
      "grad_norm": 0.08794602006673813,
      "learning_rate": 9.781484032315829e-05,
      "loss": 0.0931,
      "step": 7460
    },
    {
      "epoch": 0.2798696189726874,
      "grad_norm": 0.15319396555423737,
      "learning_rate": 9.780577435406892e-05,
      "loss": 0.0955,
      "step": 7470
    },
    {
      "epoch": 0.28024427709714883,
      "grad_norm": 0.12090076506137848,
      "learning_rate": 9.779669003897558e-05,
      "loss": 0.0929,
      "step": 7480
    },
    {
      "epoch": 0.2806189352216103,
      "grad_norm": 0.1459214836359024,
      "learning_rate": 9.778758738136447e-05,
      "loss": 0.093,
      "step": 7490
    },
    {
      "epoch": 0.2809935933460717,
      "grad_norm": 0.1476815789937973,
      "learning_rate": 9.777846638472881e-05,
      "loss": 0.0926,
      "step": 7500
    },
    {
      "epoch": 0.2813682514705331,
      "grad_norm": 0.1825629025697708,
      "learning_rate": 9.776932705256891e-05,
      "loss": 0.0934,
      "step": 7510
    },
    {
      "epoch": 0.2817429095949946,
      "grad_norm": 0.15250566601753235,
      "learning_rate": 9.776016938839208e-05,
      "loss": 0.0915,
      "step": 7520
    },
    {
      "epoch": 0.282117567719456,
      "grad_norm": 0.1286335289478302,
      "learning_rate": 9.77509933957127e-05,
      "loss": 0.0949,
      "step": 7530
    },
    {
      "epoch": 0.2824922258439174,
      "grad_norm": 0.10560109466314316,
      "learning_rate": 9.774179907805214e-05,
      "loss": 0.0947,
      "step": 7540
    },
    {
      "epoch": 0.28286688396837883,
      "grad_norm": 0.2894872725009918,
      "learning_rate": 9.773258643893881e-05,
      "loss": 0.0891,
      "step": 7550
    },
    {
      "epoch": 0.2832415420928403,
      "grad_norm": 0.2657129466533661,
      "learning_rate": 9.772335548190819e-05,
      "loss": 0.0943,
      "step": 7560
    },
    {
      "epoch": 0.2836162002173017,
      "grad_norm": 0.20214955508708954,
      "learning_rate": 9.771410621050276e-05,
      "loss": 0.0977,
      "step": 7570
    },
    {
      "epoch": 0.28399085834176313,
      "grad_norm": 0.19322803616523743,
      "learning_rate": 9.770483862827202e-05,
      "loss": 0.0937,
      "step": 7580
    },
    {
      "epoch": 0.28436551646622454,
      "grad_norm": 0.17036008834838867,
      "learning_rate": 9.769555273877254e-05,
      "loss": 0.0949,
      "step": 7590
    },
    {
      "epoch": 0.284740174590686,
      "grad_norm": 0.10477631539106369,
      "learning_rate": 9.768624854556782e-05,
      "loss": 0.0971,
      "step": 7600
    },
    {
      "epoch": 0.2851148327151474,
      "grad_norm": 0.12381335347890854,
      "learning_rate": 9.767692605222852e-05,
      "loss": 0.0936,
      "step": 7610
    },
    {
      "epoch": 0.28548949083960884,
      "grad_norm": 0.11304149776697159,
      "learning_rate": 9.766758526233221e-05,
      "loss": 0.092,
      "step": 7620
    },
    {
      "epoch": 0.2858641489640703,
      "grad_norm": 0.1335333287715912,
      "learning_rate": 9.765822617946352e-05,
      "loss": 0.0937,
      "step": 7630
    },
    {
      "epoch": 0.2862388070885317,
      "grad_norm": 0.16177676618099213,
      "learning_rate": 9.764884880721414e-05,
      "loss": 0.0951,
      "step": 7640
    },
    {
      "epoch": 0.28661346521299313,
      "grad_norm": 0.19015614688396454,
      "learning_rate": 9.76394531491827e-05,
      "loss": 0.0954,
      "step": 7650
    },
    {
      "epoch": 0.28698812333745455,
      "grad_norm": 0.11093442887067795,
      "learning_rate": 9.763003920897493e-05,
      "loss": 0.0901,
      "step": 7660
    },
    {
      "epoch": 0.287362781461916,
      "grad_norm": 0.15712706744670868,
      "learning_rate": 9.76206069902035e-05,
      "loss": 0.0894,
      "step": 7670
    },
    {
      "epoch": 0.28773743958637743,
      "grad_norm": 0.1306014508008957,
      "learning_rate": 9.761115649648816e-05,
      "loss": 0.0893,
      "step": 7680
    },
    {
      "epoch": 0.28811209771083884,
      "grad_norm": 0.1275894045829773,
      "learning_rate": 9.760168773145561e-05,
      "loss": 0.0922,
      "step": 7690
    },
    {
      "epoch": 0.2884867558353003,
      "grad_norm": 0.14877477288246155,
      "learning_rate": 9.759220069873962e-05,
      "loss": 0.0946,
      "step": 7700
    },
    {
      "epoch": 0.2888614139597617,
      "grad_norm": 0.11590014398097992,
      "learning_rate": 9.758269540198093e-05,
      "loss": 0.0916,
      "step": 7710
    },
    {
      "epoch": 0.28923607208422314,
      "grad_norm": 0.14712698757648468,
      "learning_rate": 9.757317184482733e-05,
      "loss": 0.0912,
      "step": 7720
    },
    {
      "epoch": 0.28961073020868455,
      "grad_norm": 0.12686805427074432,
      "learning_rate": 9.756363003093357e-05,
      "loss": 0.0942,
      "step": 7730
    },
    {
      "epoch": 0.289985388333146,
      "grad_norm": 0.10649986565113068,
      "learning_rate": 9.755406996396142e-05,
      "loss": 0.0919,
      "step": 7740
    },
    {
      "epoch": 0.29036004645760743,
      "grad_norm": 0.1283608376979828,
      "learning_rate": 9.754449164757969e-05,
      "loss": 0.0929,
      "step": 7750
    },
    {
      "epoch": 0.29073470458206885,
      "grad_norm": 0.19152598083019257,
      "learning_rate": 9.753489508546417e-05,
      "loss": 0.0948,
      "step": 7760
    },
    {
      "epoch": 0.2911093627065303,
      "grad_norm": 0.1568550169467926,
      "learning_rate": 9.752528028129759e-05,
      "loss": 0.0929,
      "step": 7770
    },
    {
      "epoch": 0.29148402083099173,
      "grad_norm": 0.2109997421503067,
      "learning_rate": 9.751564723876981e-05,
      "loss": 0.0933,
      "step": 7780
    },
    {
      "epoch": 0.29185867895545314,
      "grad_norm": 0.12138497084379196,
      "learning_rate": 9.75059959615776e-05,
      "loss": 0.091,
      "step": 7790
    },
    {
      "epoch": 0.29223333707991456,
      "grad_norm": 0.10257386416196823,
      "learning_rate": 9.749632645342472e-05,
      "loss": 0.0903,
      "step": 7800
    },
    {
      "epoch": 0.292607995204376,
      "grad_norm": 0.1385193020105362,
      "learning_rate": 9.748663871802198e-05,
      "loss": 0.0929,
      "step": 7810
    },
    {
      "epoch": 0.29298265332883744,
      "grad_norm": 0.14461950957775116,
      "learning_rate": 9.747693275908713e-05,
      "loss": 0.0921,
      "step": 7820
    },
    {
      "epoch": 0.29335731145329885,
      "grad_norm": 0.125665083527565,
      "learning_rate": 9.746720858034497e-05,
      "loss": 0.092,
      "step": 7830
    },
    {
      "epoch": 0.2937319695777603,
      "grad_norm": 0.25533372163772583,
      "learning_rate": 9.745746618552725e-05,
      "loss": 0.0916,
      "step": 7840
    },
    {
      "epoch": 0.29410662770222173,
      "grad_norm": 0.08109455555677414,
      "learning_rate": 9.744770557837272e-05,
      "loss": 0.0948,
      "step": 7850
    },
    {
      "epoch": 0.29448128582668315,
      "grad_norm": 0.1248219907283783,
      "learning_rate": 9.743792676262714e-05,
      "loss": 0.0936,
      "step": 7860
    },
    {
      "epoch": 0.29485594395114456,
      "grad_norm": 0.15989041328430176,
      "learning_rate": 9.742812974204323e-05,
      "loss": 0.0908,
      "step": 7870
    },
    {
      "epoch": 0.29523060207560603,
      "grad_norm": 0.15806721150875092,
      "learning_rate": 9.741831452038068e-05,
      "loss": 0.094,
      "step": 7880
    },
    {
      "epoch": 0.29560526020006744,
      "grad_norm": 0.11908919364213943,
      "learning_rate": 9.740848110140623e-05,
      "loss": 0.0932,
      "step": 7890
    },
    {
      "epoch": 0.29597991832452886,
      "grad_norm": 0.10596932470798492,
      "learning_rate": 9.739862948889357e-05,
      "loss": 0.0947,
      "step": 7900
    },
    {
      "epoch": 0.29635457644899027,
      "grad_norm": 0.12206360697746277,
      "learning_rate": 9.738875968662334e-05,
      "loss": 0.0939,
      "step": 7910
    },
    {
      "epoch": 0.29672923457345174,
      "grad_norm": 0.11174952238798141,
      "learning_rate": 9.737887169838318e-05,
      "loss": 0.092,
      "step": 7920
    },
    {
      "epoch": 0.29710389269791315,
      "grad_norm": 0.11939036846160889,
      "learning_rate": 9.736896552796775e-05,
      "loss": 0.0948,
      "step": 7930
    },
    {
      "epoch": 0.29747855082237457,
      "grad_norm": 0.1505902260541916,
      "learning_rate": 9.735904117917863e-05,
      "loss": 0.0928,
      "step": 7940
    },
    {
      "epoch": 0.29785320894683603,
      "grad_norm": 0.16549137234687805,
      "learning_rate": 9.73490986558244e-05,
      "loss": 0.095,
      "step": 7950
    },
    {
      "epoch": 0.29822786707129745,
      "grad_norm": 0.12007515877485275,
      "learning_rate": 9.733913796172064e-05,
      "loss": 0.0919,
      "step": 7960
    },
    {
      "epoch": 0.29860252519575886,
      "grad_norm": 0.10164757817983627,
      "learning_rate": 9.732915910068986e-05,
      "loss": 0.0953,
      "step": 7970
    },
    {
      "epoch": 0.2989771833202203,
      "grad_norm": 0.11747173964977264,
      "learning_rate": 9.731916207656156e-05,
      "loss": 0.0929,
      "step": 7980
    },
    {
      "epoch": 0.29935184144468174,
      "grad_norm": 0.11292897909879684,
      "learning_rate": 9.730914689317221e-05,
      "loss": 0.0953,
      "step": 7990
    },
    {
      "epoch": 0.29972649956914316,
      "grad_norm": 0.15425926446914673,
      "learning_rate": 9.729911355436526e-05,
      "loss": 0.0911,
      "step": 8000
    },
    {
      "epoch": 0.30010115769360457,
      "grad_norm": 0.13514862954616547,
      "learning_rate": 9.728906206399111e-05,
      "loss": 0.0964,
      "step": 8010
    },
    {
      "epoch": 0.30047581581806604,
      "grad_norm": 0.11680793762207031,
      "learning_rate": 9.727899242590712e-05,
      "loss": 0.0958,
      "step": 8020
    },
    {
      "epoch": 0.30085047394252745,
      "grad_norm": 0.11339794844388962,
      "learning_rate": 9.726890464397764e-05,
      "loss": 0.0944,
      "step": 8030
    },
    {
      "epoch": 0.30122513206698887,
      "grad_norm": 0.1619844287633896,
      "learning_rate": 9.725879872207398e-05,
      "loss": 0.0942,
      "step": 8040
    },
    {
      "epoch": 0.3015997901914503,
      "grad_norm": 0.1825522631406784,
      "learning_rate": 9.724867466407439e-05,
      "loss": 0.0932,
      "step": 8050
    },
    {
      "epoch": 0.30197444831591175,
      "grad_norm": 0.10536281019449234,
      "learning_rate": 9.723853247386408e-05,
      "loss": 0.0911,
      "step": 8060
    },
    {
      "epoch": 0.30234910644037316,
      "grad_norm": 0.13221512734889984,
      "learning_rate": 9.722837215533525e-05,
      "loss": 0.0964,
      "step": 8070
    },
    {
      "epoch": 0.3027237645648346,
      "grad_norm": 0.15863053500652313,
      "learning_rate": 9.721819371238702e-05,
      "loss": 0.0934,
      "step": 8080
    },
    {
      "epoch": 0.30309842268929604,
      "grad_norm": 0.17698374390602112,
      "learning_rate": 9.720799714892549e-05,
      "loss": 0.0944,
      "step": 8090
    },
    {
      "epoch": 0.30347308081375746,
      "grad_norm": 0.11124274879693985,
      "learning_rate": 9.719778246886371e-05,
      "loss": 0.0938,
      "step": 8100
    },
    {
      "epoch": 0.30384773893821887,
      "grad_norm": 0.1255231648683548,
      "learning_rate": 9.718754967612167e-05,
      "loss": 0.0912,
      "step": 8110
    },
    {
      "epoch": 0.3042223970626803,
      "grad_norm": 0.15505003929138184,
      "learning_rate": 9.717729877462632e-05,
      "loss": 0.0955,
      "step": 8120
    },
    {
      "epoch": 0.30459705518714175,
      "grad_norm": 0.13588657975196838,
      "learning_rate": 9.716702976831157e-05,
      "loss": 0.0965,
      "step": 8130
    },
    {
      "epoch": 0.30497171331160317,
      "grad_norm": 0.12204322218894958,
      "learning_rate": 9.715674266111825e-05,
      "loss": 0.0915,
      "step": 8140
    },
    {
      "epoch": 0.3053463714360646,
      "grad_norm": 0.17687076330184937,
      "learning_rate": 9.714643745699417e-05,
      "loss": 0.0919,
      "step": 8150
    },
    {
      "epoch": 0.30572102956052605,
      "grad_norm": 0.23724476993083954,
      "learning_rate": 9.713611415989405e-05,
      "loss": 0.093,
      "step": 8160
    },
    {
      "epoch": 0.30609568768498746,
      "grad_norm": 0.12397628277540207,
      "learning_rate": 9.71257727737796e-05,
      "loss": 0.0954,
      "step": 8170
    },
    {
      "epoch": 0.3064703458094489,
      "grad_norm": 0.21014469861984253,
      "learning_rate": 9.711541330261942e-05,
      "loss": 0.0869,
      "step": 8180
    },
    {
      "epoch": 0.3068450039339103,
      "grad_norm": 0.10943572968244553,
      "learning_rate": 9.710503575038907e-05,
      "loss": 0.0942,
      "step": 8190
    },
    {
      "epoch": 0.30721966205837176,
      "grad_norm": 0.12433834373950958,
      "learning_rate": 9.709464012107106e-05,
      "loss": 0.0944,
      "step": 8200
    },
    {
      "epoch": 0.30759432018283317,
      "grad_norm": 0.15153895318508148,
      "learning_rate": 9.708422641865483e-05,
      "loss": 0.0926,
      "step": 8210
    },
    {
      "epoch": 0.3079689783072946,
      "grad_norm": 0.15698295831680298,
      "learning_rate": 9.707379464713676e-05,
      "loss": 0.0914,
      "step": 8220
    },
    {
      "epoch": 0.308343636431756,
      "grad_norm": 0.14230181276798248,
      "learning_rate": 9.706334481052015e-05,
      "loss": 0.0953,
      "step": 8230
    },
    {
      "epoch": 0.30871829455621747,
      "grad_norm": 0.13746927678585052,
      "learning_rate": 9.705287691281526e-05,
      "loss": 0.0937,
      "step": 8240
    },
    {
      "epoch": 0.3090929526806789,
      "grad_norm": 0.10555973649024963,
      "learning_rate": 9.704239095803924e-05,
      "loss": 0.0939,
      "step": 8250
    },
    {
      "epoch": 0.3094676108051403,
      "grad_norm": 0.11546996980905533,
      "learning_rate": 9.703188695021621e-05,
      "loss": 0.0932,
      "step": 8260
    },
    {
      "epoch": 0.30984226892960176,
      "grad_norm": 0.11550913751125336,
      "learning_rate": 9.702136489337721e-05,
      "loss": 0.095,
      "step": 8270
    },
    {
      "epoch": 0.3102169270540632,
      "grad_norm": 0.1507987380027771,
      "learning_rate": 9.701082479156016e-05,
      "loss": 0.0943,
      "step": 8280
    },
    {
      "epoch": 0.3105915851785246,
      "grad_norm": 0.1664668619632721,
      "learning_rate": 9.700026664880999e-05,
      "loss": 0.0927,
      "step": 8290
    },
    {
      "epoch": 0.310966243302986,
      "grad_norm": 0.12068402767181396,
      "learning_rate": 9.698969046917847e-05,
      "loss": 0.0934,
      "step": 8300
    },
    {
      "epoch": 0.31134090142744747,
      "grad_norm": 0.124041847884655,
      "learning_rate": 9.697909625672435e-05,
      "loss": 0.095,
      "step": 8310
    },
    {
      "epoch": 0.3117155595519089,
      "grad_norm": 0.11720738559961319,
      "learning_rate": 9.696848401551325e-05,
      "loss": 0.0927,
      "step": 8320
    },
    {
      "epoch": 0.3120902176763703,
      "grad_norm": 0.11900676041841507,
      "learning_rate": 9.695785374961777e-05,
      "loss": 0.0944,
      "step": 8330
    },
    {
      "epoch": 0.31246487580083177,
      "grad_norm": 0.10458896309137344,
      "learning_rate": 9.694720546311738e-05,
      "loss": 0.091,
      "step": 8340
    },
    {
      "epoch": 0.3128395339252932,
      "grad_norm": 0.2802934944629669,
      "learning_rate": 9.693653916009847e-05,
      "loss": 0.094,
      "step": 8350
    },
    {
      "epoch": 0.3132141920497546,
      "grad_norm": 0.10526041686534882,
      "learning_rate": 9.692585484465437e-05,
      "loss": 0.0932,
      "step": 8360
    },
    {
      "epoch": 0.313588850174216,
      "grad_norm": 0.09069077670574188,
      "learning_rate": 9.69151525208853e-05,
      "loss": 0.0953,
      "step": 8370
    },
    {
      "epoch": 0.3139635082986775,
      "grad_norm": 0.16700516641139984,
      "learning_rate": 9.69044321928984e-05,
      "loss": 0.0949,
      "step": 8380
    },
    {
      "epoch": 0.3143381664231389,
      "grad_norm": 0.13587039709091187,
      "learning_rate": 9.689369386480773e-05,
      "loss": 0.0967,
      "step": 8390
    },
    {
      "epoch": 0.3147128245476003,
      "grad_norm": 0.10344687104225159,
      "learning_rate": 9.688293754073421e-05,
      "loss": 0.0921,
      "step": 8400
    },
    {
      "epoch": 0.31508748267206177,
      "grad_norm": 0.10494478791952133,
      "learning_rate": 9.687216322480572e-05,
      "loss": 0.0844,
      "step": 8410
    },
    {
      "epoch": 0.3154621407965232,
      "grad_norm": 0.10167830437421799,
      "learning_rate": 9.686137092115701e-05,
      "loss": 0.091,
      "step": 8420
    },
    {
      "epoch": 0.3158367989209846,
      "grad_norm": 0.10891540348529816,
      "learning_rate": 9.685056063392979e-05,
      "loss": 0.0944,
      "step": 8430
    },
    {
      "epoch": 0.316211457045446,
      "grad_norm": 0.11266051232814789,
      "learning_rate": 9.683973236727258e-05,
      "loss": 0.0942,
      "step": 8440
    },
    {
      "epoch": 0.3165861151699075,
      "grad_norm": 0.09321271628141403,
      "learning_rate": 9.682888612534089e-05,
      "loss": 0.0902,
      "step": 8450
    },
    {
      "epoch": 0.3169607732943689,
      "grad_norm": 0.08729838579893112,
      "learning_rate": 9.681802191229704e-05,
      "loss": 0.0895,
      "step": 8460
    },
    {
      "epoch": 0.3173354314188303,
      "grad_norm": 0.14923198521137238,
      "learning_rate": 9.680713973231034e-05,
      "loss": 0.0889,
      "step": 8470
    },
    {
      "epoch": 0.3177100895432918,
      "grad_norm": 0.15033842623233795,
      "learning_rate": 9.679623958955692e-05,
      "loss": 0.0943,
      "step": 8480
    },
    {
      "epoch": 0.3180847476677532,
      "grad_norm": 0.15238574147224426,
      "learning_rate": 9.678532148821986e-05,
      "loss": 0.0919,
      "step": 8490
    },
    {
      "epoch": 0.3184594057922146,
      "grad_norm": 0.130801722407341,
      "learning_rate": 9.677438543248907e-05,
      "loss": 0.096,
      "step": 8500
    },
    {
      "epoch": 0.318834063916676,
      "grad_norm": 0.10695193707942963,
      "learning_rate": 9.676343142656141e-05,
      "loss": 0.0937,
      "step": 8510
    },
    {
      "epoch": 0.3192087220411375,
      "grad_norm": 0.12810027599334717,
      "learning_rate": 9.67524594746406e-05,
      "loss": 0.0902,
      "step": 8520
    },
    {
      "epoch": 0.3195833801655989,
      "grad_norm": 0.09129580110311508,
      "learning_rate": 9.674146958093723e-05,
      "loss": 0.0947,
      "step": 8530
    },
    {
      "epoch": 0.3199580382900603,
      "grad_norm": 0.14437805116176605,
      "learning_rate": 9.673046174966883e-05,
      "loss": 0.0973,
      "step": 8540
    },
    {
      "epoch": 0.3203326964145217,
      "grad_norm": 0.14947627484798431,
      "learning_rate": 9.671943598505975e-05,
      "loss": 0.0938,
      "step": 8550
    },
    {
      "epoch": 0.3207073545389832,
      "grad_norm": 0.12765394151210785,
      "learning_rate": 9.670839229134127e-05,
      "loss": 0.0919,
      "step": 8560
    },
    {
      "epoch": 0.3210820126634446,
      "grad_norm": 0.252959668636322,
      "learning_rate": 9.669733067275151e-05,
      "loss": 0.0945,
      "step": 8570
    },
    {
      "epoch": 0.321456670787906,
      "grad_norm": 0.22820943593978882,
      "learning_rate": 9.66862511335355e-05,
      "loss": 0.0907,
      "step": 8580
    },
    {
      "epoch": 0.3218313289123675,
      "grad_norm": 0.10623115301132202,
      "learning_rate": 9.667515367794516e-05,
      "loss": 0.0899,
      "step": 8590
    },
    {
      "epoch": 0.3222059870368289,
      "grad_norm": 0.1402115672826767,
      "learning_rate": 9.666403831023923e-05,
      "loss": 0.0926,
      "step": 8600
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 0.10774970799684525,
      "learning_rate": 9.665290503468338e-05,
      "loss": 0.0912,
      "step": 8610
    },
    {
      "epoch": 0.3229553032857517,
      "grad_norm": 0.09958545863628387,
      "learning_rate": 9.664175385555013e-05,
      "loss": 0.0911,
      "step": 8620
    },
    {
      "epoch": 0.3233299614102132,
      "grad_norm": 0.1428644061088562,
      "learning_rate": 9.663058477711883e-05,
      "loss": 0.0915,
      "step": 8630
    },
    {
      "epoch": 0.3237046195346746,
      "grad_norm": 0.07905969023704529,
      "learning_rate": 9.661939780367579e-05,
      "loss": 0.0903,
      "step": 8640
    },
    {
      "epoch": 0.324079277659136,
      "grad_norm": 0.11032506078481674,
      "learning_rate": 9.660819293951413e-05,
      "loss": 0.0987,
      "step": 8650
    },
    {
      "epoch": 0.3244539357835975,
      "grad_norm": 0.1405649036169052,
      "learning_rate": 9.659697018893384e-05,
      "loss": 0.0962,
      "step": 8660
    },
    {
      "epoch": 0.3248285939080589,
      "grad_norm": 0.10318712890148163,
      "learning_rate": 9.658572955624174e-05,
      "loss": 0.0944,
      "step": 8670
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 0.11707144230604172,
      "learning_rate": 9.657447104575159e-05,
      "loss": 0.0926,
      "step": 8680
    },
    {
      "epoch": 0.32557791015698173,
      "grad_norm": 0.13206548988819122,
      "learning_rate": 9.656319466178396e-05,
      "loss": 0.0955,
      "step": 8690
    },
    {
      "epoch": 0.3259525682814432,
      "grad_norm": 0.09004028886556625,
      "learning_rate": 9.655190040866628e-05,
      "loss": 0.0887,
      "step": 8700
    },
    {
      "epoch": 0.3263272264059046,
      "grad_norm": 0.12546810507774353,
      "learning_rate": 9.654058829073287e-05,
      "loss": 0.091,
      "step": 8710
    },
    {
      "epoch": 0.326701884530366,
      "grad_norm": 0.15538644790649414,
      "learning_rate": 9.652925831232484e-05,
      "loss": 0.097,
      "step": 8720
    },
    {
      "epoch": 0.3270765426548275,
      "grad_norm": 0.11503379791975021,
      "learning_rate": 9.651791047779025e-05,
      "loss": 0.0941,
      "step": 8730
    },
    {
      "epoch": 0.3274512007792889,
      "grad_norm": 0.10563817620277405,
      "learning_rate": 9.650654479148393e-05,
      "loss": 0.0899,
      "step": 8740
    },
    {
      "epoch": 0.3278258589037503,
      "grad_norm": 0.10675156116485596,
      "learning_rate": 9.649516125776757e-05,
      "loss": 0.0938,
      "step": 8750
    },
    {
      "epoch": 0.32820051702821174,
      "grad_norm": 0.24369460344314575,
      "learning_rate": 9.648375988100978e-05,
      "loss": 0.0945,
      "step": 8760
    },
    {
      "epoch": 0.3285751751526732,
      "grad_norm": 0.39676564931869507,
      "learning_rate": 9.647234066558593e-05,
      "loss": 0.0942,
      "step": 8770
    },
    {
      "epoch": 0.3289498332771346,
      "grad_norm": 0.09564889222383499,
      "learning_rate": 9.646090361587827e-05,
      "loss": 0.0927,
      "step": 8780
    },
    {
      "epoch": 0.32932449140159603,
      "grad_norm": 0.12912766635417938,
      "learning_rate": 9.64494487362759e-05,
      "loss": 0.0956,
      "step": 8790
    },
    {
      "epoch": 0.3296991495260575,
      "grad_norm": 0.1554146409034729,
      "learning_rate": 9.64379760311748e-05,
      "loss": 0.0954,
      "step": 8800
    },
    {
      "epoch": 0.3300738076505189,
      "grad_norm": 0.12312492728233337,
      "learning_rate": 9.642648550497768e-05,
      "loss": 0.0913,
      "step": 8810
    },
    {
      "epoch": 0.3304484657749803,
      "grad_norm": 0.09754779934883118,
      "learning_rate": 9.641497716209422e-05,
      "loss": 0.0912,
      "step": 8820
    },
    {
      "epoch": 0.33082312389944174,
      "grad_norm": 0.16966478526592255,
      "learning_rate": 9.640345100694084e-05,
      "loss": 0.0917,
      "step": 8830
    },
    {
      "epoch": 0.3311977820239032,
      "grad_norm": 0.10406249016523361,
      "learning_rate": 9.639190704394086e-05,
      "loss": 0.0884,
      "step": 8840
    },
    {
      "epoch": 0.3315724401483646,
      "grad_norm": 0.134780615568161,
      "learning_rate": 9.638034527752437e-05,
      "loss": 0.092,
      "step": 8850
    },
    {
      "epoch": 0.33194709827282604,
      "grad_norm": 0.12781772017478943,
      "learning_rate": 9.636876571212834e-05,
      "loss": 0.0922,
      "step": 8860
    },
    {
      "epoch": 0.33232175639728745,
      "grad_norm": 0.1021408885717392,
      "learning_rate": 9.635716835219658e-05,
      "loss": 0.0891,
      "step": 8870
    },
    {
      "epoch": 0.3326964145217489,
      "grad_norm": 0.12573780119419098,
      "learning_rate": 9.634555320217969e-05,
      "loss": 0.0926,
      "step": 8880
    },
    {
      "epoch": 0.33307107264621033,
      "grad_norm": 0.11462800949811935,
      "learning_rate": 9.633392026653511e-05,
      "loss": 0.0896,
      "step": 8890
    },
    {
      "epoch": 0.33344573077067174,
      "grad_norm": 0.10742399841547012,
      "learning_rate": 9.632226954972713e-05,
      "loss": 0.0895,
      "step": 8900
    },
    {
      "epoch": 0.3338203888951332,
      "grad_norm": 0.12235739082098007,
      "learning_rate": 9.631060105622682e-05,
      "loss": 0.0912,
      "step": 8910
    },
    {
      "epoch": 0.3341950470195946,
      "grad_norm": 0.1090962365269661,
      "learning_rate": 9.629891479051212e-05,
      "loss": 0.0954,
      "step": 8920
    },
    {
      "epoch": 0.33456970514405604,
      "grad_norm": 0.14752410352230072,
      "learning_rate": 9.628721075706774e-05,
      "loss": 0.0917,
      "step": 8930
    },
    {
      "epoch": 0.33494436326851745,
      "grad_norm": 0.11413836479187012,
      "learning_rate": 9.627548896038525e-05,
      "loss": 0.0969,
      "step": 8940
    },
    {
      "epoch": 0.3353190213929789,
      "grad_norm": 0.15504060685634613,
      "learning_rate": 9.626374940496302e-05,
      "loss": 0.0949,
      "step": 8950
    },
    {
      "epoch": 0.33569367951744034,
      "grad_norm": 0.13611474633216858,
      "learning_rate": 9.625199209530623e-05,
      "loss": 0.0932,
      "step": 8960
    },
    {
      "epoch": 0.33606833764190175,
      "grad_norm": 0.09801104664802551,
      "learning_rate": 9.62402170359269e-05,
      "loss": 0.0985,
      "step": 8970
    },
    {
      "epoch": 0.3364429957663632,
      "grad_norm": 0.12886030972003937,
      "learning_rate": 9.622842423134382e-05,
      "loss": 0.0939,
      "step": 8980
    },
    {
      "epoch": 0.33681765389082463,
      "grad_norm": 0.11980655044317245,
      "learning_rate": 9.62166136860826e-05,
      "loss": 0.0934,
      "step": 8990
    },
    {
      "epoch": 0.33719231201528604,
      "grad_norm": 0.09756673127412796,
      "learning_rate": 9.62047854046757e-05,
      "loss": 0.0929,
      "step": 9000
    },
    {
      "epoch": 0.33756697013974746,
      "grad_norm": 0.11320918798446655,
      "learning_rate": 9.619293939166233e-05,
      "loss": 0.0962,
      "step": 9010
    },
    {
      "epoch": 0.3379416282642089,
      "grad_norm": 0.11353982985019684,
      "learning_rate": 9.618107565158858e-05,
      "loss": 0.0927,
      "step": 9020
    },
    {
      "epoch": 0.33831628638867034,
      "grad_norm": 0.12666556239128113,
      "learning_rate": 9.616919418900723e-05,
      "loss": 0.0943,
      "step": 9030
    },
    {
      "epoch": 0.33869094451313175,
      "grad_norm": 0.11129698157310486,
      "learning_rate": 9.615729500847797e-05,
      "loss": 0.0908,
      "step": 9040
    },
    {
      "epoch": 0.3390656026375932,
      "grad_norm": 0.09673140197992325,
      "learning_rate": 9.614537811456723e-05,
      "loss": 0.094,
      "step": 9050
    },
    {
      "epoch": 0.33944026076205464,
      "grad_norm": 0.10981817543506622,
      "learning_rate": 9.613344351184823e-05,
      "loss": 0.092,
      "step": 9060
    },
    {
      "epoch": 0.33981491888651605,
      "grad_norm": 0.1753319352865219,
      "learning_rate": 9.612149120490104e-05,
      "loss": 0.0944,
      "step": 9070
    },
    {
      "epoch": 0.34018957701097746,
      "grad_norm": 0.20248432457447052,
      "learning_rate": 9.610952119831247e-05,
      "loss": 0.0922,
      "step": 9080
    },
    {
      "epoch": 0.34056423513543893,
      "grad_norm": 0.12087363749742508,
      "learning_rate": 9.609753349667616e-05,
      "loss": 0.091,
      "step": 9090
    },
    {
      "epoch": 0.34093889325990034,
      "grad_norm": 0.10867036879062653,
      "learning_rate": 9.608552810459253e-05,
      "loss": 0.0922,
      "step": 9100
    },
    {
      "epoch": 0.34131355138436176,
      "grad_norm": 0.10084300488233566,
      "learning_rate": 9.607350502666876e-05,
      "loss": 0.0924,
      "step": 9110
    },
    {
      "epoch": 0.3416882095088232,
      "grad_norm": 0.2120368480682373,
      "learning_rate": 9.606146426751888e-05,
      "loss": 0.093,
      "step": 9120
    },
    {
      "epoch": 0.34206286763328464,
      "grad_norm": 0.10647203028202057,
      "learning_rate": 9.604940583176362e-05,
      "loss": 0.0902,
      "step": 9130
    },
    {
      "epoch": 0.34243752575774605,
      "grad_norm": 0.11403804272413254,
      "learning_rate": 9.603732972403058e-05,
      "loss": 0.0931,
      "step": 9140
    },
    {
      "epoch": 0.34281218388220747,
      "grad_norm": 0.11068258434534073,
      "learning_rate": 9.602523594895408e-05,
      "loss": 0.0939,
      "step": 9150
    },
    {
      "epoch": 0.34318684200666894,
      "grad_norm": 0.1318136304616928,
      "learning_rate": 9.601312451117525e-05,
      "loss": 0.0925,
      "step": 9160
    },
    {
      "epoch": 0.34356150013113035,
      "grad_norm": 0.1414923369884491,
      "learning_rate": 9.600099541534198e-05,
      "loss": 0.0905,
      "step": 9170
    },
    {
      "epoch": 0.34393615825559176,
      "grad_norm": 0.16862548887729645,
      "learning_rate": 9.598884866610896e-05,
      "loss": 0.0899,
      "step": 9180
    },
    {
      "epoch": 0.3443108163800532,
      "grad_norm": 0.11083747446537018,
      "learning_rate": 9.597668426813764e-05,
      "loss": 0.0905,
      "step": 9190
    },
    {
      "epoch": 0.34468547450451464,
      "grad_norm": 0.11086270958185196,
      "learning_rate": 9.596450222609626e-05,
      "loss": 0.0961,
      "step": 9200
    },
    {
      "epoch": 0.34506013262897606,
      "grad_norm": 0.10852617025375366,
      "learning_rate": 9.595230254465977e-05,
      "loss": 0.094,
      "step": 9210
    },
    {
      "epoch": 0.34543479075343747,
      "grad_norm": 0.10809492319822311,
      "learning_rate": 9.594008522850997e-05,
      "loss": 0.0897,
      "step": 9220
    },
    {
      "epoch": 0.34580944887789894,
      "grad_norm": 0.09008210897445679,
      "learning_rate": 9.592785028233539e-05,
      "loss": 0.0927,
      "step": 9230
    },
    {
      "epoch": 0.34618410700236035,
      "grad_norm": 0.08886649459600449,
      "learning_rate": 9.591559771083133e-05,
      "loss": 0.0914,
      "step": 9240
    },
    {
      "epoch": 0.34655876512682177,
      "grad_norm": 0.09660165756940842,
      "learning_rate": 9.590332751869984e-05,
      "loss": 0.0935,
      "step": 9250
    },
    {
      "epoch": 0.3469334232512832,
      "grad_norm": 0.10575222969055176,
      "learning_rate": 9.589103971064975e-05,
      "loss": 0.0947,
      "step": 9260
    },
    {
      "epoch": 0.34730808137574465,
      "grad_norm": 0.11694425344467163,
      "learning_rate": 9.587873429139664e-05,
      "loss": 0.0927,
      "step": 9270
    },
    {
      "epoch": 0.34768273950020606,
      "grad_norm": 0.11112310737371445,
      "learning_rate": 9.586641126566288e-05,
      "loss": 0.0878,
      "step": 9280
    },
    {
      "epoch": 0.3480573976246675,
      "grad_norm": 0.0955679789185524,
      "learning_rate": 9.585407063817753e-05,
      "loss": 0.0955,
      "step": 9290
    },
    {
      "epoch": 0.34843205574912894,
      "grad_norm": 0.15835562348365784,
      "learning_rate": 9.584171241367645e-05,
      "loss": 0.0935,
      "step": 9300
    },
    {
      "epoch": 0.34880671387359036,
      "grad_norm": 0.11486668139696121,
      "learning_rate": 9.582933659690228e-05,
      "loss": 0.0899,
      "step": 9310
    },
    {
      "epoch": 0.34918137199805177,
      "grad_norm": 0.4399772882461548,
      "learning_rate": 9.581694319260436e-05,
      "loss": 0.0939,
      "step": 9320
    },
    {
      "epoch": 0.3495560301225132,
      "grad_norm": 0.17749479413032532,
      "learning_rate": 9.58045322055388e-05,
      "loss": 0.0915,
      "step": 9330
    },
    {
      "epoch": 0.34993068824697465,
      "grad_norm": 0.14912426471710205,
      "learning_rate": 9.579210364046845e-05,
      "loss": 0.0932,
      "step": 9340
    },
    {
      "epoch": 0.35030534637143607,
      "grad_norm": 0.10746533423662186,
      "learning_rate": 9.577965750216291e-05,
      "loss": 0.0943,
      "step": 9350
    },
    {
      "epoch": 0.3506800044958975,
      "grad_norm": 0.21753212809562683,
      "learning_rate": 9.576719379539853e-05,
      "loss": 0.0932,
      "step": 9360
    },
    {
      "epoch": 0.35105466262035895,
      "grad_norm": 0.11720292270183563,
      "learning_rate": 9.57547125249584e-05,
      "loss": 0.0891,
      "step": 9370
    },
    {
      "epoch": 0.35142932074482036,
      "grad_norm": 0.09955815225839615,
      "learning_rate": 9.574221369563236e-05,
      "loss": 0.0936,
      "step": 9380
    },
    {
      "epoch": 0.3518039788692818,
      "grad_norm": 0.1045479029417038,
      "learning_rate": 9.572969731221695e-05,
      "loss": 0.0925,
      "step": 9390
    },
    {
      "epoch": 0.3521786369937432,
      "grad_norm": 0.091173455119133,
      "learning_rate": 9.57171633795155e-05,
      "loss": 0.0903,
      "step": 9400
    },
    {
      "epoch": 0.35255329511820466,
      "grad_norm": 0.20661139488220215,
      "learning_rate": 9.570461190233802e-05,
      "loss": 0.0953,
      "step": 9410
    },
    {
      "epoch": 0.35292795324266607,
      "grad_norm": 0.10097607970237732,
      "learning_rate": 9.569204288550131e-05,
      "loss": 0.0901,
      "step": 9420
    },
    {
      "epoch": 0.3533026113671275,
      "grad_norm": 0.11262395977973938,
      "learning_rate": 9.567945633382884e-05,
      "loss": 0.0912,
      "step": 9430
    },
    {
      "epoch": 0.35367726949158895,
      "grad_norm": 0.13165776431560516,
      "learning_rate": 9.566685225215087e-05,
      "loss": 0.0904,
      "step": 9440
    },
    {
      "epoch": 0.35405192761605037,
      "grad_norm": 0.10356539487838745,
      "learning_rate": 9.565423064530434e-05,
      "loss": 0.0909,
      "step": 9450
    },
    {
      "epoch": 0.3544265857405118,
      "grad_norm": 0.12295573204755783,
      "learning_rate": 9.564159151813294e-05,
      "loss": 0.0941,
      "step": 9460
    },
    {
      "epoch": 0.3548012438649732,
      "grad_norm": 0.11899244785308838,
      "learning_rate": 9.562893487548707e-05,
      "loss": 0.0899,
      "step": 9470
    },
    {
      "epoch": 0.35517590198943466,
      "grad_norm": 0.11658161878585815,
      "learning_rate": 9.561626072222388e-05,
      "loss": 0.0911,
      "step": 9480
    },
    {
      "epoch": 0.3555505601138961,
      "grad_norm": 0.11734439432621002,
      "learning_rate": 9.56035690632072e-05,
      "loss": 0.0884,
      "step": 9490
    },
    {
      "epoch": 0.3559252182383575,
      "grad_norm": 0.10218203812837601,
      "learning_rate": 9.55908599033076e-05,
      "loss": 0.0964,
      "step": 9500
    },
    {
      "epoch": 0.3562998763628189,
      "grad_norm": 0.1104339212179184,
      "learning_rate": 9.557813324740237e-05,
      "loss": 0.0913,
      "step": 9510
    },
    {
      "epoch": 0.35667453448728037,
      "grad_norm": 0.11425624042749405,
      "learning_rate": 9.556538910037549e-05,
      "loss": 0.0925,
      "step": 9520
    },
    {
      "epoch": 0.3570491926117418,
      "grad_norm": 0.11836150288581848,
      "learning_rate": 9.555262746711769e-05,
      "loss": 0.0927,
      "step": 9530
    },
    {
      "epoch": 0.3574238507362032,
      "grad_norm": 0.11556129157543182,
      "learning_rate": 9.553984835252638e-05,
      "loss": 0.093,
      "step": 9540
    },
    {
      "epoch": 0.35779850886066467,
      "grad_norm": 0.1007576435804367,
      "learning_rate": 9.552705176150571e-05,
      "loss": 0.0926,
      "step": 9550
    },
    {
      "epoch": 0.3581731669851261,
      "grad_norm": 0.09986203163862228,
      "learning_rate": 9.551423769896647e-05,
      "loss": 0.0888,
      "step": 9560
    },
    {
      "epoch": 0.3585478251095875,
      "grad_norm": 0.20770300924777985,
      "learning_rate": 9.550140616982625e-05,
      "loss": 0.0919,
      "step": 9570
    },
    {
      "epoch": 0.3589224832340489,
      "grad_norm": 0.12668824195861816,
      "learning_rate": 9.548855717900926e-05,
      "loss": 0.0935,
      "step": 9580
    },
    {
      "epoch": 0.3592971413585104,
      "grad_norm": 0.12931814789772034,
      "learning_rate": 9.547569073144646e-05,
      "loss": 0.0946,
      "step": 9590
    },
    {
      "epoch": 0.3596717994829718,
      "grad_norm": 0.13826701045036316,
      "learning_rate": 9.546280683207549e-05,
      "loss": 0.0928,
      "step": 9600
    },
    {
      "epoch": 0.3600464576074332,
      "grad_norm": 0.11651083827018738,
      "learning_rate": 9.54499054858407e-05,
      "loss": 0.0906,
      "step": 9610
    },
    {
      "epoch": 0.36042111573189467,
      "grad_norm": 0.09559416770935059,
      "learning_rate": 9.543698669769311e-05,
      "loss": 0.0926,
      "step": 9620
    },
    {
      "epoch": 0.3607957738563561,
      "grad_norm": 0.21472400426864624,
      "learning_rate": 9.542405047259046e-05,
      "loss": 0.0917,
      "step": 9630
    },
    {
      "epoch": 0.3611704319808175,
      "grad_norm": 0.09075400978326797,
      "learning_rate": 9.541109681549719e-05,
      "loss": 0.0899,
      "step": 9640
    },
    {
      "epoch": 0.3615450901052789,
      "grad_norm": 0.13545945286750793,
      "learning_rate": 9.539812573138435e-05,
      "loss": 0.0925,
      "step": 9650
    },
    {
      "epoch": 0.3619197482297404,
      "grad_norm": 0.10889790952205658,
      "learning_rate": 9.538513722522982e-05,
      "loss": 0.093,
      "step": 9660
    },
    {
      "epoch": 0.3622944063542018,
      "grad_norm": 0.1296359747648239,
      "learning_rate": 9.537213130201804e-05,
      "loss": 0.0905,
      "step": 9670
    },
    {
      "epoch": 0.3626690644786632,
      "grad_norm": 0.20582318305969238,
      "learning_rate": 9.53591079667402e-05,
      "loss": 0.093,
      "step": 9680
    },
    {
      "epoch": 0.3630437226031247,
      "grad_norm": 0.1344773918390274,
      "learning_rate": 9.534606722439412e-05,
      "loss": 0.0936,
      "step": 9690
    },
    {
      "epoch": 0.3634183807275861,
      "grad_norm": 0.10152741521596909,
      "learning_rate": 9.533300907998435e-05,
      "loss": 0.0953,
      "step": 9700
    },
    {
      "epoch": 0.3637930388520475,
      "grad_norm": 0.12237554043531418,
      "learning_rate": 9.53199335385221e-05,
      "loss": 0.0918,
      "step": 9710
    },
    {
      "epoch": 0.3641676969765089,
      "grad_norm": 0.12210015952587128,
      "learning_rate": 9.530684060502526e-05,
      "loss": 0.0959,
      "step": 9720
    },
    {
      "epoch": 0.3645423551009704,
      "grad_norm": 0.10907015204429626,
      "learning_rate": 9.52937302845184e-05,
      "loss": 0.0944,
      "step": 9730
    },
    {
      "epoch": 0.3649170132254318,
      "grad_norm": 0.09489703923463821,
      "learning_rate": 9.528060258203273e-05,
      "loss": 0.0913,
      "step": 9740
    },
    {
      "epoch": 0.3652916713498932,
      "grad_norm": 0.119943767786026,
      "learning_rate": 9.526745750260616e-05,
      "loss": 0.0927,
      "step": 9750
    },
    {
      "epoch": 0.3656663294743547,
      "grad_norm": 0.11156091839075089,
      "learning_rate": 9.525429505128327e-05,
      "loss": 0.0927,
      "step": 9760
    },
    {
      "epoch": 0.3660409875988161,
      "grad_norm": 0.09343620389699936,
      "learning_rate": 9.524111523311529e-05,
      "loss": 0.0917,
      "step": 9770
    },
    {
      "epoch": 0.3664156457232775,
      "grad_norm": 0.10615233331918716,
      "learning_rate": 9.522791805316015e-05,
      "loss": 0.0954,
      "step": 9780
    },
    {
      "epoch": 0.3667903038477389,
      "grad_norm": 0.12959757447242737,
      "learning_rate": 9.521470351648239e-05,
      "loss": 0.0947,
      "step": 9790
    },
    {
      "epoch": 0.3671649619722004,
      "grad_norm": 0.09100155532360077,
      "learning_rate": 9.520147162815322e-05,
      "loss": 0.0938,
      "step": 9800
    },
    {
      "epoch": 0.3675396200966618,
      "grad_norm": 0.16419462859630585,
      "learning_rate": 9.518822239325055e-05,
      "loss": 0.0954,
      "step": 9810
    },
    {
      "epoch": 0.3679142782211232,
      "grad_norm": 0.10222227871417999,
      "learning_rate": 9.517495581685895e-05,
      "loss": 0.0935,
      "step": 9820
    },
    {
      "epoch": 0.3682889363455846,
      "grad_norm": 0.1121838390827179,
      "learning_rate": 9.516167190406957e-05,
      "loss": 0.0961,
      "step": 9830
    },
    {
      "epoch": 0.3686635944700461,
      "grad_norm": 0.10884732753038406,
      "learning_rate": 9.514837065998028e-05,
      "loss": 0.0936,
      "step": 9840
    },
    {
      "epoch": 0.3690382525945075,
      "grad_norm": 0.10209914296865463,
      "learning_rate": 9.513505208969558e-05,
      "loss": 0.0936,
      "step": 9850
    },
    {
      "epoch": 0.3694129107189689,
      "grad_norm": 0.09488604217767715,
      "learning_rate": 9.512171619832664e-05,
      "loss": 0.0885,
      "step": 9860
    },
    {
      "epoch": 0.3697875688434304,
      "grad_norm": 0.12379080057144165,
      "learning_rate": 9.510836299099123e-05,
      "loss": 0.0901,
      "step": 9870
    },
    {
      "epoch": 0.3701622269678918,
      "grad_norm": 0.08242397010326385,
      "learning_rate": 9.50949924728138e-05,
      "loss": 0.0899,
      "step": 9880
    },
    {
      "epoch": 0.3705368850923532,
      "grad_norm": 0.12167676538228989,
      "learning_rate": 9.508160464892546e-05,
      "loss": 0.0934,
      "step": 9890
    },
    {
      "epoch": 0.37091154321681463,
      "grad_norm": 0.09992692619562149,
      "learning_rate": 9.50681995244639e-05,
      "loss": 0.0888,
      "step": 9900
    },
    {
      "epoch": 0.3712862013412761,
      "grad_norm": 0.10475137829780579,
      "learning_rate": 9.505477710457354e-05,
      "loss": 0.0942,
      "step": 9910
    },
    {
      "epoch": 0.3716608594657375,
      "grad_norm": 0.18190985918045044,
      "learning_rate": 9.504133739440532e-05,
      "loss": 0.092,
      "step": 9920
    },
    {
      "epoch": 0.3720355175901989,
      "grad_norm": 0.16354171931743622,
      "learning_rate": 9.502788039911691e-05,
      "loss": 0.0977,
      "step": 9930
    },
    {
      "epoch": 0.3724101757146604,
      "grad_norm": 0.11033950746059418,
      "learning_rate": 9.501440612387262e-05,
      "loss": 0.092,
      "step": 9940
    },
    {
      "epoch": 0.3727848338391218,
      "grad_norm": 0.08675549924373627,
      "learning_rate": 9.50009145738433e-05,
      "loss": 0.0921,
      "step": 9950
    },
    {
      "epoch": 0.3731594919635832,
      "grad_norm": 0.10029289126396179,
      "learning_rate": 9.498740575420651e-05,
      "loss": 0.0905,
      "step": 9960
    },
    {
      "epoch": 0.37353415008804464,
      "grad_norm": 0.32061663269996643,
      "learning_rate": 9.497387967014639e-05,
      "loss": 0.0957,
      "step": 9970
    },
    {
      "epoch": 0.3739088082125061,
      "grad_norm": 0.08961711078882217,
      "learning_rate": 9.496033632685378e-05,
      "loss": 0.0898,
      "step": 9980
    },
    {
      "epoch": 0.3742834663369675,
      "grad_norm": 0.16616538166999817,
      "learning_rate": 9.494677572952602e-05,
      "loss": 0.0924,
      "step": 9990
    },
    {
      "epoch": 0.37465812446142893,
      "grad_norm": 0.10769137740135193,
      "learning_rate": 9.493319788336718e-05,
      "loss": 0.0926,
      "step": 10000
    },
    {
      "epoch": 0.3750327825858904,
      "grad_norm": 0.11147578060626984,
      "learning_rate": 9.491960279358793e-05,
      "loss": 0.0913,
      "step": 10010
    },
    {
      "epoch": 0.3754074407103518,
      "grad_norm": 0.1781911700963974,
      "learning_rate": 9.49059904654055e-05,
      "loss": 0.0935,
      "step": 10020
    },
    {
      "epoch": 0.3757820988348132,
      "grad_norm": 0.10255968570709229,
      "learning_rate": 9.489236090404379e-05,
      "loss": 0.0915,
      "step": 10030
    },
    {
      "epoch": 0.37615675695927464,
      "grad_norm": 0.09653627127408981,
      "learning_rate": 9.48787141147333e-05,
      "loss": 0.094,
      "step": 10040
    },
    {
      "epoch": 0.3765314150837361,
      "grad_norm": 0.12516841292381287,
      "learning_rate": 9.486505010271114e-05,
      "loss": 0.0937,
      "step": 10050
    },
    {
      "epoch": 0.3769060732081975,
      "grad_norm": 0.14268043637275696,
      "learning_rate": 9.485136887322102e-05,
      "loss": 0.0962,
      "step": 10060
    },
    {
      "epoch": 0.37728073133265894,
      "grad_norm": 0.11578825861215591,
      "learning_rate": 9.483767043151328e-05,
      "loss": 0.0922,
      "step": 10070
    },
    {
      "epoch": 0.3776553894571204,
      "grad_norm": 0.11920197308063507,
      "learning_rate": 9.482395478284482e-05,
      "loss": 0.0909,
      "step": 10080
    },
    {
      "epoch": 0.3780300475815818,
      "grad_norm": 0.08442237228155136,
      "learning_rate": 9.481022193247923e-05,
      "loss": 0.0919,
      "step": 10090
    },
    {
      "epoch": 0.37840470570604323,
      "grad_norm": 0.16052712500095367,
      "learning_rate": 9.479647188568659e-05,
      "loss": 0.0914,
      "step": 10100
    },
    {
      "epoch": 0.37877936383050465,
      "grad_norm": 0.1114698201417923,
      "learning_rate": 9.478270464774365e-05,
      "loss": 0.0931,
      "step": 10110
    },
    {
      "epoch": 0.3791540219549661,
      "grad_norm": 0.13133366405963898,
      "learning_rate": 9.476892022393378e-05,
      "loss": 0.092,
      "step": 10120
    },
    {
      "epoch": 0.3795286800794275,
      "grad_norm": 0.10717108845710754,
      "learning_rate": 9.475511861954687e-05,
      "loss": 0.0898,
      "step": 10130
    },
    {
      "epoch": 0.37990333820388894,
      "grad_norm": 0.10715219378471375,
      "learning_rate": 9.474129983987944e-05,
      "loss": 0.0953,
      "step": 10140
    },
    {
      "epoch": 0.38027799632835035,
      "grad_norm": 0.14054840803146362,
      "learning_rate": 9.472746389023462e-05,
      "loss": 0.0919,
      "step": 10150
    },
    {
      "epoch": 0.3806526544528118,
      "grad_norm": 0.08855721354484558,
      "learning_rate": 9.47136107759221e-05,
      "loss": 0.0919,
      "step": 10160
    },
    {
      "epoch": 0.38102731257727324,
      "grad_norm": 0.123748280107975,
      "learning_rate": 9.469974050225815e-05,
      "loss": 0.0893,
      "step": 10170
    },
    {
      "epoch": 0.38140197070173465,
      "grad_norm": 0.12015905976295471,
      "learning_rate": 9.468585307456569e-05,
      "loss": 0.0961,
      "step": 10180
    },
    {
      "epoch": 0.3817766288261961,
      "grad_norm": 0.10588771849870682,
      "learning_rate": 9.467194849817414e-05,
      "loss": 0.0903,
      "step": 10190
    },
    {
      "epoch": 0.38215128695065753,
      "grad_norm": 0.09720376133918762,
      "learning_rate": 9.465802677841956e-05,
      "loss": 0.0939,
      "step": 10200
    },
    {
      "epoch": 0.38252594507511894,
      "grad_norm": 0.12977850437164307,
      "learning_rate": 9.464408792064456e-05,
      "loss": 0.0933,
      "step": 10210
    },
    {
      "epoch": 0.38290060319958036,
      "grad_norm": 0.13041552901268005,
      "learning_rate": 9.463013193019831e-05,
      "loss": 0.0927,
      "step": 10220
    },
    {
      "epoch": 0.3832752613240418,
      "grad_norm": 0.12297102063894272,
      "learning_rate": 9.461615881243662e-05,
      "loss": 0.0963,
      "step": 10230
    },
    {
      "epoch": 0.38364991944850324,
      "grad_norm": 0.10378609597682953,
      "learning_rate": 9.46021685727218e-05,
      "loss": 0.0921,
      "step": 10240
    },
    {
      "epoch": 0.38402457757296465,
      "grad_norm": 0.11019282042980194,
      "learning_rate": 9.458816121642275e-05,
      "loss": 0.0942,
      "step": 10250
    },
    {
      "epoch": 0.3843992356974261,
      "grad_norm": 0.1074274554848671,
      "learning_rate": 9.4574136748915e-05,
      "loss": 0.0931,
      "step": 10260
    },
    {
      "epoch": 0.38477389382188754,
      "grad_norm": 0.10407892614603043,
      "learning_rate": 9.456009517558057e-05,
      "loss": 0.092,
      "step": 10270
    },
    {
      "epoch": 0.38514855194634895,
      "grad_norm": 0.15118619799613953,
      "learning_rate": 9.454603650180806e-05,
      "loss": 0.0914,
      "step": 10280
    },
    {
      "epoch": 0.38552321007081036,
      "grad_norm": 0.1947125792503357,
      "learning_rate": 9.453196073299265e-05,
      "loss": 0.0924,
      "step": 10290
    },
    {
      "epoch": 0.38589786819527183,
      "grad_norm": 0.17002446949481964,
      "learning_rate": 9.451786787453609e-05,
      "loss": 0.0933,
      "step": 10300
    },
    {
      "epoch": 0.38627252631973324,
      "grad_norm": 0.16523896157741547,
      "learning_rate": 9.450375793184667e-05,
      "loss": 0.0927,
      "step": 10310
    },
    {
      "epoch": 0.38664718444419466,
      "grad_norm": 0.16110439598560333,
      "learning_rate": 9.448963091033921e-05,
      "loss": 0.0968,
      "step": 10320
    },
    {
      "epoch": 0.3870218425686561,
      "grad_norm": 0.08586767315864563,
      "learning_rate": 9.447548681543514e-05,
      "loss": 0.0914,
      "step": 10330
    },
    {
      "epoch": 0.38739650069311754,
      "grad_norm": 0.10253600031137466,
      "learning_rate": 9.446132565256243e-05,
      "loss": 0.0927,
      "step": 10340
    },
    {
      "epoch": 0.38777115881757895,
      "grad_norm": 0.0990893617272377,
      "learning_rate": 9.444714742715555e-05,
      "loss": 0.0947,
      "step": 10350
    },
    {
      "epoch": 0.38814581694204037,
      "grad_norm": 0.08888891339302063,
      "learning_rate": 9.443295214465558e-05,
      "loss": 0.0914,
      "step": 10360
    },
    {
      "epoch": 0.38852047506650184,
      "grad_norm": 0.14694370329380035,
      "learning_rate": 9.441873981051011e-05,
      "loss": 0.0925,
      "step": 10370
    },
    {
      "epoch": 0.38889513319096325,
      "grad_norm": 0.10704772919416428,
      "learning_rate": 9.440451043017329e-05,
      "loss": 0.092,
      "step": 10380
    },
    {
      "epoch": 0.38926979131542466,
      "grad_norm": 0.09346403926610947,
      "learning_rate": 9.439026400910578e-05,
      "loss": 0.089,
      "step": 10390
    },
    {
      "epoch": 0.38964444943988613,
      "grad_norm": 0.09334466606378555,
      "learning_rate": 9.437600055277485e-05,
      "loss": 0.0907,
      "step": 10400
    },
    {
      "epoch": 0.39001910756434754,
      "grad_norm": 0.10595987737178802,
      "learning_rate": 9.43617200666542e-05,
      "loss": 0.0964,
      "step": 10410
    },
    {
      "epoch": 0.39039376568880896,
      "grad_norm": 0.09932826459407806,
      "learning_rate": 9.434742255622418e-05,
      "loss": 0.0907,
      "step": 10420
    },
    {
      "epoch": 0.39076842381327037,
      "grad_norm": 0.12705154716968536,
      "learning_rate": 9.433310802697161e-05,
      "loss": 0.0913,
      "step": 10430
    },
    {
      "epoch": 0.39114308193773184,
      "grad_norm": 0.12113645672798157,
      "learning_rate": 9.431877648438983e-05,
      "loss": 0.0949,
      "step": 10440
    },
    {
      "epoch": 0.39151774006219325,
      "grad_norm": 0.08331453800201416,
      "learning_rate": 9.430442793397875e-05,
      "loss": 0.0923,
      "step": 10450
    },
    {
      "epoch": 0.39189239818665467,
      "grad_norm": 0.15700310468673706,
      "learning_rate": 9.429006238124478e-05,
      "loss": 0.0878,
      "step": 10460
    },
    {
      "epoch": 0.3922670563111161,
      "grad_norm": 0.19568172097206116,
      "learning_rate": 9.427567983170086e-05,
      "loss": 0.0891,
      "step": 10470
    },
    {
      "epoch": 0.39264171443557755,
      "grad_norm": 0.11182955652475357,
      "learning_rate": 9.42612802908665e-05,
      "loss": 0.0918,
      "step": 10480
    },
    {
      "epoch": 0.39301637256003896,
      "grad_norm": 0.10366565734148026,
      "learning_rate": 9.42468637642676e-05,
      "loss": 0.0924,
      "step": 10490
    },
    {
      "epoch": 0.3933910306845004,
      "grad_norm": 0.10439899563789368,
      "learning_rate": 9.423243025743674e-05,
      "loss": 0.0955,
      "step": 10500
    },
    {
      "epoch": 0.39376568880896184,
      "grad_norm": 0.14828532934188843,
      "learning_rate": 9.421797977591289e-05,
      "loss": 0.096,
      "step": 10510
    },
    {
      "epoch": 0.39414034693342326,
      "grad_norm": 0.12751469016075134,
      "learning_rate": 9.42035123252416e-05,
      "loss": 0.091,
      "step": 10520
    },
    {
      "epoch": 0.39451500505788467,
      "grad_norm": 0.08733318001031876,
      "learning_rate": 9.418902791097495e-05,
      "loss": 0.0899,
      "step": 10530
    },
    {
      "epoch": 0.3948896631823461,
      "grad_norm": 0.09039156883955002,
      "learning_rate": 9.417452653867145e-05,
      "loss": 0.0915,
      "step": 10540
    },
    {
      "epoch": 0.39526432130680755,
      "grad_norm": 0.09399200230836868,
      "learning_rate": 9.41600082138962e-05,
      "loss": 0.091,
      "step": 10550
    },
    {
      "epoch": 0.39563897943126897,
      "grad_norm": 0.08987849205732346,
      "learning_rate": 9.414547294222075e-05,
      "loss": 0.0958,
      "step": 10560
    },
    {
      "epoch": 0.3960136375557304,
      "grad_norm": 0.08042332530021667,
      "learning_rate": 9.413092072922318e-05,
      "loss": 0.0908,
      "step": 10570
    },
    {
      "epoch": 0.39638829568019185,
      "grad_norm": 0.09800808131694794,
      "learning_rate": 9.411635158048806e-05,
      "loss": 0.0929,
      "step": 10580
    },
    {
      "epoch": 0.39676295380465326,
      "grad_norm": 0.1672300398349762,
      "learning_rate": 9.410176550160648e-05,
      "loss": 0.097,
      "step": 10590
    },
    {
      "epoch": 0.3971376119291147,
      "grad_norm": 0.110560342669487,
      "learning_rate": 9.408716249817599e-05,
      "loss": 0.0917,
      "step": 10600
    },
    {
      "epoch": 0.3975122700535761,
      "grad_norm": 0.07400932163000107,
      "learning_rate": 9.407254257580067e-05,
      "loss": 0.0933,
      "step": 10610
    },
    {
      "epoch": 0.39788692817803756,
      "grad_norm": 0.12678955495357513,
      "learning_rate": 9.40579057400911e-05,
      "loss": 0.0996,
      "step": 10620
    },
    {
      "epoch": 0.39826158630249897,
      "grad_norm": 0.09956461191177368,
      "learning_rate": 9.404325199666428e-05,
      "loss": 0.0898,
      "step": 10630
    },
    {
      "epoch": 0.3986362444269604,
      "grad_norm": 0.0826391950249672,
      "learning_rate": 9.40285813511438e-05,
      "loss": 0.0932,
      "step": 10640
    },
    {
      "epoch": 0.39901090255142185,
      "grad_norm": 0.14403954148292542,
      "learning_rate": 9.401389380915964e-05,
      "loss": 0.0946,
      "step": 10650
    },
    {
      "epoch": 0.39938556067588327,
      "grad_norm": 0.13987340033054352,
      "learning_rate": 9.399918937634835e-05,
      "loss": 0.0928,
      "step": 10660
    },
    {
      "epoch": 0.3997602188003447,
      "grad_norm": 0.1263953149318695,
      "learning_rate": 9.39844680583529e-05,
      "loss": 0.09,
      "step": 10670
    },
    {
      "epoch": 0.4001348769248061,
      "grad_norm": 0.09087454527616501,
      "learning_rate": 9.396972986082277e-05,
      "loss": 0.0914,
      "step": 10680
    },
    {
      "epoch": 0.40050953504926756,
      "grad_norm": 0.08771653473377228,
      "learning_rate": 9.395497478941392e-05,
      "loss": 0.0929,
      "step": 10690
    },
    {
      "epoch": 0.400884193173729,
      "grad_norm": 0.22649043798446655,
      "learning_rate": 9.394020284978874e-05,
      "loss": 0.09,
      "step": 10700
    },
    {
      "epoch": 0.4012588512981904,
      "grad_norm": 0.10438195616006851,
      "learning_rate": 9.392541404761616e-05,
      "loss": 0.0901,
      "step": 10710
    },
    {
      "epoch": 0.40163350942265186,
      "grad_norm": 0.1407187134027481,
      "learning_rate": 9.391060838857154e-05,
      "loss": 0.0903,
      "step": 10720
    },
    {
      "epoch": 0.40200816754711327,
      "grad_norm": 0.09891790896654129,
      "learning_rate": 9.38957858783367e-05,
      "loss": 0.0903,
      "step": 10730
    },
    {
      "epoch": 0.4023828256715747,
      "grad_norm": 0.11277534812688828,
      "learning_rate": 9.388094652259995e-05,
      "loss": 0.0893,
      "step": 10740
    },
    {
      "epoch": 0.4027574837960361,
      "grad_norm": 0.09229706972837448,
      "learning_rate": 9.386609032705608e-05,
      "loss": 0.0907,
      "step": 10750
    },
    {
      "epoch": 0.40313214192049757,
      "grad_norm": 0.184994176030159,
      "learning_rate": 9.385121729740634e-05,
      "loss": 0.0953,
      "step": 10760
    },
    {
      "epoch": 0.403506800044959,
      "grad_norm": 0.10516514629125595,
      "learning_rate": 9.383632743935837e-05,
      "loss": 0.0946,
      "step": 10770
    },
    {
      "epoch": 0.4038814581694204,
      "grad_norm": 0.11093316227197647,
      "learning_rate": 9.382142075862637e-05,
      "loss": 0.0923,
      "step": 10780
    },
    {
      "epoch": 0.4042561162938818,
      "grad_norm": 0.1135198175907135,
      "learning_rate": 9.38064972609309e-05,
      "loss": 0.0904,
      "step": 10790
    },
    {
      "epoch": 0.4046307744183433,
      "grad_norm": 0.08758924901485443,
      "learning_rate": 9.379155695199904e-05,
      "loss": 0.0913,
      "step": 10800
    },
    {
      "epoch": 0.4050054325428047,
      "grad_norm": 0.3139612376689911,
      "learning_rate": 9.377659983756432e-05,
      "loss": 0.0899,
      "step": 10810
    },
    {
      "epoch": 0.4053800906672661,
      "grad_norm": 0.13348861038684845,
      "learning_rate": 9.376162592336668e-05,
      "loss": 0.0861,
      "step": 10820
    },
    {
      "epoch": 0.40575474879172757,
      "grad_norm": 0.08235537260770798,
      "learning_rate": 9.374663521515253e-05,
      "loss": 0.0912,
      "step": 10830
    },
    {
      "epoch": 0.406129406916189,
      "grad_norm": 0.12248372286558151,
      "learning_rate": 9.373162771867473e-05,
      "loss": 0.09,
      "step": 10840
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 0.16740725934505463,
      "learning_rate": 9.371660343969258e-05,
      "loss": 0.0909,
      "step": 10850
    },
    {
      "epoch": 0.4068787231651118,
      "grad_norm": 0.09606920182704926,
      "learning_rate": 9.370156238397179e-05,
      "loss": 0.0959,
      "step": 10860
    },
    {
      "epoch": 0.4072533812895733,
      "grad_norm": 0.1062837466597557,
      "learning_rate": 9.368650455728455e-05,
      "loss": 0.0919,
      "step": 10870
    },
    {
      "epoch": 0.4076280394140347,
      "grad_norm": 0.11045733094215393,
      "learning_rate": 9.36714299654095e-05,
      "loss": 0.0937,
      "step": 10880
    },
    {
      "epoch": 0.4080026975384961,
      "grad_norm": 0.13147634267807007,
      "learning_rate": 9.365633861413165e-05,
      "loss": 0.0955,
      "step": 10890
    },
    {
      "epoch": 0.4083773556629576,
      "grad_norm": 0.1268179565668106,
      "learning_rate": 9.364123050924248e-05,
      "loss": 0.0949,
      "step": 10900
    },
    {
      "epoch": 0.408752013787419,
      "grad_norm": 0.08087330311536789,
      "learning_rate": 9.362610565653989e-05,
      "loss": 0.0894,
      "step": 10910
    },
    {
      "epoch": 0.4091266719118804,
      "grad_norm": 0.15854844450950623,
      "learning_rate": 9.361096406182823e-05,
      "loss": 0.0879,
      "step": 10920
    },
    {
      "epoch": 0.4095013300363418,
      "grad_norm": 0.17314434051513672,
      "learning_rate": 9.359580573091824e-05,
      "loss": 0.0934,
      "step": 10930
    },
    {
      "epoch": 0.4098759881608033,
      "grad_norm": 0.13229361176490784,
      "learning_rate": 9.358063066962713e-05,
      "loss": 0.0908,
      "step": 10940
    },
    {
      "epoch": 0.4102506462852647,
      "grad_norm": 0.12803973257541656,
      "learning_rate": 9.356543888377847e-05,
      "loss": 0.0972,
      "step": 10950
    },
    {
      "epoch": 0.4106253044097261,
      "grad_norm": 0.09262610226869583,
      "learning_rate": 9.355023037920227e-05,
      "loss": 0.0934,
      "step": 10960
    },
    {
      "epoch": 0.4109999625341876,
      "grad_norm": 0.125746950507164,
      "learning_rate": 9.3535005161735e-05,
      "loss": 0.0942,
      "step": 10970
    },
    {
      "epoch": 0.411374620658649,
      "grad_norm": 0.15985049307346344,
      "learning_rate": 9.351976323721949e-05,
      "loss": 0.094,
      "step": 10980
    },
    {
      "epoch": 0.4117492787831104,
      "grad_norm": 0.07530819624662399,
      "learning_rate": 9.3504504611505e-05,
      "loss": 0.0939,
      "step": 10990
    },
    {
      "epoch": 0.4121239369075718,
      "grad_norm": 0.10089851170778275,
      "learning_rate": 9.348922929044721e-05,
      "loss": 0.0926,
      "step": 11000
    },
    {
      "epoch": 0.4124985950320333,
      "grad_norm": 0.09454523772001266,
      "learning_rate": 9.347393727990818e-05,
      "loss": 0.0915,
      "step": 11010
    },
    {
      "epoch": 0.4128732531564947,
      "grad_norm": 0.11363720893859863,
      "learning_rate": 9.345862858575639e-05,
      "loss": 0.0899,
      "step": 11020
    },
    {
      "epoch": 0.4132479112809561,
      "grad_norm": 0.097095787525177,
      "learning_rate": 9.344330321386674e-05,
      "loss": 0.091,
      "step": 11030
    },
    {
      "epoch": 0.4136225694054176,
      "grad_norm": 0.07531018555164337,
      "learning_rate": 9.342796117012052e-05,
      "loss": 0.0965,
      "step": 11040
    },
    {
      "epoch": 0.413997227529879,
      "grad_norm": 0.1423042267560959,
      "learning_rate": 9.341260246040537e-05,
      "loss": 0.0942,
      "step": 11050
    },
    {
      "epoch": 0.4143718856543404,
      "grad_norm": 0.10154014825820923,
      "learning_rate": 9.339722709061542e-05,
      "loss": 0.0908,
      "step": 11060
    },
    {
      "epoch": 0.4147465437788018,
      "grad_norm": 0.0955197662115097,
      "learning_rate": 9.338183506665112e-05,
      "loss": 0.0925,
      "step": 11070
    },
    {
      "epoch": 0.4151212019032633,
      "grad_norm": 0.11936717480421066,
      "learning_rate": 9.33664263944193e-05,
      "loss": 0.0933,
      "step": 11080
    },
    {
      "epoch": 0.4154958600277247,
      "grad_norm": 0.12441383302211761,
      "learning_rate": 9.335100107983327e-05,
      "loss": 0.0936,
      "step": 11090
    },
    {
      "epoch": 0.4158705181521861,
      "grad_norm": 0.08951012045145035,
      "learning_rate": 9.333555912881263e-05,
      "loss": 0.0964,
      "step": 11100
    },
    {
      "epoch": 0.41624517627664753,
      "grad_norm": 0.13658365607261658,
      "learning_rate": 9.332010054728341e-05,
      "loss": 0.094,
      "step": 11110
    },
    {
      "epoch": 0.416619834401109,
      "grad_norm": 0.08910737931728363,
      "learning_rate": 9.330462534117801e-05,
      "loss": 0.0896,
      "step": 11120
    },
    {
      "epoch": 0.4169944925255704,
      "grad_norm": 0.11517507582902908,
      "learning_rate": 9.328913351643523e-05,
      "loss": 0.0902,
      "step": 11130
    },
    {
      "epoch": 0.41736915065003183,
      "grad_norm": 0.13331477344036102,
      "learning_rate": 9.327362507900022e-05,
      "loss": 0.0942,
      "step": 11140
    },
    {
      "epoch": 0.4177438087744933,
      "grad_norm": 0.12314661592245102,
      "learning_rate": 9.325810003482453e-05,
      "loss": 0.0934,
      "step": 11150
    },
    {
      "epoch": 0.4181184668989547,
      "grad_norm": 0.13296619057655334,
      "learning_rate": 9.324255838986604e-05,
      "loss": 0.0903,
      "step": 11160
    },
    {
      "epoch": 0.4184931250234161,
      "grad_norm": 0.08296117186546326,
      "learning_rate": 9.322700015008907e-05,
      "loss": 0.0885,
      "step": 11170
    },
    {
      "epoch": 0.41886778314787754,
      "grad_norm": 0.10327958315610886,
      "learning_rate": 9.321142532146426e-05,
      "loss": 0.0915,
      "step": 11180
    },
    {
      "epoch": 0.419242441272339,
      "grad_norm": 0.12757806479930878,
      "learning_rate": 9.31958339099686e-05,
      "loss": 0.0919,
      "step": 11190
    },
    {
      "epoch": 0.4196170993968004,
      "grad_norm": 0.13464127480983734,
      "learning_rate": 9.318022592158549e-05,
      "loss": 0.0945,
      "step": 11200
    },
    {
      "epoch": 0.41999175752126183,
      "grad_norm": 0.0873439833521843,
      "learning_rate": 9.316460136230467e-05,
      "loss": 0.0941,
      "step": 11210
    },
    {
      "epoch": 0.4203664156457233,
      "grad_norm": 0.09453345835208893,
      "learning_rate": 9.314896023812223e-05,
      "loss": 0.0961,
      "step": 11220
    },
    {
      "epoch": 0.4207410737701847,
      "grad_norm": 0.13223399221897125,
      "learning_rate": 9.313330255504065e-05,
      "loss": 0.0926,
      "step": 11230
    },
    {
      "epoch": 0.42111573189464613,
      "grad_norm": 0.11371414363384247,
      "learning_rate": 9.311762831906871e-05,
      "loss": 0.0915,
      "step": 11240
    },
    {
      "epoch": 0.42149039001910754,
      "grad_norm": 0.10827188938856125,
      "learning_rate": 9.310193753622162e-05,
      "loss": 0.095,
      "step": 11250
    },
    {
      "epoch": 0.421865048143569,
      "grad_norm": 0.10768783092498779,
      "learning_rate": 9.308623021252085e-05,
      "loss": 0.0924,
      "step": 11260
    },
    {
      "epoch": 0.4222397062680304,
      "grad_norm": 0.08473241329193115,
      "learning_rate": 9.307050635399429e-05,
      "loss": 0.0926,
      "step": 11270
    },
    {
      "epoch": 0.42261436439249184,
      "grad_norm": 0.15456847846508026,
      "learning_rate": 9.305476596667613e-05,
      "loss": 0.0925,
      "step": 11280
    },
    {
      "epoch": 0.4229890225169533,
      "grad_norm": 0.10743868350982666,
      "learning_rate": 9.303900905660692e-05,
      "loss": 0.0992,
      "step": 11290
    },
    {
      "epoch": 0.4233636806414147,
      "grad_norm": 0.0946912094950676,
      "learning_rate": 9.302323562983358e-05,
      "loss": 0.0936,
      "step": 11300
    },
    {
      "epoch": 0.42373833876587613,
      "grad_norm": 0.0811762660741806,
      "learning_rate": 9.300744569240929e-05,
      "loss": 0.0883,
      "step": 11310
    },
    {
      "epoch": 0.42411299689033755,
      "grad_norm": 0.104071706533432,
      "learning_rate": 9.299163925039367e-05,
      "loss": 0.089,
      "step": 11320
    },
    {
      "epoch": 0.424487655014799,
      "grad_norm": 0.09982677549123764,
      "learning_rate": 9.29758163098526e-05,
      "loss": 0.0924,
      "step": 11330
    },
    {
      "epoch": 0.42486231313926043,
      "grad_norm": 0.11594849824905396,
      "learning_rate": 9.295997687685828e-05,
      "loss": 0.093,
      "step": 11340
    },
    {
      "epoch": 0.42523697126372184,
      "grad_norm": 0.09448819607496262,
      "learning_rate": 9.294412095748933e-05,
      "loss": 0.0925,
      "step": 11350
    },
    {
      "epoch": 0.4256116293881833,
      "grad_norm": 0.17349554598331451,
      "learning_rate": 9.292824855783058e-05,
      "loss": 0.0944,
      "step": 11360
    },
    {
      "epoch": 0.4259862875126447,
      "grad_norm": 0.1439124196767807,
      "learning_rate": 9.291235968397329e-05,
      "loss": 0.0892,
      "step": 11370
    },
    {
      "epoch": 0.42636094563710614,
      "grad_norm": 0.0941842719912529,
      "learning_rate": 9.289645434201495e-05,
      "loss": 0.0936,
      "step": 11380
    },
    {
      "epoch": 0.42673560376156755,
      "grad_norm": 0.20970365405082703,
      "learning_rate": 9.288053253805945e-05,
      "loss": 0.0917,
      "step": 11390
    },
    {
      "epoch": 0.427110261886029,
      "grad_norm": 0.1041451096534729,
      "learning_rate": 9.286459427821694e-05,
      "loss": 0.0911,
      "step": 11400
    },
    {
      "epoch": 0.42748492001049043,
      "grad_norm": 0.09113486856222153,
      "learning_rate": 9.284863956860392e-05,
      "loss": 0.0926,
      "step": 11410
    },
    {
      "epoch": 0.42785957813495185,
      "grad_norm": 0.10426510125398636,
      "learning_rate": 9.28326684153432e-05,
      "loss": 0.0901,
      "step": 11420
    },
    {
      "epoch": 0.42823423625941326,
      "grad_norm": 0.09662312269210815,
      "learning_rate": 9.281668082456385e-05,
      "loss": 0.0921,
      "step": 11430
    },
    {
      "epoch": 0.42860889438387473,
      "grad_norm": 0.10374632477760315,
      "learning_rate": 9.280067680240132e-05,
      "loss": 0.0887,
      "step": 11440
    },
    {
      "epoch": 0.42898355250833614,
      "grad_norm": 0.11557256430387497,
      "learning_rate": 9.278465635499731e-05,
      "loss": 0.0937,
      "step": 11450
    },
    {
      "epoch": 0.42935821063279755,
      "grad_norm": 0.07858216017484665,
      "learning_rate": 9.276861948849988e-05,
      "loss": 0.0934,
      "step": 11460
    },
    {
      "epoch": 0.429732868757259,
      "grad_norm": 0.12749063968658447,
      "learning_rate": 9.275256620906333e-05,
      "loss": 0.093,
      "step": 11470
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 0.08466286957263947,
      "learning_rate": 9.273649652284831e-05,
      "loss": 0.0909,
      "step": 11480
    },
    {
      "epoch": 0.43048218500618185,
      "grad_norm": 0.12780237197875977,
      "learning_rate": 9.272041043602171e-05,
      "loss": 0.0917,
      "step": 11490
    },
    {
      "epoch": 0.43085684313064326,
      "grad_norm": 0.10329349339008331,
      "learning_rate": 9.270430795475677e-05,
      "loss": 0.0918,
      "step": 11500
    },
    {
      "epoch": 0.43123150125510473,
      "grad_norm": 0.11004869639873505,
      "learning_rate": 9.2688189085233e-05,
      "loss": 0.093,
      "step": 11510
    },
    {
      "epoch": 0.43160615937956615,
      "grad_norm": 0.08662603795528412,
      "learning_rate": 9.26720538336362e-05,
      "loss": 0.0916,
      "step": 11520
    },
    {
      "epoch": 0.43198081750402756,
      "grad_norm": 0.12321444600820541,
      "learning_rate": 9.265590220615844e-05,
      "loss": 0.0888,
      "step": 11530
    },
    {
      "epoch": 0.432355475628489,
      "grad_norm": 0.12894617021083832,
      "learning_rate": 9.26397342089981e-05,
      "loss": 0.0911,
      "step": 11540
    },
    {
      "epoch": 0.43273013375295044,
      "grad_norm": 0.10789177566766739,
      "learning_rate": 9.262354984835982e-05,
      "loss": 0.0935,
      "step": 11550
    },
    {
      "epoch": 0.43310479187741185,
      "grad_norm": 0.1113291084766388,
      "learning_rate": 9.260734913045454e-05,
      "loss": 0.0925,
      "step": 11560
    },
    {
      "epoch": 0.43347945000187327,
      "grad_norm": 0.10423023253679276,
      "learning_rate": 9.259113206149947e-05,
      "loss": 0.0864,
      "step": 11570
    },
    {
      "epoch": 0.43385410812633474,
      "grad_norm": 0.13546952605247498,
      "learning_rate": 9.257489864771807e-05,
      "loss": 0.0876,
      "step": 11580
    },
    {
      "epoch": 0.43422876625079615,
      "grad_norm": 0.11847960203886032,
      "learning_rate": 9.255864889534013e-05,
      "loss": 0.0909,
      "step": 11590
    },
    {
      "epoch": 0.43460342437525756,
      "grad_norm": 0.09540679305791855,
      "learning_rate": 9.254238281060165e-05,
      "loss": 0.0916,
      "step": 11600
    },
    {
      "epoch": 0.43497808249971903,
      "grad_norm": 0.09392727166414261,
      "learning_rate": 9.252610039974494e-05,
      "loss": 0.0936,
      "step": 11610
    },
    {
      "epoch": 0.43535274062418045,
      "grad_norm": 0.17048601806163788,
      "learning_rate": 9.250980166901855e-05,
      "loss": 0.0871,
      "step": 11620
    },
    {
      "epoch": 0.43572739874864186,
      "grad_norm": 0.14108218252658844,
      "learning_rate": 9.249348662467729e-05,
      "loss": 0.0961,
      "step": 11630
    },
    {
      "epoch": 0.43610205687310327,
      "grad_norm": 0.10009776055812836,
      "learning_rate": 9.247715527298226e-05,
      "loss": 0.091,
      "step": 11640
    },
    {
      "epoch": 0.43647671499756474,
      "grad_norm": 0.10810202360153198,
      "learning_rate": 9.24608076202008e-05,
      "loss": 0.0907,
      "step": 11650
    },
    {
      "epoch": 0.43685137312202615,
      "grad_norm": 0.09694614261388779,
      "learning_rate": 9.244444367260648e-05,
      "loss": 0.0921,
      "step": 11660
    },
    {
      "epoch": 0.43722603124648757,
      "grad_norm": 0.1407969743013382,
      "learning_rate": 9.242806343647919e-05,
      "loss": 0.0887,
      "step": 11670
    },
    {
      "epoch": 0.43760068937094904,
      "grad_norm": 0.1083025112748146,
      "learning_rate": 9.241166691810499e-05,
      "loss": 0.0915,
      "step": 11680
    },
    {
      "epoch": 0.43797534749541045,
      "grad_norm": 0.0742814689874649,
      "learning_rate": 9.239525412377625e-05,
      "loss": 0.0911,
      "step": 11690
    },
    {
      "epoch": 0.43835000561987186,
      "grad_norm": 0.1200178787112236,
      "learning_rate": 9.237882505979157e-05,
      "loss": 0.0965,
      "step": 11700
    },
    {
      "epoch": 0.4387246637443333,
      "grad_norm": 0.10371473431587219,
      "learning_rate": 9.236237973245576e-05,
      "loss": 0.0888,
      "step": 11710
    },
    {
      "epoch": 0.43909932186879475,
      "grad_norm": 0.09563038498163223,
      "learning_rate": 9.234591814807994e-05,
      "loss": 0.0894,
      "step": 11720
    },
    {
      "epoch": 0.43947397999325616,
      "grad_norm": 0.16496458649635315,
      "learning_rate": 9.232944031298139e-05,
      "loss": 0.0953,
      "step": 11730
    },
    {
      "epoch": 0.43984863811771757,
      "grad_norm": 0.12774613499641418,
      "learning_rate": 9.23129462334837e-05,
      "loss": 0.0918,
      "step": 11740
    },
    {
      "epoch": 0.440223296242179,
      "grad_norm": 0.11365571618080139,
      "learning_rate": 9.229643591591663e-05,
      "loss": 0.0928,
      "step": 11750
    },
    {
      "epoch": 0.44059795436664045,
      "grad_norm": 0.10093624889850616,
      "learning_rate": 9.227990936661622e-05,
      "loss": 0.0946,
      "step": 11760
    },
    {
      "epoch": 0.44097261249110187,
      "grad_norm": 0.08553203195333481,
      "learning_rate": 9.22633665919247e-05,
      "loss": 0.0922,
      "step": 11770
    },
    {
      "epoch": 0.4413472706155633,
      "grad_norm": 0.07899393141269684,
      "learning_rate": 9.224680759819055e-05,
      "loss": 0.0917,
      "step": 11780
    },
    {
      "epoch": 0.44172192874002475,
      "grad_norm": 0.10832040011882782,
      "learning_rate": 9.223023239176848e-05,
      "loss": 0.0923,
      "step": 11790
    },
    {
      "epoch": 0.44209658686448616,
      "grad_norm": 0.07088714838027954,
      "learning_rate": 9.221364097901941e-05,
      "loss": 0.0851,
      "step": 11800
    },
    {
      "epoch": 0.4424712449889476,
      "grad_norm": 0.10452616959810257,
      "learning_rate": 9.219703336631049e-05,
      "loss": 0.0929,
      "step": 11810
    },
    {
      "epoch": 0.442845903113409,
      "grad_norm": 0.09925369173288345,
      "learning_rate": 9.218040956001505e-05,
      "loss": 0.0922,
      "step": 11820
    },
    {
      "epoch": 0.44322056123787046,
      "grad_norm": 0.09915123879909515,
      "learning_rate": 9.216376956651269e-05,
      "loss": 0.0925,
      "step": 11830
    },
    {
      "epoch": 0.44359521936233187,
      "grad_norm": 0.07972002029418945,
      "learning_rate": 9.214711339218919e-05,
      "loss": 0.0914,
      "step": 11840
    },
    {
      "epoch": 0.4439698774867933,
      "grad_norm": 0.11136935651302338,
      "learning_rate": 9.213044104343653e-05,
      "loss": 0.091,
      "step": 11850
    },
    {
      "epoch": 0.44434453561125475,
      "grad_norm": 0.09587037563323975,
      "learning_rate": 9.211375252665293e-05,
      "loss": 0.0881,
      "step": 11860
    },
    {
      "epoch": 0.44471919373571617,
      "grad_norm": 0.0829576849937439,
      "learning_rate": 9.209704784824278e-05,
      "loss": 0.0945,
      "step": 11870
    },
    {
      "epoch": 0.4450938518601776,
      "grad_norm": 0.10171514004468918,
      "learning_rate": 9.208032701461668e-05,
      "loss": 0.0908,
      "step": 11880
    },
    {
      "epoch": 0.445468509984639,
      "grad_norm": 0.08968640863895416,
      "learning_rate": 9.20635900321915e-05,
      "loss": 0.0961,
      "step": 11890
    },
    {
      "epoch": 0.44584316810910046,
      "grad_norm": 0.1663486510515213,
      "learning_rate": 9.204683690739016e-05,
      "loss": 0.093,
      "step": 11900
    },
    {
      "epoch": 0.4462178262335619,
      "grad_norm": 0.1179189682006836,
      "learning_rate": 9.203006764664191e-05,
      "loss": 0.089,
      "step": 11910
    },
    {
      "epoch": 0.4465924843580233,
      "grad_norm": 0.08510489016771317,
      "learning_rate": 9.201328225638213e-05,
      "loss": 0.0874,
      "step": 11920
    },
    {
      "epoch": 0.44696714248248476,
      "grad_norm": 0.11893576383590698,
      "learning_rate": 9.19964807430524e-05,
      "loss": 0.0925,
      "step": 11930
    },
    {
      "epoch": 0.44734180060694617,
      "grad_norm": 0.14515233039855957,
      "learning_rate": 9.197966311310052e-05,
      "loss": 0.0932,
      "step": 11940
    },
    {
      "epoch": 0.4477164587314076,
      "grad_norm": 0.1614086925983429,
      "learning_rate": 9.196282937298038e-05,
      "loss": 0.0921,
      "step": 11950
    },
    {
      "epoch": 0.448091116855869,
      "grad_norm": 0.07709938287734985,
      "learning_rate": 9.194597952915218e-05,
      "loss": 0.093,
      "step": 11960
    },
    {
      "epoch": 0.44846577498033047,
      "grad_norm": 0.08691170811653137,
      "learning_rate": 9.192911358808221e-05,
      "loss": 0.0913,
      "step": 11970
    },
    {
      "epoch": 0.4488404331047919,
      "grad_norm": 0.09022459387779236,
      "learning_rate": 9.191223155624297e-05,
      "loss": 0.0922,
      "step": 11980
    },
    {
      "epoch": 0.4492150912292533,
      "grad_norm": 0.09107810258865356,
      "learning_rate": 9.189533344011313e-05,
      "loss": 0.0884,
      "step": 11990
    },
    {
      "epoch": 0.44958974935371476,
      "grad_norm": 0.09740490466356277,
      "learning_rate": 9.187841924617753e-05,
      "loss": 0.0944,
      "step": 12000
    },
    {
      "epoch": 0.4499644074781762,
      "grad_norm": 0.12759745121002197,
      "learning_rate": 9.186148898092717e-05,
      "loss": 0.0935,
      "step": 12010
    },
    {
      "epoch": 0.4503390656026376,
      "grad_norm": 0.0919414833188057,
      "learning_rate": 9.184454265085925e-05,
      "loss": 0.094,
      "step": 12020
    },
    {
      "epoch": 0.450713723727099,
      "grad_norm": 0.07162762433290482,
      "learning_rate": 9.18275802624771e-05,
      "loss": 0.089,
      "step": 12030
    },
    {
      "epoch": 0.45108838185156047,
      "grad_norm": 0.07563909888267517,
      "learning_rate": 9.181060182229025e-05,
      "loss": 0.0882,
      "step": 12040
    },
    {
      "epoch": 0.4514630399760219,
      "grad_norm": 0.10733218491077423,
      "learning_rate": 9.179360733681435e-05,
      "loss": 0.0907,
      "step": 12050
    },
    {
      "epoch": 0.4518376981004833,
      "grad_norm": 0.21211843192577362,
      "learning_rate": 9.177659681257123e-05,
      "loss": 0.0906,
      "step": 12060
    },
    {
      "epoch": 0.4522123562249447,
      "grad_norm": 0.1055498719215393,
      "learning_rate": 9.175957025608885e-05,
      "loss": 0.0956,
      "step": 12070
    },
    {
      "epoch": 0.4525870143494062,
      "grad_norm": 0.11032190918922424,
      "learning_rate": 9.174252767390136e-05,
      "loss": 0.0922,
      "step": 12080
    },
    {
      "epoch": 0.4529616724738676,
      "grad_norm": 0.0984591543674469,
      "learning_rate": 9.172546907254907e-05,
      "loss": 0.0888,
      "step": 12090
    },
    {
      "epoch": 0.453336330598329,
      "grad_norm": 0.12678594887256622,
      "learning_rate": 9.170839445857835e-05,
      "loss": 0.0951,
      "step": 12100
    },
    {
      "epoch": 0.4537109887227905,
      "grad_norm": 0.08311598747968674,
      "learning_rate": 9.169130383854184e-05,
      "loss": 0.0903,
      "step": 12110
    },
    {
      "epoch": 0.4540856468472519,
      "grad_norm": 0.0818161591887474,
      "learning_rate": 9.16741972189982e-05,
      "loss": 0.09,
      "step": 12120
    },
    {
      "epoch": 0.4544603049717133,
      "grad_norm": 0.10731813311576843,
      "learning_rate": 9.165707460651234e-05,
      "loss": 0.0932,
      "step": 12130
    },
    {
      "epoch": 0.4548349630961747,
      "grad_norm": 0.1323172003030777,
      "learning_rate": 9.163993600765521e-05,
      "loss": 0.0922,
      "step": 12140
    },
    {
      "epoch": 0.4552096212206362,
      "grad_norm": 0.0939604863524437,
      "learning_rate": 9.162278142900395e-05,
      "loss": 0.0914,
      "step": 12150
    },
    {
      "epoch": 0.4555842793450976,
      "grad_norm": 0.11558011174201965,
      "learning_rate": 9.160561087714185e-05,
      "loss": 0.0939,
      "step": 12160
    },
    {
      "epoch": 0.455958937469559,
      "grad_norm": 0.09073864668607712,
      "learning_rate": 9.158842435865828e-05,
      "loss": 0.0908,
      "step": 12170
    },
    {
      "epoch": 0.4563335955940205,
      "grad_norm": 0.07657715678215027,
      "learning_rate": 9.157122188014877e-05,
      "loss": 0.0926,
      "step": 12180
    },
    {
      "epoch": 0.4567082537184819,
      "grad_norm": 0.09782280027866364,
      "learning_rate": 9.155400344821495e-05,
      "loss": 0.0916,
      "step": 12190
    },
    {
      "epoch": 0.4570829118429433,
      "grad_norm": 0.10755901783704758,
      "learning_rate": 9.15367690694646e-05,
      "loss": 0.0937,
      "step": 12200
    },
    {
      "epoch": 0.4574575699674047,
      "grad_norm": 0.0913219228386879,
      "learning_rate": 9.151951875051161e-05,
      "loss": 0.0961,
      "step": 12210
    },
    {
      "epoch": 0.4578322280918662,
      "grad_norm": 0.061721380800008774,
      "learning_rate": 9.150225249797597e-05,
      "loss": 0.0921,
      "step": 12220
    },
    {
      "epoch": 0.4582068862163276,
      "grad_norm": 0.13137590885162354,
      "learning_rate": 9.14849703184838e-05,
      "loss": 0.0908,
      "step": 12230
    },
    {
      "epoch": 0.458581544340789,
      "grad_norm": 0.08957704156637192,
      "learning_rate": 9.146767221866735e-05,
      "loss": 0.0933,
      "step": 12240
    },
    {
      "epoch": 0.4589562024652505,
      "grad_norm": 0.1851860135793686,
      "learning_rate": 9.145035820516494e-05,
      "loss": 0.0937,
      "step": 12250
    },
    {
      "epoch": 0.4593308605897119,
      "grad_norm": 0.07796899974346161,
      "learning_rate": 9.143302828462104e-05,
      "loss": 0.0962,
      "step": 12260
    },
    {
      "epoch": 0.4597055187141733,
      "grad_norm": 0.10891109704971313,
      "learning_rate": 9.141568246368617e-05,
      "loss": 0.0917,
      "step": 12270
    },
    {
      "epoch": 0.4600801768386347,
      "grad_norm": 0.08480220288038254,
      "learning_rate": 9.1398320749017e-05,
      "loss": 0.0909,
      "step": 12280
    },
    {
      "epoch": 0.4604548349630962,
      "grad_norm": 0.10098781436681747,
      "learning_rate": 9.138094314727628e-05,
      "loss": 0.0915,
      "step": 12290
    },
    {
      "epoch": 0.4608294930875576,
      "grad_norm": 0.095777228474617,
      "learning_rate": 9.136354966513288e-05,
      "loss": 0.092,
      "step": 12300
    },
    {
      "epoch": 0.461204151212019,
      "grad_norm": 0.08137518912553787,
      "learning_rate": 9.134614030926173e-05,
      "loss": 0.0917,
      "step": 12310
    },
    {
      "epoch": 0.4615788093364805,
      "grad_norm": 0.12619158625602722,
      "learning_rate": 9.132871508634386e-05,
      "loss": 0.0955,
      "step": 12320
    },
    {
      "epoch": 0.4619534674609419,
      "grad_norm": 0.07434850186109543,
      "learning_rate": 9.131127400306639e-05,
      "loss": 0.0942,
      "step": 12330
    },
    {
      "epoch": 0.4623281255854033,
      "grad_norm": 0.15795713663101196,
      "learning_rate": 9.129381706612257e-05,
      "loss": 0.0942,
      "step": 12340
    },
    {
      "epoch": 0.46270278370986473,
      "grad_norm": 0.06579012423753738,
      "learning_rate": 9.127634428221165e-05,
      "loss": 0.0898,
      "step": 12350
    },
    {
      "epoch": 0.4630774418343262,
      "grad_norm": 0.09593982249498367,
      "learning_rate": 9.125885565803902e-05,
      "loss": 0.0939,
      "step": 12360
    },
    {
      "epoch": 0.4634520999587876,
      "grad_norm": 0.08713896572589874,
      "learning_rate": 9.124135120031616e-05,
      "loss": 0.0912,
      "step": 12370
    },
    {
      "epoch": 0.463826758083249,
      "grad_norm": 0.09481179714202881,
      "learning_rate": 9.122383091576057e-05,
      "loss": 0.0924,
      "step": 12380
    },
    {
      "epoch": 0.46420141620771044,
      "grad_norm": 0.07804815471172333,
      "learning_rate": 9.120629481109589e-05,
      "loss": 0.0886,
      "step": 12390
    },
    {
      "epoch": 0.4645760743321719,
      "grad_norm": 0.13399626314640045,
      "learning_rate": 9.118874289305176e-05,
      "loss": 0.0957,
      "step": 12400
    },
    {
      "epoch": 0.4649507324566333,
      "grad_norm": 0.09597503393888474,
      "learning_rate": 9.117117516836396e-05,
      "loss": 0.092,
      "step": 12410
    },
    {
      "epoch": 0.46532539058109473,
      "grad_norm": 0.1006985530257225,
      "learning_rate": 9.115359164377428e-05,
      "loss": 0.0917,
      "step": 12420
    },
    {
      "epoch": 0.4657000487055562,
      "grad_norm": 0.07743123918771744,
      "learning_rate": 9.11359923260306e-05,
      "loss": 0.0922,
      "step": 12430
    },
    {
      "epoch": 0.4660747068300176,
      "grad_norm": 0.0910111516714096,
      "learning_rate": 9.111837722188687e-05,
      "loss": 0.0944,
      "step": 12440
    },
    {
      "epoch": 0.46644936495447903,
      "grad_norm": 0.09661805629730225,
      "learning_rate": 9.110074633810306e-05,
      "loss": 0.0956,
      "step": 12450
    },
    {
      "epoch": 0.46682402307894044,
      "grad_norm": 0.12135278433561325,
      "learning_rate": 9.108309968144525e-05,
      "loss": 0.0915,
      "step": 12460
    },
    {
      "epoch": 0.4671986812034019,
      "grad_norm": 0.09754665940999985,
      "learning_rate": 9.106543725868553e-05,
      "loss": 0.091,
      "step": 12470
    },
    {
      "epoch": 0.4675733393278633,
      "grad_norm": 0.11625304073095322,
      "learning_rate": 9.104775907660204e-05,
      "loss": 0.0946,
      "step": 12480
    },
    {
      "epoch": 0.46794799745232474,
      "grad_norm": 0.08950602263212204,
      "learning_rate": 9.103006514197898e-05,
      "loss": 0.0886,
      "step": 12490
    },
    {
      "epoch": 0.4683226555767862,
      "grad_norm": 0.20167946815490723,
      "learning_rate": 9.101235546160663e-05,
      "loss": 0.0932,
      "step": 12500
    },
    {
      "epoch": 0.4686973137012476,
      "grad_norm": 0.1025046706199646,
      "learning_rate": 9.099463004228125e-05,
      "loss": 0.0968,
      "step": 12510
    },
    {
      "epoch": 0.46907197182570903,
      "grad_norm": 0.16168183088302612,
      "learning_rate": 9.097688889080518e-05,
      "loss": 0.095,
      "step": 12520
    },
    {
      "epoch": 0.46944662995017045,
      "grad_norm": 0.09315826743841171,
      "learning_rate": 9.095913201398679e-05,
      "loss": 0.0934,
      "step": 12530
    },
    {
      "epoch": 0.4698212880746319,
      "grad_norm": 0.08572694659233093,
      "learning_rate": 9.094135941864045e-05,
      "loss": 0.0905,
      "step": 12540
    },
    {
      "epoch": 0.47019594619909333,
      "grad_norm": 0.13376623392105103,
      "learning_rate": 9.092357111158662e-05,
      "loss": 0.0981,
      "step": 12550
    },
    {
      "epoch": 0.47057060432355474,
      "grad_norm": 0.10322246700525284,
      "learning_rate": 9.09057670996518e-05,
      "loss": 0.0896,
      "step": 12560
    },
    {
      "epoch": 0.4709452624480162,
      "grad_norm": 0.18757104873657227,
      "learning_rate": 9.088794738966841e-05,
      "loss": 0.0943,
      "step": 12570
    },
    {
      "epoch": 0.4713199205724776,
      "grad_norm": 0.1620442122220993,
      "learning_rate": 9.087011198847499e-05,
      "loss": 0.0939,
      "step": 12580
    },
    {
      "epoch": 0.47169457869693904,
      "grad_norm": 0.08560463041067123,
      "learning_rate": 9.08522609029161e-05,
      "loss": 0.0918,
      "step": 12590
    },
    {
      "epoch": 0.47206923682140045,
      "grad_norm": 0.33044955134391785,
      "learning_rate": 9.083439413984227e-05,
      "loss": 0.0919,
      "step": 12600
    },
    {
      "epoch": 0.4724438949458619,
      "grad_norm": 0.10918297618627548,
      "learning_rate": 9.081651170611008e-05,
      "loss": 0.0937,
      "step": 12610
    },
    {
      "epoch": 0.47281855307032333,
      "grad_norm": 0.10370343923568726,
      "learning_rate": 9.079861360858211e-05,
      "loss": 0.0894,
      "step": 12620
    },
    {
      "epoch": 0.47319321119478475,
      "grad_norm": 0.09621058404445648,
      "learning_rate": 9.078069985412696e-05,
      "loss": 0.0946,
      "step": 12630
    },
    {
      "epoch": 0.4735678693192462,
      "grad_norm": 0.08830779045820236,
      "learning_rate": 9.076277044961923e-05,
      "loss": 0.0893,
      "step": 12640
    },
    {
      "epoch": 0.47394252744370763,
      "grad_norm": 0.09664353728294373,
      "learning_rate": 9.074482540193954e-05,
      "loss": 0.0912,
      "step": 12650
    },
    {
      "epoch": 0.47431718556816904,
      "grad_norm": 0.0893642008304596,
      "learning_rate": 9.072686471797451e-05,
      "loss": 0.0903,
      "step": 12660
    },
    {
      "epoch": 0.47469184369263046,
      "grad_norm": 0.11730880290269852,
      "learning_rate": 9.070888840461673e-05,
      "loss": 0.0881,
      "step": 12670
    },
    {
      "epoch": 0.4750665018170919,
      "grad_norm": 0.08326727151870728,
      "learning_rate": 9.069089646876484e-05,
      "loss": 0.0916,
      "step": 12680
    },
    {
      "epoch": 0.47544115994155334,
      "grad_norm": 0.11950046569108963,
      "learning_rate": 9.067288891732342e-05,
      "loss": 0.0923,
      "step": 12690
    },
    {
      "epoch": 0.47581581806601475,
      "grad_norm": 0.10143418610095978,
      "learning_rate": 9.065486575720307e-05,
      "loss": 0.0945,
      "step": 12700
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.11657628417015076,
      "learning_rate": 9.063682699532043e-05,
      "loss": 0.0881,
      "step": 12710
    },
    {
      "epoch": 0.47656513431493763,
      "grad_norm": 0.28511911630630493,
      "learning_rate": 9.061877263859803e-05,
      "loss": 0.0969,
      "step": 12720
    },
    {
      "epoch": 0.47693979243939905,
      "grad_norm": 0.1225869283080101,
      "learning_rate": 9.060070269396445e-05,
      "loss": 0.0933,
      "step": 12730
    },
    {
      "epoch": 0.47731445056386046,
      "grad_norm": 0.08331914991140366,
      "learning_rate": 9.058261716835422e-05,
      "loss": 0.093,
      "step": 12740
    },
    {
      "epoch": 0.47768910868832193,
      "grad_norm": 0.0875621885061264,
      "learning_rate": 9.056451606870788e-05,
      "loss": 0.0951,
      "step": 12750
    },
    {
      "epoch": 0.47806376681278334,
      "grad_norm": 0.09373962879180908,
      "learning_rate": 9.054639940197194e-05,
      "loss": 0.0964,
      "step": 12760
    },
    {
      "epoch": 0.47843842493724476,
      "grad_norm": 0.08544201403856277,
      "learning_rate": 9.052826717509883e-05,
      "loss": 0.0904,
      "step": 12770
    },
    {
      "epoch": 0.47881308306170617,
      "grad_norm": 0.09357696026563644,
      "learning_rate": 9.051011939504706e-05,
      "loss": 0.0937,
      "step": 12780
    },
    {
      "epoch": 0.47918774118616764,
      "grad_norm": 0.10553896427154541,
      "learning_rate": 9.049195606878099e-05,
      "loss": 0.0929,
      "step": 12790
    },
    {
      "epoch": 0.47956239931062905,
      "grad_norm": 0.12111057341098785,
      "learning_rate": 9.047377720327102e-05,
      "loss": 0.0936,
      "step": 12800
    },
    {
      "epoch": 0.47993705743509046,
      "grad_norm": 0.07924791425466537,
      "learning_rate": 9.04555828054935e-05,
      "loss": 0.0949,
      "step": 12810
    },
    {
      "epoch": 0.48031171555955193,
      "grad_norm": 0.0968291237950325,
      "learning_rate": 9.043737288243073e-05,
      "loss": 0.0923,
      "step": 12820
    },
    {
      "epoch": 0.48068637368401335,
      "grad_norm": 0.08297190815210342,
      "learning_rate": 9.041914744107098e-05,
      "loss": 0.0923,
      "step": 12830
    },
    {
      "epoch": 0.48106103180847476,
      "grad_norm": 0.10840027034282684,
      "learning_rate": 9.040090648840847e-05,
      "loss": 0.0906,
      "step": 12840
    },
    {
      "epoch": 0.4814356899329362,
      "grad_norm": 0.08907796442508698,
      "learning_rate": 9.038265003144335e-05,
      "loss": 0.093,
      "step": 12850
    },
    {
      "epoch": 0.48181034805739764,
      "grad_norm": 0.0797598659992218,
      "learning_rate": 9.036437807718176e-05,
      "loss": 0.0939,
      "step": 12860
    },
    {
      "epoch": 0.48218500618185905,
      "grad_norm": 0.24202083051204681,
      "learning_rate": 9.034609063263576e-05,
      "loss": 0.0918,
      "step": 12870
    },
    {
      "epoch": 0.48255966430632047,
      "grad_norm": 0.10049501061439514,
      "learning_rate": 9.032778770482337e-05,
      "loss": 0.0917,
      "step": 12880
    },
    {
      "epoch": 0.48293432243078194,
      "grad_norm": 0.16622218489646912,
      "learning_rate": 9.030946930076854e-05,
      "loss": 0.0912,
      "step": 12890
    },
    {
      "epoch": 0.48330898055524335,
      "grad_norm": 0.07716729491949081,
      "learning_rate": 9.029113542750116e-05,
      "loss": 0.0908,
      "step": 12900
    },
    {
      "epoch": 0.48368363867970476,
      "grad_norm": 0.10578674077987671,
      "learning_rate": 9.027278609205705e-05,
      "loss": 0.0903,
      "step": 12910
    },
    {
      "epoch": 0.4840582968041662,
      "grad_norm": 0.0822107121348381,
      "learning_rate": 9.0254421301478e-05,
      "loss": 0.0899,
      "step": 12920
    },
    {
      "epoch": 0.48443295492862765,
      "grad_norm": 0.08998429030179977,
      "learning_rate": 9.023604106281169e-05,
      "loss": 0.0923,
      "step": 12930
    },
    {
      "epoch": 0.48480761305308906,
      "grad_norm": 0.12404396384954453,
      "learning_rate": 9.021764538311175e-05,
      "loss": 0.0919,
      "step": 12940
    },
    {
      "epoch": 0.4851822711775505,
      "grad_norm": 0.07116363942623138,
      "learning_rate": 9.019923426943773e-05,
      "loss": 0.0905,
      "step": 12950
    },
    {
      "epoch": 0.48555692930201194,
      "grad_norm": 0.10736896097660065,
      "learning_rate": 9.018080772885508e-05,
      "loss": 0.0937,
      "step": 12960
    },
    {
      "epoch": 0.48593158742647335,
      "grad_norm": 0.06923230737447739,
      "learning_rate": 9.016236576843523e-05,
      "loss": 0.0909,
      "step": 12970
    },
    {
      "epoch": 0.48630624555093477,
      "grad_norm": 0.08938329666852951,
      "learning_rate": 9.014390839525545e-05,
      "loss": 0.0876,
      "step": 12980
    },
    {
      "epoch": 0.4866809036753962,
      "grad_norm": 0.11136705428361893,
      "learning_rate": 9.012543561639901e-05,
      "loss": 0.0901,
      "step": 12990
    },
    {
      "epoch": 0.48705556179985765,
      "grad_norm": 0.07875264436006546,
      "learning_rate": 9.0106947438955e-05,
      "loss": 0.0956,
      "step": 13000
    },
    {
      "epoch": 0.48743021992431906,
      "grad_norm": 0.10777647793292999,
      "learning_rate": 9.00884438700185e-05,
      "loss": 0.0913,
      "step": 13010
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.10232259333133698,
      "learning_rate": 9.006992491669045e-05,
      "loss": 0.0946,
      "step": 13020
    },
    {
      "epoch": 0.4881795361732419,
      "grad_norm": 0.13408119976520538,
      "learning_rate": 9.005139058607769e-05,
      "loss": 0.0923,
      "step": 13030
    },
    {
      "epoch": 0.48855419429770336,
      "grad_norm": 0.3912006616592407,
      "learning_rate": 9.0032840885293e-05,
      "loss": 0.0934,
      "step": 13040
    },
    {
      "epoch": 0.4889288524221648,
      "grad_norm": 0.13431932032108307,
      "learning_rate": 9.001427582145504e-05,
      "loss": 0.0928,
      "step": 13050
    },
    {
      "epoch": 0.4893035105466262,
      "grad_norm": 0.0691816508769989,
      "learning_rate": 8.999569540168835e-05,
      "loss": 0.0905,
      "step": 13060
    },
    {
      "epoch": 0.48967816867108765,
      "grad_norm": 0.07752549648284912,
      "learning_rate": 8.997709963312339e-05,
      "loss": 0.091,
      "step": 13070
    },
    {
      "epoch": 0.49005282679554907,
      "grad_norm": 0.2991708815097809,
      "learning_rate": 8.995848852289646e-05,
      "loss": 0.0911,
      "step": 13080
    },
    {
      "epoch": 0.4904274849200105,
      "grad_norm": 0.09564033895730972,
      "learning_rate": 8.993986207814983e-05,
      "loss": 0.0897,
      "step": 13090
    },
    {
      "epoch": 0.4908021430444719,
      "grad_norm": 0.07966788113117218,
      "learning_rate": 8.992122030603158e-05,
      "loss": 0.0936,
      "step": 13100
    },
    {
      "epoch": 0.49117680116893336,
      "grad_norm": 0.09958023577928543,
      "learning_rate": 8.99025632136957e-05,
      "loss": 0.0896,
      "step": 13110
    },
    {
      "epoch": 0.4915514592933948,
      "grad_norm": 0.08696236461400986,
      "learning_rate": 8.988389080830206e-05,
      "loss": 0.0904,
      "step": 13120
    },
    {
      "epoch": 0.4919261174178562,
      "grad_norm": 0.14108020067214966,
      "learning_rate": 8.98652030970164e-05,
      "loss": 0.0958,
      "step": 13130
    },
    {
      "epoch": 0.49230077554231766,
      "grad_norm": 0.12760692834854126,
      "learning_rate": 8.984650008701038e-05,
      "loss": 0.0944,
      "step": 13140
    },
    {
      "epoch": 0.4926754336667791,
      "grad_norm": 0.08826977759599686,
      "learning_rate": 8.982778178546145e-05,
      "loss": 0.0907,
      "step": 13150
    },
    {
      "epoch": 0.4930500917912405,
      "grad_norm": 0.09020905941724777,
      "learning_rate": 8.980904819955296e-05,
      "loss": 0.0872,
      "step": 13160
    },
    {
      "epoch": 0.4934247499157019,
      "grad_norm": 0.11739683896303177,
      "learning_rate": 8.979029933647417e-05,
      "loss": 0.0934,
      "step": 13170
    },
    {
      "epoch": 0.49379940804016337,
      "grad_norm": 0.10866144299507141,
      "learning_rate": 8.977153520342015e-05,
      "loss": 0.0941,
      "step": 13180
    },
    {
      "epoch": 0.4941740661646248,
      "grad_norm": 0.07776475697755814,
      "learning_rate": 8.975275580759184e-05,
      "loss": 0.0924,
      "step": 13190
    },
    {
      "epoch": 0.4945487242890862,
      "grad_norm": 0.09435133635997772,
      "learning_rate": 8.973396115619606e-05,
      "loss": 0.0947,
      "step": 13200
    },
    {
      "epoch": 0.49492338241354766,
      "grad_norm": 0.1183307096362114,
      "learning_rate": 8.971515125644546e-05,
      "loss": 0.0933,
      "step": 13210
    },
    {
      "epoch": 0.4952980405380091,
      "grad_norm": 0.09808337688446045,
      "learning_rate": 8.969632611555855e-05,
      "loss": 0.0923,
      "step": 13220
    },
    {
      "epoch": 0.4956726986624705,
      "grad_norm": 0.08790884166955948,
      "learning_rate": 8.96774857407597e-05,
      "loss": 0.0891,
      "step": 13230
    },
    {
      "epoch": 0.4960473567869319,
      "grad_norm": 0.10136625915765762,
      "learning_rate": 8.96586301392791e-05,
      "loss": 0.0895,
      "step": 13240
    },
    {
      "epoch": 0.49642201491139337,
      "grad_norm": 0.20062047243118286,
      "learning_rate": 8.96397593183528e-05,
      "loss": 0.0898,
      "step": 13250
    },
    {
      "epoch": 0.4967966730358548,
      "grad_norm": 0.08913619816303253,
      "learning_rate": 8.962087328522271e-05,
      "loss": 0.093,
      "step": 13260
    },
    {
      "epoch": 0.4971713311603162,
      "grad_norm": 0.21826337277889252,
      "learning_rate": 8.960197204713653e-05,
      "loss": 0.0928,
      "step": 13270
    },
    {
      "epoch": 0.49754598928477767,
      "grad_norm": 0.0863100215792656,
      "learning_rate": 8.958305561134786e-05,
      "loss": 0.0909,
      "step": 13280
    },
    {
      "epoch": 0.4979206474092391,
      "grad_norm": 0.0903920903801918,
      "learning_rate": 8.956412398511605e-05,
      "loss": 0.0881,
      "step": 13290
    },
    {
      "epoch": 0.4982953055337005,
      "grad_norm": 0.09619035571813583,
      "learning_rate": 8.954517717570637e-05,
      "loss": 0.0868,
      "step": 13300
    },
    {
      "epoch": 0.4986699636581619,
      "grad_norm": 0.10852247476577759,
      "learning_rate": 8.952621519038982e-05,
      "loss": 0.0906,
      "step": 13310
    },
    {
      "epoch": 0.4990446217826234,
      "grad_norm": 0.11846721172332764,
      "learning_rate": 8.950723803644333e-05,
      "loss": 0.0866,
      "step": 13320
    },
    {
      "epoch": 0.4994192799070848,
      "grad_norm": 0.12248057872056961,
      "learning_rate": 8.948824572114956e-05,
      "loss": 0.0909,
      "step": 13330
    },
    {
      "epoch": 0.4997939380315462,
      "grad_norm": 0.09015626460313797,
      "learning_rate": 8.946923825179704e-05,
      "loss": 0.0909,
      "step": 13340
    },
    {
      "epoch": 0.5001685961560076,
      "grad_norm": 0.1029876098036766,
      "learning_rate": 8.94502156356801e-05,
      "loss": 0.0873,
      "step": 13350
    },
    {
      "epoch": 0.5005432542804691,
      "grad_norm": 0.11639919131994247,
      "learning_rate": 8.943117788009887e-05,
      "loss": 0.0941,
      "step": 13360
    },
    {
      "epoch": 0.5009179124049306,
      "grad_norm": 0.09197452664375305,
      "learning_rate": 8.941212499235931e-05,
      "loss": 0.0905,
      "step": 13370
    },
    {
      "epoch": 0.5012925705293919,
      "grad_norm": 0.1254664659500122,
      "learning_rate": 8.939305697977318e-05,
      "loss": 0.0934,
      "step": 13380
    },
    {
      "epoch": 0.5016672286538534,
      "grad_norm": 0.07254403829574585,
      "learning_rate": 8.937397384965805e-05,
      "loss": 0.0912,
      "step": 13390
    },
    {
      "epoch": 0.5020418867783147,
      "grad_norm": 0.08107350021600723,
      "learning_rate": 8.935487560933729e-05,
      "loss": 0.0903,
      "step": 13400
    },
    {
      "epoch": 0.5024165449027762,
      "grad_norm": 0.09846761077642441,
      "learning_rate": 8.933576226614004e-05,
      "loss": 0.0982,
      "step": 13410
    },
    {
      "epoch": 0.5027912030272377,
      "grad_norm": 0.0772622674703598,
      "learning_rate": 8.931663382740129e-05,
      "loss": 0.0935,
      "step": 13420
    },
    {
      "epoch": 0.503165861151699,
      "grad_norm": 0.1124751940369606,
      "learning_rate": 8.929749030046177e-05,
      "loss": 0.091,
      "step": 13430
    },
    {
      "epoch": 0.5035405192761605,
      "grad_norm": 0.08743829280138016,
      "learning_rate": 8.927833169266804e-05,
      "loss": 0.0914,
      "step": 13440
    },
    {
      "epoch": 0.503915177400622,
      "grad_norm": 0.09986497461795807,
      "learning_rate": 8.925915801137244e-05,
      "loss": 0.0948,
      "step": 13450
    },
    {
      "epoch": 0.5042898355250833,
      "grad_norm": 0.10717087984085083,
      "learning_rate": 8.923996926393305e-05,
      "loss": 0.0907,
      "step": 13460
    },
    {
      "epoch": 0.5046644936495448,
      "grad_norm": 0.14541582763195038,
      "learning_rate": 8.92207654577138e-05,
      "loss": 0.0922,
      "step": 13470
    },
    {
      "epoch": 0.5050391517740063,
      "grad_norm": 0.14332766830921173,
      "learning_rate": 8.920154660008436e-05,
      "loss": 0.0918,
      "step": 13480
    },
    {
      "epoch": 0.5054138098984676,
      "grad_norm": 0.07041767984628677,
      "learning_rate": 8.918231269842018e-05,
      "loss": 0.0944,
      "step": 13490
    },
    {
      "epoch": 0.5057884680229291,
      "grad_norm": 0.13149240612983704,
      "learning_rate": 8.916306376010248e-05,
      "loss": 0.0916,
      "step": 13500
    },
    {
      "epoch": 0.5061631261473905,
      "grad_norm": 0.1182851493358612,
      "learning_rate": 8.914379979251828e-05,
      "loss": 0.0906,
      "step": 13510
    },
    {
      "epoch": 0.5065377842718519,
      "grad_norm": 0.13583949208259583,
      "learning_rate": 8.91245208030603e-05,
      "loss": 0.0938,
      "step": 13520
    },
    {
      "epoch": 0.5069124423963134,
      "grad_norm": 0.07353866845369339,
      "learning_rate": 8.910522679912712e-05,
      "loss": 0.09,
      "step": 13530
    },
    {
      "epoch": 0.5072871005207747,
      "grad_norm": 0.07993441820144653,
      "learning_rate": 8.9085917788123e-05,
      "loss": 0.0971,
      "step": 13540
    },
    {
      "epoch": 0.5076617586452362,
      "grad_norm": 0.06650984287261963,
      "learning_rate": 8.906659377745799e-05,
      "loss": 0.0944,
      "step": 13550
    },
    {
      "epoch": 0.5080364167696977,
      "grad_norm": 0.09266078472137451,
      "learning_rate": 8.904725477454791e-05,
      "loss": 0.0897,
      "step": 13560
    },
    {
      "epoch": 0.508411074894159,
      "grad_norm": 0.10865949094295502,
      "learning_rate": 8.902790078681432e-05,
      "loss": 0.0903,
      "step": 13570
    },
    {
      "epoch": 0.5087857330186205,
      "grad_norm": 0.13176944851875305,
      "learning_rate": 8.900853182168452e-05,
      "loss": 0.091,
      "step": 13580
    },
    {
      "epoch": 0.509160391143082,
      "grad_norm": 0.1636187881231308,
      "learning_rate": 8.898914788659159e-05,
      "loss": 0.0944,
      "step": 13590
    },
    {
      "epoch": 0.5095350492675433,
      "grad_norm": 0.07519587874412537,
      "learning_rate": 8.89697489889743e-05,
      "loss": 0.0907,
      "step": 13600
    },
    {
      "epoch": 0.5099097073920048,
      "grad_norm": 0.13796040415763855,
      "learning_rate": 8.89503351362772e-05,
      "loss": 0.0956,
      "step": 13610
    },
    {
      "epoch": 0.5102843655164663,
      "grad_norm": 0.10641434788703918,
      "learning_rate": 8.89309063359506e-05,
      "loss": 0.0967,
      "step": 13620
    },
    {
      "epoch": 0.5106590236409276,
      "grad_norm": 0.09057549387216568,
      "learning_rate": 8.891146259545051e-05,
      "loss": 0.0877,
      "step": 13630
    },
    {
      "epoch": 0.5110336817653891,
      "grad_norm": 0.1890234500169754,
      "learning_rate": 8.889200392223869e-05,
      "loss": 0.0912,
      "step": 13640
    },
    {
      "epoch": 0.5114083398898505,
      "grad_norm": 0.09797576814889908,
      "learning_rate": 8.88725303237826e-05,
      "loss": 0.091,
      "step": 13650
    },
    {
      "epoch": 0.5117829980143119,
      "grad_norm": 0.08308463543653488,
      "learning_rate": 8.885304180755548e-05,
      "loss": 0.0922,
      "step": 13660
    },
    {
      "epoch": 0.5121576561387734,
      "grad_norm": 0.11467308551073074,
      "learning_rate": 8.883353838103625e-05,
      "loss": 0.0904,
      "step": 13670
    },
    {
      "epoch": 0.5125323142632348,
      "grad_norm": 0.07602611929178238,
      "learning_rate": 8.881402005170958e-05,
      "loss": 0.087,
      "step": 13680
    },
    {
      "epoch": 0.5129069723876962,
      "grad_norm": 0.09603403508663177,
      "learning_rate": 8.879448682706583e-05,
      "loss": 0.0917,
      "step": 13690
    },
    {
      "epoch": 0.5132816305121577,
      "grad_norm": 0.0981946587562561,
      "learning_rate": 8.877493871460114e-05,
      "loss": 0.0926,
      "step": 13700
    },
    {
      "epoch": 0.513656288636619,
      "grad_norm": 0.12341996282339096,
      "learning_rate": 8.875537572181726e-05,
      "loss": 0.085,
      "step": 13710
    },
    {
      "epoch": 0.5140309467610805,
      "grad_norm": 0.11052656173706055,
      "learning_rate": 8.873579785622175e-05,
      "loss": 0.0886,
      "step": 13720
    },
    {
      "epoch": 0.514405604885542,
      "grad_norm": 0.1375368982553482,
      "learning_rate": 8.871620512532782e-05,
      "loss": 0.0917,
      "step": 13730
    },
    {
      "epoch": 0.5147802630100033,
      "grad_norm": 0.07448016107082367,
      "learning_rate": 8.86965975366544e-05,
      "loss": 0.09,
      "step": 13740
    },
    {
      "epoch": 0.5151549211344648,
      "grad_norm": 0.07821156084537506,
      "learning_rate": 8.867697509772612e-05,
      "loss": 0.0929,
      "step": 13750
    },
    {
      "epoch": 0.5155295792589263,
      "grad_norm": 0.12760889530181885,
      "learning_rate": 8.865733781607333e-05,
      "loss": 0.0924,
      "step": 13760
    },
    {
      "epoch": 0.5159042373833876,
      "grad_norm": 0.1282084435224533,
      "learning_rate": 8.863768569923205e-05,
      "loss": 0.0882,
      "step": 13770
    },
    {
      "epoch": 0.5162788955078491,
      "grad_norm": 0.09819264709949493,
      "learning_rate": 8.8618018754744e-05,
      "loss": 0.0901,
      "step": 13780
    },
    {
      "epoch": 0.5166535536323105,
      "grad_norm": 0.0855647549033165,
      "learning_rate": 8.859833699015659e-05,
      "loss": 0.0927,
      "step": 13790
    },
    {
      "epoch": 0.5170282117567719,
      "grad_norm": 0.07630591094493866,
      "learning_rate": 8.857864041302288e-05,
      "loss": 0.0957,
      "step": 13800
    },
    {
      "epoch": 0.5174028698812334,
      "grad_norm": 0.09370938688516617,
      "learning_rate": 8.855892903090173e-05,
      "loss": 0.0919,
      "step": 13810
    },
    {
      "epoch": 0.5177775280056948,
      "grad_norm": 0.1030026525259018,
      "learning_rate": 8.853920285135755e-05,
      "loss": 0.0916,
      "step": 13820
    },
    {
      "epoch": 0.5181521861301562,
      "grad_norm": 0.17626041173934937,
      "learning_rate": 8.851946188196049e-05,
      "loss": 0.0922,
      "step": 13830
    },
    {
      "epoch": 0.5185268442546177,
      "grad_norm": 0.12834404408931732,
      "learning_rate": 8.84997061302864e-05,
      "loss": 0.0882,
      "step": 13840
    },
    {
      "epoch": 0.5189015023790791,
      "grad_norm": 0.10749797523021698,
      "learning_rate": 8.847993560391673e-05,
      "loss": 0.0977,
      "step": 13850
    },
    {
      "epoch": 0.5192761605035405,
      "grad_norm": 0.08410743623971939,
      "learning_rate": 8.846015031043869e-05,
      "loss": 0.0902,
      "step": 13860
    },
    {
      "epoch": 0.519650818628002,
      "grad_norm": 0.1332259327173233,
      "learning_rate": 8.844035025744504e-05,
      "loss": 0.093,
      "step": 13870
    },
    {
      "epoch": 0.5200254767524634,
      "grad_norm": 0.08596709370613098,
      "learning_rate": 8.842053545253435e-05,
      "loss": 0.0908,
      "step": 13880
    },
    {
      "epoch": 0.5204001348769248,
      "grad_norm": 0.0810680091381073,
      "learning_rate": 8.84007059033107e-05,
      "loss": 0.0884,
      "step": 13890
    },
    {
      "epoch": 0.5207747930013862,
      "grad_norm": 0.07947596162557602,
      "learning_rate": 8.838086161738397e-05,
      "loss": 0.0945,
      "step": 13900
    },
    {
      "epoch": 0.5211494511258477,
      "grad_norm": 0.08602423220872879,
      "learning_rate": 8.836100260236959e-05,
      "loss": 0.092,
      "step": 13910
    },
    {
      "epoch": 0.5215241092503091,
      "grad_norm": 0.10766587406396866,
      "learning_rate": 8.83411288658887e-05,
      "loss": 0.0916,
      "step": 13920
    },
    {
      "epoch": 0.5218987673747705,
      "grad_norm": 0.11679573357105255,
      "learning_rate": 8.832124041556807e-05,
      "loss": 0.0925,
      "step": 13930
    },
    {
      "epoch": 0.522273425499232,
      "grad_norm": 0.13680517673492432,
      "learning_rate": 8.830133725904008e-05,
      "loss": 0.0921,
      "step": 13940
    },
    {
      "epoch": 0.5226480836236934,
      "grad_norm": 0.08649622648954391,
      "learning_rate": 8.828141940394283e-05,
      "loss": 0.0892,
      "step": 13950
    },
    {
      "epoch": 0.5230227417481548,
      "grad_norm": 0.0662446841597557,
      "learning_rate": 8.826148685792e-05,
      "loss": 0.0905,
      "step": 13960
    },
    {
      "epoch": 0.5233973998726162,
      "grad_norm": 0.07849320769309998,
      "learning_rate": 8.824153962862093e-05,
      "loss": 0.0902,
      "step": 13970
    },
    {
      "epoch": 0.5237720579970777,
      "grad_norm": 0.14367710053920746,
      "learning_rate": 8.82215777237006e-05,
      "loss": 0.0935,
      "step": 13980
    },
    {
      "epoch": 0.5241467161215391,
      "grad_norm": 0.11953908205032349,
      "learning_rate": 8.820160115081964e-05,
      "loss": 0.0897,
      "step": 13990
    },
    {
      "epoch": 0.5245213742460005,
      "grad_norm": 0.09789091348648071,
      "learning_rate": 8.818160991764426e-05,
      "loss": 0.0921,
      "step": 14000
    },
    {
      "epoch": 0.524896032370462,
      "grad_norm": 0.14416316151618958,
      "learning_rate": 8.81616040318463e-05,
      "loss": 0.0932,
      "step": 14010
    },
    {
      "epoch": 0.5252706904949234,
      "grad_norm": 0.07359756529331207,
      "learning_rate": 8.814158350110329e-05,
      "loss": 0.0933,
      "step": 14020
    },
    {
      "epoch": 0.5256453486193848,
      "grad_norm": 0.11102470010519028,
      "learning_rate": 8.812154833309829e-05,
      "loss": 0.0913,
      "step": 14030
    },
    {
      "epoch": 0.5260200067438462,
      "grad_norm": 0.07469561696052551,
      "learning_rate": 8.810149853552004e-05,
      "loss": 0.0929,
      "step": 14040
    },
    {
      "epoch": 0.5263946648683077,
      "grad_norm": 0.07195598632097244,
      "learning_rate": 8.80814341160629e-05,
      "loss": 0.0914,
      "step": 14050
    },
    {
      "epoch": 0.5267693229927691,
      "grad_norm": 0.12084944546222687,
      "learning_rate": 8.806135508242677e-05,
      "loss": 0.0872,
      "step": 14060
    },
    {
      "epoch": 0.5271439811172305,
      "grad_norm": 0.07603852450847626,
      "learning_rate": 8.804126144231724e-05,
      "loss": 0.0936,
      "step": 14070
    },
    {
      "epoch": 0.527518639241692,
      "grad_norm": 0.12016540765762329,
      "learning_rate": 8.802115320344544e-05,
      "loss": 0.0933,
      "step": 14080
    },
    {
      "epoch": 0.5278932973661534,
      "grad_norm": 0.11218193918466568,
      "learning_rate": 8.800103037352816e-05,
      "loss": 0.0921,
      "step": 14090
    },
    {
      "epoch": 0.5282679554906148,
      "grad_norm": 0.10116841644048691,
      "learning_rate": 8.798089296028777e-05,
      "loss": 0.0907,
      "step": 14100
    },
    {
      "epoch": 0.5286426136150763,
      "grad_norm": 0.11748742312192917,
      "learning_rate": 8.79607409714522e-05,
      "loss": 0.0943,
      "step": 14110
    },
    {
      "epoch": 0.5290172717395377,
      "grad_norm": 0.09682758897542953,
      "learning_rate": 8.794057441475501e-05,
      "loss": 0.0932,
      "step": 14120
    },
    {
      "epoch": 0.5293919298639991,
      "grad_norm": 0.09239206463098526,
      "learning_rate": 8.792039329793536e-05,
      "loss": 0.0892,
      "step": 14130
    },
    {
      "epoch": 0.5297665879884605,
      "grad_norm": 0.07235291600227356,
      "learning_rate": 8.790019762873797e-05,
      "loss": 0.0941,
      "step": 14140
    },
    {
      "epoch": 0.5301412461129219,
      "grad_norm": 0.07735350728034973,
      "learning_rate": 8.787998741491314e-05,
      "loss": 0.0912,
      "step": 14150
    },
    {
      "epoch": 0.5305159042373834,
      "grad_norm": 0.0636868104338646,
      "learning_rate": 8.78597626642168e-05,
      "loss": 0.0935,
      "step": 14160
    },
    {
      "epoch": 0.5308905623618448,
      "grad_norm": 0.3817064166069031,
      "learning_rate": 8.783952338441041e-05,
      "loss": 0.093,
      "step": 14170
    },
    {
      "epoch": 0.5312652204863062,
      "grad_norm": 0.08537569642066956,
      "learning_rate": 8.781926958326101e-05,
      "loss": 0.092,
      "step": 14180
    },
    {
      "epoch": 0.5316398786107677,
      "grad_norm": 0.09227780252695084,
      "learning_rate": 8.779900126854123e-05,
      "loss": 0.0916,
      "step": 14190
    },
    {
      "epoch": 0.5320145367352291,
      "grad_norm": 0.07070720195770264,
      "learning_rate": 8.777871844802926e-05,
      "loss": 0.0931,
      "step": 14200
    },
    {
      "epoch": 0.5323891948596905,
      "grad_norm": 0.08983027935028076,
      "learning_rate": 8.775842112950888e-05,
      "loss": 0.0933,
      "step": 14210
    },
    {
      "epoch": 0.532763852984152,
      "grad_norm": 0.08057471364736557,
      "learning_rate": 8.77381093207694e-05,
      "loss": 0.0895,
      "step": 14220
    },
    {
      "epoch": 0.5331385111086134,
      "grad_norm": 0.23801515996456146,
      "learning_rate": 8.771778302960571e-05,
      "loss": 0.0926,
      "step": 14230
    },
    {
      "epoch": 0.5335131692330748,
      "grad_norm": 0.10427748411893845,
      "learning_rate": 8.769744226381826e-05,
      "loss": 0.0915,
      "step": 14240
    },
    {
      "epoch": 0.5338878273575363,
      "grad_norm": 0.08607453107833862,
      "learning_rate": 8.767708703121303e-05,
      "loss": 0.0929,
      "step": 14250
    },
    {
      "epoch": 0.5342624854819977,
      "grad_norm": 0.18657559156417847,
      "learning_rate": 8.765671733960157e-05,
      "loss": 0.0913,
      "step": 14260
    },
    {
      "epoch": 0.5346371436064591,
      "grad_norm": 0.11224634945392609,
      "learning_rate": 8.763633319680097e-05,
      "loss": 0.0933,
      "step": 14270
    },
    {
      "epoch": 0.5350118017309206,
      "grad_norm": 0.0785103514790535,
      "learning_rate": 8.761593461063393e-05,
      "loss": 0.0893,
      "step": 14280
    },
    {
      "epoch": 0.5353864598553819,
      "grad_norm": 0.06791777908802032,
      "learning_rate": 8.759552158892859e-05,
      "loss": 0.0876,
      "step": 14290
    },
    {
      "epoch": 0.5357611179798434,
      "grad_norm": 0.06703680008649826,
      "learning_rate": 8.757509413951868e-05,
      "loss": 0.0924,
      "step": 14300
    },
    {
      "epoch": 0.5361357761043049,
      "grad_norm": 0.10695187002420425,
      "learning_rate": 8.755465227024344e-05,
      "loss": 0.0892,
      "step": 14310
    },
    {
      "epoch": 0.5365104342287662,
      "grad_norm": 0.17954042553901672,
      "learning_rate": 8.753419598894772e-05,
      "loss": 0.0942,
      "step": 14320
    },
    {
      "epoch": 0.5368850923532277,
      "grad_norm": 0.08970466256141663,
      "learning_rate": 8.751372530348183e-05,
      "loss": 0.092,
      "step": 14330
    },
    {
      "epoch": 0.5372597504776891,
      "grad_norm": 0.08906188607215881,
      "learning_rate": 8.74932402217016e-05,
      "loss": 0.0913,
      "step": 14340
    },
    {
      "epoch": 0.5376344086021505,
      "grad_norm": 0.1154620423913002,
      "learning_rate": 8.747274075146846e-05,
      "loss": 0.0877,
      "step": 14350
    },
    {
      "epoch": 0.538009066726612,
      "grad_norm": 0.09983456134796143,
      "learning_rate": 8.745222690064927e-05,
      "loss": 0.0901,
      "step": 14360
    },
    {
      "epoch": 0.5383837248510734,
      "grad_norm": 0.11268147081136703,
      "learning_rate": 8.743169867711644e-05,
      "loss": 0.091,
      "step": 14370
    },
    {
      "epoch": 0.5387583829755348,
      "grad_norm": 0.07984820753335953,
      "learning_rate": 8.741115608874796e-05,
      "loss": 0.0891,
      "step": 14380
    },
    {
      "epoch": 0.5391330410999963,
      "grad_norm": 0.08264634013175964,
      "learning_rate": 8.739059914342724e-05,
      "loss": 0.093,
      "step": 14390
    },
    {
      "epoch": 0.5395076992244577,
      "grad_norm": 0.07262377440929413,
      "learning_rate": 8.737002784904325e-05,
      "loss": 0.0966,
      "step": 14400
    },
    {
      "epoch": 0.5398823573489191,
      "grad_norm": 0.08270526677370071,
      "learning_rate": 8.734944221349046e-05,
      "loss": 0.0912,
      "step": 14410
    },
    {
      "epoch": 0.5402570154733806,
      "grad_norm": 0.07483181357383728,
      "learning_rate": 8.732884224466883e-05,
      "loss": 0.0913,
      "step": 14420
    },
    {
      "epoch": 0.5406316735978419,
      "grad_norm": 0.12770822644233704,
      "learning_rate": 8.730822795048383e-05,
      "loss": 0.0964,
      "step": 14430
    },
    {
      "epoch": 0.5410063317223034,
      "grad_norm": 0.09269195050001144,
      "learning_rate": 8.728759933884643e-05,
      "loss": 0.0929,
      "step": 14440
    },
    {
      "epoch": 0.5413809898467649,
      "grad_norm": 0.07374361157417297,
      "learning_rate": 8.72669564176731e-05,
      "loss": 0.0941,
      "step": 14450
    },
    {
      "epoch": 0.5417556479712262,
      "grad_norm": 0.12269775569438934,
      "learning_rate": 8.724629919488581e-05,
      "loss": 0.093,
      "step": 14460
    },
    {
      "epoch": 0.5421303060956877,
      "grad_norm": 0.10353396832942963,
      "learning_rate": 8.722562767841194e-05,
      "loss": 0.0929,
      "step": 14470
    },
    {
      "epoch": 0.5425049642201492,
      "grad_norm": 0.07957889139652252,
      "learning_rate": 8.720494187618448e-05,
      "loss": 0.0899,
      "step": 14480
    },
    {
      "epoch": 0.5428796223446105,
      "grad_norm": 0.20737779140472412,
      "learning_rate": 8.71842417961418e-05,
      "loss": 0.0913,
      "step": 14490
    },
    {
      "epoch": 0.543254280469072,
      "grad_norm": 0.12423491477966309,
      "learning_rate": 8.716352744622783e-05,
      "loss": 0.092,
      "step": 14500
    },
    {
      "epoch": 0.5436289385935335,
      "grad_norm": 0.12529349327087402,
      "learning_rate": 8.714279883439186e-05,
      "loss": 0.0918,
      "step": 14510
    },
    {
      "epoch": 0.5440035967179948,
      "grad_norm": 0.0975787341594696,
      "learning_rate": 8.712205596858882e-05,
      "loss": 0.0892,
      "step": 14520
    },
    {
      "epoch": 0.5443782548424563,
      "grad_norm": 0.09003287553787231,
      "learning_rate": 8.710129885677898e-05,
      "loss": 0.093,
      "step": 14530
    },
    {
      "epoch": 0.5447529129669176,
      "grad_norm": 0.08799020946025848,
      "learning_rate": 8.708052750692807e-05,
      "loss": 0.0917,
      "step": 14540
    },
    {
      "epoch": 0.5451275710913791,
      "grad_norm": 0.090851791203022,
      "learning_rate": 8.705974192700741e-05,
      "loss": 0.0948,
      "step": 14550
    },
    {
      "epoch": 0.5455022292158406,
      "grad_norm": 0.08417072147130966,
      "learning_rate": 8.703894212499364e-05,
      "loss": 0.0947,
      "step": 14560
    },
    {
      "epoch": 0.5458768873403019,
      "grad_norm": 0.0925147533416748,
      "learning_rate": 8.701812810886894e-05,
      "loss": 0.0956,
      "step": 14570
    },
    {
      "epoch": 0.5462515454647634,
      "grad_norm": 0.13407014310359955,
      "learning_rate": 8.699729988662093e-05,
      "loss": 0.0879,
      "step": 14580
    },
    {
      "epoch": 0.5466262035892249,
      "grad_norm": 0.08521648496389389,
      "learning_rate": 8.697645746624266e-05,
      "loss": 0.0889,
      "step": 14590
    },
    {
      "epoch": 0.5470008617136862,
      "grad_norm": 0.0798412412405014,
      "learning_rate": 8.695560085573265e-05,
      "loss": 0.089,
      "step": 14600
    },
    {
      "epoch": 0.5473755198381477,
      "grad_norm": 0.07696473598480225,
      "learning_rate": 8.693473006309486e-05,
      "loss": 0.09,
      "step": 14610
    },
    {
      "epoch": 0.5477501779626092,
      "grad_norm": 0.19699552655220032,
      "learning_rate": 8.691384509633869e-05,
      "loss": 0.0944,
      "step": 14620
    },
    {
      "epoch": 0.5481248360870705,
      "grad_norm": 0.07227194309234619,
      "learning_rate": 8.689294596347898e-05,
      "loss": 0.0898,
      "step": 14630
    },
    {
      "epoch": 0.548499494211532,
      "grad_norm": 0.08292566984891891,
      "learning_rate": 8.687203267253601e-05,
      "loss": 0.0938,
      "step": 14640
    },
    {
      "epoch": 0.5488741523359935,
      "grad_norm": 0.07597068697214127,
      "learning_rate": 8.68511052315355e-05,
      "loss": 0.0887,
      "step": 14650
    },
    {
      "epoch": 0.5492488104604548,
      "grad_norm": 0.07140975445508957,
      "learning_rate": 8.683016364850856e-05,
      "loss": 0.093,
      "step": 14660
    },
    {
      "epoch": 0.5496234685849163,
      "grad_norm": 0.11379386484622955,
      "learning_rate": 8.680920793149178e-05,
      "loss": 0.089,
      "step": 14670
    },
    {
      "epoch": 0.5499981267093776,
      "grad_norm": 0.08548212796449661,
      "learning_rate": 8.678823808852716e-05,
      "loss": 0.0931,
      "step": 14680
    },
    {
      "epoch": 0.5503727848338391,
      "grad_norm": 0.08458978682756424,
      "learning_rate": 8.676725412766211e-05,
      "loss": 0.0929,
      "step": 14690
    },
    {
      "epoch": 0.5507474429583006,
      "grad_norm": 0.10222116112709045,
      "learning_rate": 8.674625605694943e-05,
      "loss": 0.094,
      "step": 14700
    },
    {
      "epoch": 0.5511221010827619,
      "grad_norm": 0.08858794718980789,
      "learning_rate": 8.672524388444742e-05,
      "loss": 0.0926,
      "step": 14710
    },
    {
      "epoch": 0.5514967592072234,
      "grad_norm": 0.07638176530599594,
      "learning_rate": 8.67042176182197e-05,
      "loss": 0.0918,
      "step": 14720
    },
    {
      "epoch": 0.5518714173316849,
      "grad_norm": 0.07361935824155807,
      "learning_rate": 8.668317726633534e-05,
      "loss": 0.0915,
      "step": 14730
    },
    {
      "epoch": 0.5522460754561462,
      "grad_norm": 0.07069608569145203,
      "learning_rate": 8.666212283686881e-05,
      "loss": 0.0932,
      "step": 14740
    },
    {
      "epoch": 0.5526207335806077,
      "grad_norm": 0.13218334317207336,
      "learning_rate": 8.664105433790002e-05,
      "loss": 0.0927,
      "step": 14750
    },
    {
      "epoch": 0.5529953917050692,
      "grad_norm": 0.10064811259508133,
      "learning_rate": 8.66199717775142e-05,
      "loss": 0.0946,
      "step": 14760
    },
    {
      "epoch": 0.5533700498295305,
      "grad_norm": 0.13476915657520294,
      "learning_rate": 8.659887516380202e-05,
      "loss": 0.0907,
      "step": 14770
    },
    {
      "epoch": 0.553744707953992,
      "grad_norm": 0.09052827209234238,
      "learning_rate": 8.657776450485957e-05,
      "loss": 0.0942,
      "step": 14780
    },
    {
      "epoch": 0.5541193660784534,
      "grad_norm": 0.1931912750005722,
      "learning_rate": 8.65566398087883e-05,
      "loss": 0.0929,
      "step": 14790
    },
    {
      "epoch": 0.5544940242029148,
      "grad_norm": 0.08758287131786346,
      "learning_rate": 8.653550108369503e-05,
      "loss": 0.0901,
      "step": 14800
    },
    {
      "epoch": 0.5548686823273763,
      "grad_norm": 0.07472407817840576,
      "learning_rate": 8.6514348337692e-05,
      "loss": 0.0928,
      "step": 14810
    },
    {
      "epoch": 0.5552433404518377,
      "grad_norm": 0.07730678468942642,
      "learning_rate": 8.64931815788968e-05,
      "loss": 0.0918,
      "step": 14820
    },
    {
      "epoch": 0.5556179985762991,
      "grad_norm": 0.4771749675273895,
      "learning_rate": 8.647200081543243e-05,
      "loss": 0.0935,
      "step": 14830
    },
    {
      "epoch": 0.5559926567007606,
      "grad_norm": 0.0720374584197998,
      "learning_rate": 8.645080605542723e-05,
      "loss": 0.0919,
      "step": 14840
    },
    {
      "epoch": 0.556367314825222,
      "grad_norm": 0.0939740464091301,
      "learning_rate": 8.642959730701493e-05,
      "loss": 0.0931,
      "step": 14850
    },
    {
      "epoch": 0.5567419729496834,
      "grad_norm": 0.07578045129776001,
      "learning_rate": 8.640837457833463e-05,
      "loss": 0.0925,
      "step": 14860
    },
    {
      "epoch": 0.5571166310741449,
      "grad_norm": 0.07435263693332672,
      "learning_rate": 8.638713787753082e-05,
      "loss": 0.0885,
      "step": 14870
    },
    {
      "epoch": 0.5574912891986062,
      "grad_norm": 0.08967050164937973,
      "learning_rate": 8.636588721275327e-05,
      "loss": 0.0939,
      "step": 14880
    },
    {
      "epoch": 0.5578659473230677,
      "grad_norm": 0.0734366923570633,
      "learning_rate": 8.634462259215719e-05,
      "loss": 0.0888,
      "step": 14890
    },
    {
      "epoch": 0.5582406054475292,
      "grad_norm": 0.08929958194494247,
      "learning_rate": 8.632334402390312e-05,
      "loss": 0.0925,
      "step": 14900
    },
    {
      "epoch": 0.5586152635719905,
      "grad_norm": 0.07585282623767853,
      "learning_rate": 8.630205151615693e-05,
      "loss": 0.0901,
      "step": 14910
    },
    {
      "epoch": 0.558989921696452,
      "grad_norm": 0.08144503831863403,
      "learning_rate": 8.62807450770899e-05,
      "loss": 0.0926,
      "step": 14920
    },
    {
      "epoch": 0.5593645798209134,
      "grad_norm": 0.08042251318693161,
      "learning_rate": 8.625942471487857e-05,
      "loss": 0.0937,
      "step": 14930
    },
    {
      "epoch": 0.5597392379453748,
      "grad_norm": 0.07543661445379257,
      "learning_rate": 8.623809043770489e-05,
      "loss": 0.0933,
      "step": 14940
    },
    {
      "epoch": 0.5601138960698363,
      "grad_norm": 0.07542544603347778,
      "learning_rate": 8.621674225375613e-05,
      "loss": 0.091,
      "step": 14950
    },
    {
      "epoch": 0.5604885541942977,
      "grad_norm": 0.0772402361035347,
      "learning_rate": 8.619538017122489e-05,
      "loss": 0.0981,
      "step": 14960
    },
    {
      "epoch": 0.5608632123187591,
      "grad_norm": 0.08557114005088806,
      "learning_rate": 8.61740041983091e-05,
      "loss": 0.0923,
      "step": 14970
    },
    {
      "epoch": 0.5612378704432206,
      "grad_norm": 0.10251908749341965,
      "learning_rate": 8.615261434321207e-05,
      "loss": 0.0883,
      "step": 14980
    },
    {
      "epoch": 0.561612528567682,
      "grad_norm": 0.08659018576145172,
      "learning_rate": 8.613121061414236e-05,
      "loss": 0.0882,
      "step": 14990
    },
    {
      "epoch": 0.5619871866921434,
      "grad_norm": 0.1279488205909729,
      "learning_rate": 8.61097930193139e-05,
      "loss": 0.0932,
      "step": 15000
    },
    {
      "epoch": 0.5623618448166049,
      "grad_norm": 0.09893359988927841,
      "learning_rate": 8.608836156694593e-05,
      "loss": 0.0903,
      "step": 15010
    },
    {
      "epoch": 0.5627365029410663,
      "grad_norm": 0.0981581062078476,
      "learning_rate": 8.606691626526302e-05,
      "loss": 0.0936,
      "step": 15020
    },
    {
      "epoch": 0.5631111610655277,
      "grad_norm": 0.08899565786123276,
      "learning_rate": 8.604545712249506e-05,
      "loss": 0.0897,
      "step": 15030
    },
    {
      "epoch": 0.5634858191899892,
      "grad_norm": 0.19587111473083496,
      "learning_rate": 8.60239841468772e-05,
      "loss": 0.096,
      "step": 15040
    },
    {
      "epoch": 0.5638604773144505,
      "grad_norm": 0.06233642250299454,
      "learning_rate": 8.600249734664996e-05,
      "loss": 0.0916,
      "step": 15050
    },
    {
      "epoch": 0.564235135438912,
      "grad_norm": 0.07573149353265762,
      "learning_rate": 8.598099673005914e-05,
      "loss": 0.0909,
      "step": 15060
    },
    {
      "epoch": 0.5646097935633734,
      "grad_norm": 0.0833044946193695,
      "learning_rate": 8.595948230535585e-05,
      "loss": 0.0914,
      "step": 15070
    },
    {
      "epoch": 0.5649844516878348,
      "grad_norm": 0.1013360321521759,
      "learning_rate": 8.593795408079646e-05,
      "loss": 0.0916,
      "step": 15080
    },
    {
      "epoch": 0.5653591098122963,
      "grad_norm": 0.0892147496342659,
      "learning_rate": 8.591641206464271e-05,
      "loss": 0.0903,
      "step": 15090
    },
    {
      "epoch": 0.5657337679367577,
      "grad_norm": 0.09352301806211472,
      "learning_rate": 8.589485626516159e-05,
      "loss": 0.0908,
      "step": 15100
    },
    {
      "epoch": 0.5661084260612191,
      "grad_norm": 0.10302760452032089,
      "learning_rate": 8.587328669062534e-05,
      "loss": 0.0908,
      "step": 15110
    },
    {
      "epoch": 0.5664830841856806,
      "grad_norm": 0.10756945610046387,
      "learning_rate": 8.585170334931155e-05,
      "loss": 0.092,
      "step": 15120
    },
    {
      "epoch": 0.566857742310142,
      "grad_norm": 0.07103700190782547,
      "learning_rate": 8.583010624950309e-05,
      "loss": 0.0912,
      "step": 15130
    },
    {
      "epoch": 0.5672324004346034,
      "grad_norm": 0.0947858914732933,
      "learning_rate": 8.580849539948805e-05,
      "loss": 0.0908,
      "step": 15140
    },
    {
      "epoch": 0.5676070585590649,
      "grad_norm": 0.09153829514980316,
      "learning_rate": 8.578687080755987e-05,
      "loss": 0.0929,
      "step": 15150
    },
    {
      "epoch": 0.5679817166835263,
      "grad_norm": 0.07079989463090897,
      "learning_rate": 8.576523248201722e-05,
      "loss": 0.0907,
      "step": 15160
    },
    {
      "epoch": 0.5683563748079877,
      "grad_norm": 0.12585411965847015,
      "learning_rate": 8.574358043116407e-05,
      "loss": 0.0912,
      "step": 15170
    },
    {
      "epoch": 0.5687310329324491,
      "grad_norm": 0.08004272729158401,
      "learning_rate": 8.572191466330957e-05,
      "loss": 0.091,
      "step": 15180
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 0.07804276794195175,
      "learning_rate": 8.570023518676829e-05,
      "loss": 0.0889,
      "step": 15190
    },
    {
      "epoch": 0.569480349181372,
      "grad_norm": 0.08297335356473923,
      "learning_rate": 8.567854200985992e-05,
      "loss": 0.0921,
      "step": 15200
    },
    {
      "epoch": 0.5698550073058334,
      "grad_norm": 0.07500796765089035,
      "learning_rate": 8.565683514090948e-05,
      "loss": 0.0851,
      "step": 15210
    },
    {
      "epoch": 0.5702296654302949,
      "grad_norm": 0.10381504893302917,
      "learning_rate": 8.56351145882472e-05,
      "loss": 0.0871,
      "step": 15220
    },
    {
      "epoch": 0.5706043235547563,
      "grad_norm": 0.0882953554391861,
      "learning_rate": 8.561338036020863e-05,
      "loss": 0.093,
      "step": 15230
    },
    {
      "epoch": 0.5709789816792177,
      "grad_norm": 0.0894831120967865,
      "learning_rate": 8.559163246513449e-05,
      "loss": 0.0925,
      "step": 15240
    },
    {
      "epoch": 0.5713536398036791,
      "grad_norm": 0.11512839794158936,
      "learning_rate": 8.556987091137079e-05,
      "loss": 0.0905,
      "step": 15250
    },
    {
      "epoch": 0.5717282979281406,
      "grad_norm": 0.09587452560663223,
      "learning_rate": 8.554809570726879e-05,
      "loss": 0.0922,
      "step": 15260
    },
    {
      "epoch": 0.572102956052602,
      "grad_norm": 0.08319902420043945,
      "learning_rate": 8.552630686118492e-05,
      "loss": 0.09,
      "step": 15270
    },
    {
      "epoch": 0.5724776141770634,
      "grad_norm": 0.06979721784591675,
      "learning_rate": 8.550450438148096e-05,
      "loss": 0.0912,
      "step": 15280
    },
    {
      "epoch": 0.5728522723015249,
      "grad_norm": 0.0612628310918808,
      "learning_rate": 8.548268827652381e-05,
      "loss": 0.092,
      "step": 15290
    },
    {
      "epoch": 0.5732269304259863,
      "grad_norm": 0.19011417031288147,
      "learning_rate": 8.546085855468564e-05,
      "loss": 0.0945,
      "step": 15300
    },
    {
      "epoch": 0.5736015885504477,
      "grad_norm": 0.07105802744626999,
      "learning_rate": 8.54390152243439e-05,
      "loss": 0.0926,
      "step": 15310
    },
    {
      "epoch": 0.5739762466749091,
      "grad_norm": 0.0743524357676506,
      "learning_rate": 8.541715829388116e-05,
      "loss": 0.0887,
      "step": 15320
    },
    {
      "epoch": 0.5743509047993706,
      "grad_norm": 0.07241430878639221,
      "learning_rate": 8.539528777168531e-05,
      "loss": 0.0894,
      "step": 15330
    },
    {
      "epoch": 0.574725562923832,
      "grad_norm": 0.08933433145284653,
      "learning_rate": 8.537340366614936e-05,
      "loss": 0.0918,
      "step": 15340
    },
    {
      "epoch": 0.5751002210482934,
      "grad_norm": 0.08443553000688553,
      "learning_rate": 8.535150598567163e-05,
      "loss": 0.0941,
      "step": 15350
    },
    {
      "epoch": 0.5754748791727549,
      "grad_norm": 0.07115086168050766,
      "learning_rate": 8.532959473865558e-05,
      "loss": 0.0909,
      "step": 15360
    },
    {
      "epoch": 0.5758495372972163,
      "grad_norm": 0.08950618654489517,
      "learning_rate": 8.530766993350989e-05,
      "loss": 0.0926,
      "step": 15370
    },
    {
      "epoch": 0.5762241954216777,
      "grad_norm": 0.07729791104793549,
      "learning_rate": 8.528573157864846e-05,
      "loss": 0.0912,
      "step": 15380
    },
    {
      "epoch": 0.5765988535461392,
      "grad_norm": 0.08382527530193329,
      "learning_rate": 8.526377968249038e-05,
      "loss": 0.0922,
      "step": 15390
    },
    {
      "epoch": 0.5769735116706006,
      "grad_norm": 0.11183003336191177,
      "learning_rate": 8.524181425345991e-05,
      "loss": 0.0931,
      "step": 15400
    },
    {
      "epoch": 0.577348169795062,
      "grad_norm": 0.09659656882286072,
      "learning_rate": 8.52198352999866e-05,
      "loss": 0.0886,
      "step": 15410
    },
    {
      "epoch": 0.5777228279195235,
      "grad_norm": 0.09640096873044968,
      "learning_rate": 8.519784283050505e-05,
      "loss": 0.0893,
      "step": 15420
    },
    {
      "epoch": 0.5780974860439848,
      "grad_norm": 0.09109008312225342,
      "learning_rate": 8.517583685345515e-05,
      "loss": 0.0905,
      "step": 15430
    },
    {
      "epoch": 0.5784721441684463,
      "grad_norm": 0.05300864949822426,
      "learning_rate": 8.515381737728194e-05,
      "loss": 0.0901,
      "step": 15440
    },
    {
      "epoch": 0.5788468022929077,
      "grad_norm": 0.07916824519634247,
      "learning_rate": 8.513178441043564e-05,
      "loss": 0.0927,
      "step": 15450
    },
    {
      "epoch": 0.5792214604173691,
      "grad_norm": 0.06773560494184494,
      "learning_rate": 8.510973796137165e-05,
      "loss": 0.0883,
      "step": 15460
    },
    {
      "epoch": 0.5795961185418306,
      "grad_norm": 0.08126570284366608,
      "learning_rate": 8.508767803855054e-05,
      "loss": 0.0927,
      "step": 15470
    },
    {
      "epoch": 0.579970776666292,
      "grad_norm": 0.10878796875476837,
      "learning_rate": 8.506560465043805e-05,
      "loss": 0.0925,
      "step": 15480
    },
    {
      "epoch": 0.5803454347907534,
      "grad_norm": 0.08961881697177887,
      "learning_rate": 8.504351780550508e-05,
      "loss": 0.0939,
      "step": 15490
    },
    {
      "epoch": 0.5807200929152149,
      "grad_norm": 0.08944340795278549,
      "learning_rate": 8.502141751222774e-05,
      "loss": 0.0928,
      "step": 15500
    },
    {
      "epoch": 0.5810947510396763,
      "grad_norm": 0.08446818590164185,
      "learning_rate": 8.499930377908724e-05,
      "loss": 0.0909,
      "step": 15510
    },
    {
      "epoch": 0.5814694091641377,
      "grad_norm": 0.08148128539323807,
      "learning_rate": 8.497717661456998e-05,
      "loss": 0.0924,
      "step": 15520
    },
    {
      "epoch": 0.5818440672885992,
      "grad_norm": 0.0770718902349472,
      "learning_rate": 8.495503602716751e-05,
      "loss": 0.0926,
      "step": 15530
    },
    {
      "epoch": 0.5822187254130606,
      "grad_norm": 0.09125377237796783,
      "learning_rate": 8.493288202537653e-05,
      "loss": 0.092,
      "step": 15540
    },
    {
      "epoch": 0.582593383537522,
      "grad_norm": 0.07747470587491989,
      "learning_rate": 8.491071461769889e-05,
      "loss": 0.0941,
      "step": 15550
    },
    {
      "epoch": 0.5829680416619835,
      "grad_norm": 0.11192608624696732,
      "learning_rate": 8.488853381264157e-05,
      "loss": 0.0891,
      "step": 15560
    },
    {
      "epoch": 0.5833426997864448,
      "grad_norm": 0.1071840450167656,
      "learning_rate": 8.486633961871673e-05,
      "loss": 0.0938,
      "step": 15570
    },
    {
      "epoch": 0.5837173579109063,
      "grad_norm": 0.08382411301136017,
      "learning_rate": 8.48441320444416e-05,
      "loss": 0.0924,
      "step": 15580
    },
    {
      "epoch": 0.5840920160353678,
      "grad_norm": 0.07654900848865509,
      "learning_rate": 8.482191109833863e-05,
      "loss": 0.0928,
      "step": 15590
    },
    {
      "epoch": 0.5844666741598291,
      "grad_norm": 0.07770340144634247,
      "learning_rate": 8.479967678893532e-05,
      "loss": 0.0914,
      "step": 15600
    },
    {
      "epoch": 0.5848413322842906,
      "grad_norm": 0.06900238990783691,
      "learning_rate": 8.477742912476436e-05,
      "loss": 0.0899,
      "step": 15610
    },
    {
      "epoch": 0.585215990408752,
      "grad_norm": 0.0905410498380661,
      "learning_rate": 8.475516811436355e-05,
      "loss": 0.0924,
      "step": 15620
    },
    {
      "epoch": 0.5855906485332134,
      "grad_norm": 0.10244694352149963,
      "learning_rate": 8.473289376627578e-05,
      "loss": 0.0931,
      "step": 15630
    },
    {
      "epoch": 0.5859653066576749,
      "grad_norm": 0.1050238311290741,
      "learning_rate": 8.47106060890491e-05,
      "loss": 0.0914,
      "step": 15640
    },
    {
      "epoch": 0.5863399647821363,
      "grad_norm": 0.09912718832492828,
      "learning_rate": 8.468830509123664e-05,
      "loss": 0.0925,
      "step": 15650
    },
    {
      "epoch": 0.5867146229065977,
      "grad_norm": 0.11432640999555588,
      "learning_rate": 8.466599078139668e-05,
      "loss": 0.0918,
      "step": 15660
    },
    {
      "epoch": 0.5870892810310592,
      "grad_norm": 0.10021224617958069,
      "learning_rate": 8.464366316809255e-05,
      "loss": 0.0916,
      "step": 15670
    },
    {
      "epoch": 0.5874639391555206,
      "grad_norm": 0.10272622853517532,
      "learning_rate": 8.462132225989276e-05,
      "loss": 0.0906,
      "step": 15680
    },
    {
      "epoch": 0.587838597279982,
      "grad_norm": 0.08951197564601898,
      "learning_rate": 8.459896806537089e-05,
      "loss": 0.0955,
      "step": 15690
    },
    {
      "epoch": 0.5882132554044435,
      "grad_norm": 0.08186561614274979,
      "learning_rate": 8.457660059310557e-05,
      "loss": 0.0912,
      "step": 15700
    },
    {
      "epoch": 0.5885879135289048,
      "grad_norm": 0.08976541459560394,
      "learning_rate": 8.455421985168062e-05,
      "loss": 0.0905,
      "step": 15710
    },
    {
      "epoch": 0.5889625716533663,
      "grad_norm": 0.07494165003299713,
      "learning_rate": 8.453182584968487e-05,
      "loss": 0.0913,
      "step": 15720
    },
    {
      "epoch": 0.5893372297778278,
      "grad_norm": 0.08241727948188782,
      "learning_rate": 8.450941859571226e-05,
      "loss": 0.0878,
      "step": 15730
    },
    {
      "epoch": 0.5897118879022891,
      "grad_norm": 0.3177529275417328,
      "learning_rate": 8.448699809836187e-05,
      "loss": 0.0912,
      "step": 15740
    },
    {
      "epoch": 0.5900865460267506,
      "grad_norm": 0.11941198259592056,
      "learning_rate": 8.446456436623778e-05,
      "loss": 0.0923,
      "step": 15750
    },
    {
      "epoch": 0.5904612041512121,
      "grad_norm": 0.16621936857700348,
      "learning_rate": 8.444211740794921e-05,
      "loss": 0.0909,
      "step": 15760
    },
    {
      "epoch": 0.5908358622756734,
      "grad_norm": 0.1151508241891861,
      "learning_rate": 8.44196572321104e-05,
      "loss": 0.0916,
      "step": 15770
    },
    {
      "epoch": 0.5912105204001349,
      "grad_norm": 0.09319703280925751,
      "learning_rate": 8.439718384734074e-05,
      "loss": 0.0919,
      "step": 15780
    },
    {
      "epoch": 0.5915851785245964,
      "grad_norm": 0.09301187098026276,
      "learning_rate": 8.437469726226458e-05,
      "loss": 0.0944,
      "step": 15790
    },
    {
      "epoch": 0.5919598366490577,
      "grad_norm": 0.07248188555240631,
      "learning_rate": 8.435219748551149e-05,
      "loss": 0.0882,
      "step": 15800
    },
    {
      "epoch": 0.5923344947735192,
      "grad_norm": 0.08640526235103607,
      "learning_rate": 8.432968452571595e-05,
      "loss": 0.0946,
      "step": 15810
    },
    {
      "epoch": 0.5927091528979805,
      "grad_norm": 0.0787850171327591,
      "learning_rate": 8.430715839151757e-05,
      "loss": 0.0939,
      "step": 15820
    },
    {
      "epoch": 0.593083811022442,
      "grad_norm": 0.09815488010644913,
      "learning_rate": 8.428461909156102e-05,
      "loss": 0.0925,
      "step": 15830
    },
    {
      "epoch": 0.5934584691469035,
      "grad_norm": 0.08864352852106094,
      "learning_rate": 8.4262066634496e-05,
      "loss": 0.0949,
      "step": 15840
    },
    {
      "epoch": 0.5938331272713648,
      "grad_norm": 0.1366189569234848,
      "learning_rate": 8.423950102897728e-05,
      "loss": 0.0904,
      "step": 15850
    },
    {
      "epoch": 0.5942077853958263,
      "grad_norm": 0.1469774842262268,
      "learning_rate": 8.421692228366465e-05,
      "loss": 0.0937,
      "step": 15860
    },
    {
      "epoch": 0.5945824435202878,
      "grad_norm": 0.100039042532444,
      "learning_rate": 8.419433040722297e-05,
      "loss": 0.0905,
      "step": 15870
    },
    {
      "epoch": 0.5949571016447491,
      "grad_norm": 0.2691190838813782,
      "learning_rate": 8.41717254083221e-05,
      "loss": 0.0916,
      "step": 15880
    },
    {
      "epoch": 0.5953317597692106,
      "grad_norm": 0.07284002006053925,
      "learning_rate": 8.414910729563703e-05,
      "loss": 0.0941,
      "step": 15890
    },
    {
      "epoch": 0.5957064178936721,
      "grad_norm": 0.07872074097394943,
      "learning_rate": 8.412647607784762e-05,
      "loss": 0.0924,
      "step": 15900
    },
    {
      "epoch": 0.5960810760181334,
      "grad_norm": 0.0769670158624649,
      "learning_rate": 8.410383176363894e-05,
      "loss": 0.0911,
      "step": 15910
    },
    {
      "epoch": 0.5964557341425949,
      "grad_norm": 0.0820620208978653,
      "learning_rate": 8.408117436170096e-05,
      "loss": 0.0876,
      "step": 15920
    },
    {
      "epoch": 0.5968303922670564,
      "grad_norm": 0.10056507587432861,
      "learning_rate": 8.405850388072871e-05,
      "loss": 0.0941,
      "step": 15930
    },
    {
      "epoch": 0.5972050503915177,
      "grad_norm": 0.08700082451105118,
      "learning_rate": 8.403582032942226e-05,
      "loss": 0.0897,
      "step": 15940
    },
    {
      "epoch": 0.5975797085159792,
      "grad_norm": 0.09100421518087387,
      "learning_rate": 8.401312371648666e-05,
      "loss": 0.0906,
      "step": 15950
    },
    {
      "epoch": 0.5979543666404405,
      "grad_norm": 0.1110483705997467,
      "learning_rate": 8.399041405063201e-05,
      "loss": 0.0902,
      "step": 15960
    },
    {
      "epoch": 0.598329024764902,
      "grad_norm": 0.11751565337181091,
      "learning_rate": 8.396769134057339e-05,
      "loss": 0.0903,
      "step": 15970
    },
    {
      "epoch": 0.5987036828893635,
      "grad_norm": 0.12498179823160172,
      "learning_rate": 8.394495559503089e-05,
      "loss": 0.0954,
      "step": 15980
    },
    {
      "epoch": 0.5990783410138248,
      "grad_norm": 0.08252905309200287,
      "learning_rate": 8.392220682272962e-05,
      "loss": 0.0936,
      "step": 15990
    },
    {
      "epoch": 0.5994529991382863,
      "grad_norm": 0.09035209566354752,
      "learning_rate": 8.389944503239966e-05,
      "loss": 0.0896,
      "step": 16000
    },
    {
      "epoch": 0.5998276572627478,
      "grad_norm": 0.08680161833763123,
      "learning_rate": 8.387667023277612e-05,
      "loss": 0.0924,
      "step": 16010
    },
    {
      "epoch": 0.6002023153872091,
      "grad_norm": 0.06400823593139648,
      "learning_rate": 8.385388243259906e-05,
      "loss": 0.0855,
      "step": 16020
    },
    {
      "epoch": 0.6005769735116706,
      "grad_norm": 0.09836270660161972,
      "learning_rate": 8.383108164061361e-05,
      "loss": 0.0918,
      "step": 16030
    },
    {
      "epoch": 0.6009516316361321,
      "grad_norm": 0.10825089365243912,
      "learning_rate": 8.380826786556977e-05,
      "loss": 0.0922,
      "step": 16040
    },
    {
      "epoch": 0.6013262897605934,
      "grad_norm": 0.1075710654258728,
      "learning_rate": 8.378544111622262e-05,
      "loss": 0.0914,
      "step": 16050
    },
    {
      "epoch": 0.6017009478850549,
      "grad_norm": 0.08423485606908798,
      "learning_rate": 8.376260140133218e-05,
      "loss": 0.0903,
      "step": 16060
    },
    {
      "epoch": 0.6020756060095163,
      "grad_norm": 0.05999244749546051,
      "learning_rate": 8.373974872966341e-05,
      "loss": 0.0954,
      "step": 16070
    },
    {
      "epoch": 0.6024502641339777,
      "grad_norm": 0.07474619150161743,
      "learning_rate": 8.371688310998634e-05,
      "loss": 0.0914,
      "step": 16080
    },
    {
      "epoch": 0.6028249222584392,
      "grad_norm": 0.10097341239452362,
      "learning_rate": 8.369400455107584e-05,
      "loss": 0.0885,
      "step": 16090
    },
    {
      "epoch": 0.6031995803829006,
      "grad_norm": 0.08407288044691086,
      "learning_rate": 8.367111306171188e-05,
      "loss": 0.0956,
      "step": 16100
    },
    {
      "epoch": 0.603574238507362,
      "grad_norm": 0.1081371158361435,
      "learning_rate": 8.364820865067928e-05,
      "loss": 0.0894,
      "step": 16110
    },
    {
      "epoch": 0.6039488966318235,
      "grad_norm": 0.07586708664894104,
      "learning_rate": 8.362529132676791e-05,
      "loss": 0.0927,
      "step": 16120
    },
    {
      "epoch": 0.6043235547562849,
      "grad_norm": 0.08421047776937485,
      "learning_rate": 8.36023610987725e-05,
      "loss": 0.0956,
      "step": 16130
    },
    {
      "epoch": 0.6046982128807463,
      "grad_norm": 0.07299963384866714,
      "learning_rate": 8.35794179754928e-05,
      "loss": 0.0885,
      "step": 16140
    },
    {
      "epoch": 0.6050728710052078,
      "grad_norm": 0.06912711262702942,
      "learning_rate": 8.355646196573353e-05,
      "loss": 0.0912,
      "step": 16150
    },
    {
      "epoch": 0.6054475291296691,
      "grad_norm": 0.07181476056575775,
      "learning_rate": 8.353349307830428e-05,
      "loss": 0.0914,
      "step": 16160
    },
    {
      "epoch": 0.6058221872541306,
      "grad_norm": 0.22852283716201782,
      "learning_rate": 8.35105113220196e-05,
      "loss": 0.0968,
      "step": 16170
    },
    {
      "epoch": 0.6061968453785921,
      "grad_norm": 0.08500006049871445,
      "learning_rate": 8.348751670569903e-05,
      "loss": 0.0919,
      "step": 16180
    },
    {
      "epoch": 0.6065715035030534,
      "grad_norm": 0.07449325919151306,
      "learning_rate": 8.346450923816701e-05,
      "loss": 0.0955,
      "step": 16190
    },
    {
      "epoch": 0.6069461616275149,
      "grad_norm": 0.09716876596212387,
      "learning_rate": 8.344148892825294e-05,
      "loss": 0.0926,
      "step": 16200
    },
    {
      "epoch": 0.6073208197519763,
      "grad_norm": 0.09307972341775894,
      "learning_rate": 8.341845578479106e-05,
      "loss": 0.0889,
      "step": 16210
    },
    {
      "epoch": 0.6076954778764377,
      "grad_norm": 0.07818035036325455,
      "learning_rate": 8.339540981662065e-05,
      "loss": 0.0859,
      "step": 16220
    },
    {
      "epoch": 0.6080701360008992,
      "grad_norm": 0.09265580773353577,
      "learning_rate": 8.337235103258583e-05,
      "loss": 0.0895,
      "step": 16230
    },
    {
      "epoch": 0.6084447941253606,
      "grad_norm": 0.15701182186603546,
      "learning_rate": 8.334927944153567e-05,
      "loss": 0.0897,
      "step": 16240
    },
    {
      "epoch": 0.608819452249822,
      "grad_norm": 0.11745678633451462,
      "learning_rate": 8.332619505232417e-05,
      "loss": 0.0928,
      "step": 16250
    },
    {
      "epoch": 0.6091941103742835,
      "grad_norm": 0.09414710104465485,
      "learning_rate": 8.330309787381024e-05,
      "loss": 0.0931,
      "step": 16260
    },
    {
      "epoch": 0.6095687684987449,
      "grad_norm": 0.09373024106025696,
      "learning_rate": 8.327998791485763e-05,
      "loss": 0.091,
      "step": 16270
    },
    {
      "epoch": 0.6099434266232063,
      "grad_norm": 0.08687394112348557,
      "learning_rate": 8.325686518433508e-05,
      "loss": 0.092,
      "step": 16280
    },
    {
      "epoch": 0.6103180847476678,
      "grad_norm": 0.11166942119598389,
      "learning_rate": 8.32337296911162e-05,
      "loss": 0.0892,
      "step": 16290
    },
    {
      "epoch": 0.6106927428721292,
      "grad_norm": 0.09951819479465485,
      "learning_rate": 8.32105814440795e-05,
      "loss": 0.0917,
      "step": 16300
    },
    {
      "epoch": 0.6110674009965906,
      "grad_norm": 0.12878204882144928,
      "learning_rate": 8.318742045210834e-05,
      "loss": 0.0906,
      "step": 16310
    },
    {
      "epoch": 0.6114420591210521,
      "grad_norm": 0.07877501100301743,
      "learning_rate": 8.316424672409106e-05,
      "loss": 0.092,
      "step": 16320
    },
    {
      "epoch": 0.6118167172455135,
      "grad_norm": 0.09153178334236145,
      "learning_rate": 8.314106026892083e-05,
      "loss": 0.0894,
      "step": 16330
    },
    {
      "epoch": 0.6121913753699749,
      "grad_norm": 0.09364452213048935,
      "learning_rate": 8.311786109549569e-05,
      "loss": 0.0918,
      "step": 16340
    },
    {
      "epoch": 0.6125660334944363,
      "grad_norm": 0.07044399529695511,
      "learning_rate": 8.309464921271859e-05,
      "loss": 0.09,
      "step": 16350
    },
    {
      "epoch": 0.6129406916188977,
      "grad_norm": 0.07290782034397125,
      "learning_rate": 8.307142462949737e-05,
      "loss": 0.0929,
      "step": 16360
    },
    {
      "epoch": 0.6133153497433592,
      "grad_norm": 0.13498447835445404,
      "learning_rate": 8.304818735474471e-05,
      "loss": 0.0917,
      "step": 16370
    },
    {
      "epoch": 0.6136900078678206,
      "grad_norm": 0.08312181383371353,
      "learning_rate": 8.302493739737818e-05,
      "loss": 0.0917,
      "step": 16380
    },
    {
      "epoch": 0.614064665992282,
      "grad_norm": 0.08062487840652466,
      "learning_rate": 8.300167476632021e-05,
      "loss": 0.089,
      "step": 16390
    },
    {
      "epoch": 0.6144393241167435,
      "grad_norm": 0.07531629502773285,
      "learning_rate": 8.297839947049811e-05,
      "loss": 0.0921,
      "step": 16400
    },
    {
      "epoch": 0.6148139822412049,
      "grad_norm": 0.10232018679380417,
      "learning_rate": 8.295511151884401e-05,
      "loss": 0.0924,
      "step": 16410
    },
    {
      "epoch": 0.6151886403656663,
      "grad_norm": 0.07201625406742096,
      "learning_rate": 8.293181092029494e-05,
      "loss": 0.0928,
      "step": 16420
    },
    {
      "epoch": 0.6155632984901278,
      "grad_norm": 0.0912773534655571,
      "learning_rate": 8.290849768379279e-05,
      "loss": 0.0931,
      "step": 16430
    },
    {
      "epoch": 0.6159379566145892,
      "grad_norm": 0.08819358050823212,
      "learning_rate": 8.288517181828423e-05,
      "loss": 0.0907,
      "step": 16440
    },
    {
      "epoch": 0.6163126147390506,
      "grad_norm": 0.07523075491189957,
      "learning_rate": 8.286183333272084e-05,
      "loss": 0.0886,
      "step": 16450
    },
    {
      "epoch": 0.616687272863512,
      "grad_norm": 0.08305984735488892,
      "learning_rate": 8.283848223605905e-05,
      "loss": 0.0886,
      "step": 16460
    },
    {
      "epoch": 0.6170619309879735,
      "grad_norm": 0.07791759818792343,
      "learning_rate": 8.281511853726007e-05,
      "loss": 0.0928,
      "step": 16470
    },
    {
      "epoch": 0.6174365891124349,
      "grad_norm": 0.10205521434545517,
      "learning_rate": 8.279174224529003e-05,
      "loss": 0.095,
      "step": 16480
    },
    {
      "epoch": 0.6178112472368963,
      "grad_norm": 0.09094066172838211,
      "learning_rate": 8.276835336911979e-05,
      "loss": 0.0902,
      "step": 16490
    },
    {
      "epoch": 0.6181859053613578,
      "grad_norm": 0.0787530317902565,
      "learning_rate": 8.274495191772514e-05,
      "loss": 0.0945,
      "step": 16500
    },
    {
      "epoch": 0.6185605634858192,
      "grad_norm": 0.10158484429121017,
      "learning_rate": 8.272153790008661e-05,
      "loss": 0.0928,
      "step": 16510
    },
    {
      "epoch": 0.6189352216102806,
      "grad_norm": 0.13360625505447388,
      "learning_rate": 8.269811132518963e-05,
      "loss": 0.0894,
      "step": 16520
    },
    {
      "epoch": 0.619309879734742,
      "grad_norm": 0.16778914630413055,
      "learning_rate": 8.267467220202438e-05,
      "loss": 0.0913,
      "step": 16530
    },
    {
      "epoch": 0.6196845378592035,
      "grad_norm": 0.3074120581150055,
      "learning_rate": 8.265122053958592e-05,
      "loss": 0.0941,
      "step": 16540
    },
    {
      "epoch": 0.6200591959836649,
      "grad_norm": 0.08451668918132782,
      "learning_rate": 8.262775634687407e-05,
      "loss": 0.0913,
      "step": 16550
    },
    {
      "epoch": 0.6204338541081263,
      "grad_norm": 0.08225712925195694,
      "learning_rate": 8.260427963289348e-05,
      "loss": 0.0925,
      "step": 16560
    },
    {
      "epoch": 0.6208085122325878,
      "grad_norm": 0.07638025283813477,
      "learning_rate": 8.25807904066536e-05,
      "loss": 0.0914,
      "step": 16570
    },
    {
      "epoch": 0.6211831703570492,
      "grad_norm": 0.09158512949943542,
      "learning_rate": 8.255728867716869e-05,
      "loss": 0.0899,
      "step": 16580
    },
    {
      "epoch": 0.6215578284815106,
      "grad_norm": 0.072728231549263,
      "learning_rate": 8.253377445345783e-05,
      "loss": 0.0944,
      "step": 16590
    },
    {
      "epoch": 0.621932486605972,
      "grad_norm": 0.1354667842388153,
      "learning_rate": 8.25102477445448e-05,
      "loss": 0.0901,
      "step": 16600
    },
    {
      "epoch": 0.6223071447304335,
      "grad_norm": 0.08412391692399979,
      "learning_rate": 8.248670855945831e-05,
      "loss": 0.0942,
      "step": 16610
    },
    {
      "epoch": 0.6226818028548949,
      "grad_norm": 0.09246669709682465,
      "learning_rate": 8.246315690723173e-05,
      "loss": 0.0919,
      "step": 16620
    },
    {
      "epoch": 0.6230564609793563,
      "grad_norm": 0.08143550902605057,
      "learning_rate": 8.243959279690331e-05,
      "loss": 0.0915,
      "step": 16630
    },
    {
      "epoch": 0.6234311191038178,
      "grad_norm": 0.14370179176330566,
      "learning_rate": 8.241601623751603e-05,
      "loss": 0.0935,
      "step": 16640
    },
    {
      "epoch": 0.6238057772282792,
      "grad_norm": 0.09849212318658829,
      "learning_rate": 8.239242723811767e-05,
      "loss": 0.0937,
      "step": 16650
    },
    {
      "epoch": 0.6241804353527406,
      "grad_norm": 0.0928366631269455,
      "learning_rate": 8.236882580776075e-05,
      "loss": 0.093,
      "step": 16660
    },
    {
      "epoch": 0.6245550934772021,
      "grad_norm": 0.08391215652227402,
      "learning_rate": 8.23452119555026e-05,
      "loss": 0.0888,
      "step": 16670
    },
    {
      "epoch": 0.6249297516016635,
      "grad_norm": 0.09380543977022171,
      "learning_rate": 8.232158569040531e-05,
      "loss": 0.0935,
      "step": 16680
    },
    {
      "epoch": 0.6253044097261249,
      "grad_norm": 0.0799625888466835,
      "learning_rate": 8.229794702153571e-05,
      "loss": 0.0891,
      "step": 16690
    },
    {
      "epoch": 0.6256790678505864,
      "grad_norm": 0.07922448217868805,
      "learning_rate": 8.22742959579654e-05,
      "loss": 0.0909,
      "step": 16700
    },
    {
      "epoch": 0.6260537259750477,
      "grad_norm": 0.09941712766885757,
      "learning_rate": 8.225063250877079e-05,
      "loss": 0.0917,
      "step": 16710
    },
    {
      "epoch": 0.6264283840995092,
      "grad_norm": 0.07882058620452881,
      "learning_rate": 8.222695668303293e-05,
      "loss": 0.0903,
      "step": 16720
    },
    {
      "epoch": 0.6268030422239707,
      "grad_norm": 0.0921078771352768,
      "learning_rate": 8.220326848983773e-05,
      "loss": 0.0914,
      "step": 16730
    },
    {
      "epoch": 0.627177700348432,
      "grad_norm": 0.0755043774843216,
      "learning_rate": 8.217956793827575e-05,
      "loss": 0.0895,
      "step": 16740
    },
    {
      "epoch": 0.6275523584728935,
      "grad_norm": 0.11463562399148941,
      "learning_rate": 8.215585503744241e-05,
      "loss": 0.0938,
      "step": 16750
    },
    {
      "epoch": 0.627927016597355,
      "grad_norm": 0.11515578627586365,
      "learning_rate": 8.213212979643776e-05,
      "loss": 0.0898,
      "step": 16760
    },
    {
      "epoch": 0.6283016747218163,
      "grad_norm": 0.08474552631378174,
      "learning_rate": 8.210839222436664e-05,
      "loss": 0.0909,
      "step": 16770
    },
    {
      "epoch": 0.6286763328462778,
      "grad_norm": 0.09382610768079758,
      "learning_rate": 8.208464233033861e-05,
      "loss": 0.0927,
      "step": 16780
    },
    {
      "epoch": 0.6290509909707392,
      "grad_norm": 0.08380134403705597,
      "learning_rate": 8.206088012346796e-05,
      "loss": 0.0905,
      "step": 16790
    },
    {
      "epoch": 0.6294256490952006,
      "grad_norm": 0.08149988204240799,
      "learning_rate": 8.203710561287369e-05,
      "loss": 0.0889,
      "step": 16800
    },
    {
      "epoch": 0.6298003072196621,
      "grad_norm": 0.11140380054712296,
      "learning_rate": 8.201331880767954e-05,
      "loss": 0.0891,
      "step": 16810
    },
    {
      "epoch": 0.6301749653441235,
      "grad_norm": 0.11914663761854172,
      "learning_rate": 8.198951971701399e-05,
      "loss": 0.0925,
      "step": 16820
    },
    {
      "epoch": 0.6305496234685849,
      "grad_norm": 0.1315298080444336,
      "learning_rate": 8.196570835001018e-05,
      "loss": 0.0887,
      "step": 16830
    },
    {
      "epoch": 0.6309242815930464,
      "grad_norm": 0.07154175639152527,
      "learning_rate": 8.194188471580602e-05,
      "loss": 0.0938,
      "step": 16840
    },
    {
      "epoch": 0.6312989397175077,
      "grad_norm": 0.09430141746997833,
      "learning_rate": 8.191804882354408e-05,
      "loss": 0.095,
      "step": 16850
    },
    {
      "epoch": 0.6316735978419692,
      "grad_norm": 0.08857183158397675,
      "learning_rate": 8.189420068237164e-05,
      "loss": 0.0903,
      "step": 16860
    },
    {
      "epoch": 0.6320482559664307,
      "grad_norm": 0.06561184674501419,
      "learning_rate": 8.18703403014407e-05,
      "loss": 0.087,
      "step": 16870
    },
    {
      "epoch": 0.632422914090892,
      "grad_norm": 0.0897509977221489,
      "learning_rate": 8.184646768990795e-05,
      "loss": 0.0894,
      "step": 16880
    },
    {
      "epoch": 0.6327975722153535,
      "grad_norm": 0.07742781937122345,
      "learning_rate": 8.182258285693479e-05,
      "loss": 0.0924,
      "step": 16890
    },
    {
      "epoch": 0.633172230339815,
      "grad_norm": 0.1262010782957077,
      "learning_rate": 8.179868581168726e-05,
      "loss": 0.0899,
      "step": 16900
    },
    {
      "epoch": 0.6335468884642763,
      "grad_norm": 0.08192194998264313,
      "learning_rate": 8.177477656333616e-05,
      "loss": 0.0904,
      "step": 16910
    },
    {
      "epoch": 0.6339215465887378,
      "grad_norm": 0.1331128478050232,
      "learning_rate": 8.17508551210569e-05,
      "loss": 0.0862,
      "step": 16920
    },
    {
      "epoch": 0.6342962047131993,
      "grad_norm": 0.0842098817229271,
      "learning_rate": 8.172692149402962e-05,
      "loss": 0.092,
      "step": 16930
    },
    {
      "epoch": 0.6346708628376606,
      "grad_norm": 0.07798827439546585,
      "learning_rate": 8.170297569143913e-05,
      "loss": 0.0947,
      "step": 16940
    },
    {
      "epoch": 0.6350455209621221,
      "grad_norm": 0.10475778579711914,
      "learning_rate": 8.167901772247486e-05,
      "loss": 0.0946,
      "step": 16950
    },
    {
      "epoch": 0.6354201790865835,
      "grad_norm": 0.08175870776176453,
      "learning_rate": 8.165504759633099e-05,
      "loss": 0.0927,
      "step": 16960
    },
    {
      "epoch": 0.6357948372110449,
      "grad_norm": 0.08633478730916977,
      "learning_rate": 8.163106532220631e-05,
      "loss": 0.0943,
      "step": 16970
    },
    {
      "epoch": 0.6361694953355064,
      "grad_norm": 0.08260287344455719,
      "learning_rate": 8.16070709093043e-05,
      "loss": 0.0928,
      "step": 16980
    },
    {
      "epoch": 0.6365441534599677,
      "grad_norm": 0.09696637839078903,
      "learning_rate": 8.158306436683308e-05,
      "loss": 0.0937,
      "step": 16990
    },
    {
      "epoch": 0.6369188115844292,
      "grad_norm": 0.0736791342496872,
      "learning_rate": 8.155904570400538e-05,
      "loss": 0.089,
      "step": 17000
    },
    {
      "epoch": 0.6372934697088907,
      "grad_norm": 0.10233953595161438,
      "learning_rate": 8.153501493003872e-05,
      "loss": 0.0903,
      "step": 17010
    },
    {
      "epoch": 0.637668127833352,
      "grad_norm": 0.07804244011640549,
      "learning_rate": 8.151097205415513e-05,
      "loss": 0.0895,
      "step": 17020
    },
    {
      "epoch": 0.6380427859578135,
      "grad_norm": 0.10553185641765594,
      "learning_rate": 8.148691708558136e-05,
      "loss": 0.0941,
      "step": 17030
    },
    {
      "epoch": 0.638417444082275,
      "grad_norm": 0.09435397386550903,
      "learning_rate": 8.146285003354873e-05,
      "loss": 0.0924,
      "step": 17040
    },
    {
      "epoch": 0.6387921022067363,
      "grad_norm": 0.08422964066267014,
      "learning_rate": 8.143877090729329e-05,
      "loss": 0.0941,
      "step": 17050
    },
    {
      "epoch": 0.6391667603311978,
      "grad_norm": 0.07969632744789124,
      "learning_rate": 8.141467971605565e-05,
      "loss": 0.0921,
      "step": 17060
    },
    {
      "epoch": 0.6395414184556593,
      "grad_norm": 0.08604080975055695,
      "learning_rate": 8.139057646908108e-05,
      "loss": 0.0912,
      "step": 17070
    },
    {
      "epoch": 0.6399160765801206,
      "grad_norm": 0.07678554952144623,
      "learning_rate": 8.136646117561948e-05,
      "loss": 0.0906,
      "step": 17080
    },
    {
      "epoch": 0.6402907347045821,
      "grad_norm": 0.1380370557308197,
      "learning_rate": 8.134233384492535e-05,
      "loss": 0.0919,
      "step": 17090
    },
    {
      "epoch": 0.6406653928290434,
      "grad_norm": 0.08732911944389343,
      "learning_rate": 8.131819448625784e-05,
      "loss": 0.091,
      "step": 17100
    },
    {
      "epoch": 0.6410400509535049,
      "grad_norm": 0.11221770197153091,
      "learning_rate": 8.129404310888068e-05,
      "loss": 0.0917,
      "step": 17110
    },
    {
      "epoch": 0.6414147090779664,
      "grad_norm": 0.11292890459299088,
      "learning_rate": 8.126987972206226e-05,
      "loss": 0.0874,
      "step": 17120
    },
    {
      "epoch": 0.6417893672024277,
      "grad_norm": 0.11218444257974625,
      "learning_rate": 8.124570433507555e-05,
      "loss": 0.0909,
      "step": 17130
    },
    {
      "epoch": 0.6421640253268892,
      "grad_norm": 0.08096038550138474,
      "learning_rate": 8.122151695719809e-05,
      "loss": 0.0899,
      "step": 17140
    },
    {
      "epoch": 0.6425386834513507,
      "grad_norm": 0.09687554091215134,
      "learning_rate": 8.11973175977121e-05,
      "loss": 0.089,
      "step": 17150
    },
    {
      "epoch": 0.642913341575812,
      "grad_norm": 0.10014260560274124,
      "learning_rate": 8.117310626590434e-05,
      "loss": 0.0906,
      "step": 17160
    },
    {
      "epoch": 0.6432879997002735,
      "grad_norm": 0.1626431792974472,
      "learning_rate": 8.114888297106619e-05,
      "loss": 0.0927,
      "step": 17170
    },
    {
      "epoch": 0.643662657824735,
      "grad_norm": 0.06641888618469238,
      "learning_rate": 8.112464772249358e-05,
      "loss": 0.092,
      "step": 17180
    },
    {
      "epoch": 0.6440373159491963,
      "grad_norm": 0.10644754022359848,
      "learning_rate": 8.11004005294871e-05,
      "loss": 0.0937,
      "step": 17190
    },
    {
      "epoch": 0.6444119740736578,
      "grad_norm": 0.08080504089593887,
      "learning_rate": 8.107614140135187e-05,
      "loss": 0.094,
      "step": 17200
    },
    {
      "epoch": 0.6447866321981193,
      "grad_norm": 0.1010752022266388,
      "learning_rate": 8.105187034739761e-05,
      "loss": 0.0909,
      "step": 17210
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 0.09943053871393204,
      "learning_rate": 8.102758737693859e-05,
      "loss": 0.0943,
      "step": 17220
    },
    {
      "epoch": 0.6455359484470421,
      "grad_norm": 0.08028140664100647,
      "learning_rate": 8.100329249929366e-05,
      "loss": 0.0936,
      "step": 17230
    },
    {
      "epoch": 0.6459106065715035,
      "grad_norm": 0.07044041901826859,
      "learning_rate": 8.097898572378631e-05,
      "loss": 0.0941,
      "step": 17240
    },
    {
      "epoch": 0.6462852646959649,
      "grad_norm": 0.07866329699754715,
      "learning_rate": 8.095466705974449e-05,
      "loss": 0.0909,
      "step": 17250
    },
    {
      "epoch": 0.6466599228204264,
      "grad_norm": 0.08798770606517792,
      "learning_rate": 8.093033651650077e-05,
      "loss": 0.0928,
      "step": 17260
    },
    {
      "epoch": 0.6470345809448877,
      "grad_norm": 0.0748181864619255,
      "learning_rate": 8.09059941033923e-05,
      "loss": 0.0901,
      "step": 17270
    },
    {
      "epoch": 0.6474092390693492,
      "grad_norm": 0.07471845299005508,
      "learning_rate": 8.088163982976071e-05,
      "loss": 0.0954,
      "step": 17280
    },
    {
      "epoch": 0.6477838971938107,
      "grad_norm": 0.07771959900856018,
      "learning_rate": 8.085727370495225e-05,
      "loss": 0.0891,
      "step": 17290
    },
    {
      "epoch": 0.648158555318272,
      "grad_norm": 0.09628766030073166,
      "learning_rate": 8.08328957383177e-05,
      "loss": 0.089,
      "step": 17300
    },
    {
      "epoch": 0.6485332134427335,
      "grad_norm": 0.05810623615980148,
      "learning_rate": 8.080850593921236e-05,
      "loss": 0.0942,
      "step": 17310
    },
    {
      "epoch": 0.648907871567195,
      "grad_norm": 0.06506873667240143,
      "learning_rate": 8.07841043169961e-05,
      "loss": 0.091,
      "step": 17320
    },
    {
      "epoch": 0.6492825296916563,
      "grad_norm": 0.08029497414827347,
      "learning_rate": 8.075969088103334e-05,
      "loss": 0.092,
      "step": 17330
    },
    {
      "epoch": 0.6496571878161178,
      "grad_norm": 0.0936148390173912,
      "learning_rate": 8.073526564069297e-05,
      "loss": 0.0939,
      "step": 17340
    },
    {
      "epoch": 0.6500318459405792,
      "grad_norm": 0.09170481562614441,
      "learning_rate": 8.071082860534847e-05,
      "loss": 0.096,
      "step": 17350
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 0.07361823320388794,
      "learning_rate": 8.068637978437784e-05,
      "loss": 0.0902,
      "step": 17360
    },
    {
      "epoch": 0.6507811621895021,
      "grad_norm": 0.07220061123371124,
      "learning_rate": 8.066191918716356e-05,
      "loss": 0.0921,
      "step": 17370
    },
    {
      "epoch": 0.6511558203139635,
      "grad_norm": 0.090299591422081,
      "learning_rate": 8.06374468230927e-05,
      "loss": 0.0906,
      "step": 17380
    },
    {
      "epoch": 0.6515304784384249,
      "grad_norm": 0.08907389640808105,
      "learning_rate": 8.061296270155676e-05,
      "loss": 0.0924,
      "step": 17390
    },
    {
      "epoch": 0.6519051365628864,
      "grad_norm": 0.07859243452548981,
      "learning_rate": 8.058846683195183e-05,
      "loss": 0.0892,
      "step": 17400
    },
    {
      "epoch": 0.6522797946873478,
      "grad_norm": 0.13524273037910461,
      "learning_rate": 8.056395922367847e-05,
      "loss": 0.0883,
      "step": 17410
    },
    {
      "epoch": 0.6526544528118092,
      "grad_norm": 0.06992160528898239,
      "learning_rate": 8.053943988614175e-05,
      "loss": 0.0895,
      "step": 17420
    },
    {
      "epoch": 0.6530291109362707,
      "grad_norm": 0.11911080032587051,
      "learning_rate": 8.051490882875122e-05,
      "loss": 0.0861,
      "step": 17430
    },
    {
      "epoch": 0.653403769060732,
      "grad_norm": 0.06959903240203857,
      "learning_rate": 8.049036606092098e-05,
      "loss": 0.0884,
      "step": 17440
    },
    {
      "epoch": 0.6537784271851935,
      "grad_norm": 0.07742156833410263,
      "learning_rate": 8.046581159206957e-05,
      "loss": 0.092,
      "step": 17450
    },
    {
      "epoch": 0.654153085309655,
      "grad_norm": 0.1163753792643547,
      "learning_rate": 8.044124543162007e-05,
      "loss": 0.0894,
      "step": 17460
    },
    {
      "epoch": 0.6545277434341163,
      "grad_norm": 0.08407920598983765,
      "learning_rate": 8.041666758899999e-05,
      "loss": 0.094,
      "step": 17470
    },
    {
      "epoch": 0.6549024015585778,
      "grad_norm": 0.07472743093967438,
      "learning_rate": 8.039207807364137e-05,
      "loss": 0.0915,
      "step": 17480
    },
    {
      "epoch": 0.6552770596830392,
      "grad_norm": 0.06386947631835938,
      "learning_rate": 8.036747689498072e-05,
      "loss": 0.0918,
      "step": 17490
    },
    {
      "epoch": 0.6556517178075006,
      "grad_norm": 0.06385071575641632,
      "learning_rate": 8.0342864062459e-05,
      "loss": 0.0925,
      "step": 17500
    },
    {
      "epoch": 0.6560263759319621,
      "grad_norm": 0.07393170148134232,
      "learning_rate": 8.031823958552168e-05,
      "loss": 0.0888,
      "step": 17510
    },
    {
      "epoch": 0.6564010340564235,
      "grad_norm": 0.08716963976621628,
      "learning_rate": 8.029360347361868e-05,
      "loss": 0.0926,
      "step": 17520
    },
    {
      "epoch": 0.6567756921808849,
      "grad_norm": 0.07086996734142303,
      "learning_rate": 8.026895573620437e-05,
      "loss": 0.0925,
      "step": 17530
    },
    {
      "epoch": 0.6571503503053464,
      "grad_norm": 0.09642327576875687,
      "learning_rate": 8.024429638273759e-05,
      "loss": 0.0884,
      "step": 17540
    },
    {
      "epoch": 0.6575250084298078,
      "grad_norm": 0.07001423090696335,
      "learning_rate": 8.021962542268167e-05,
      "loss": 0.0906,
      "step": 17550
    },
    {
      "epoch": 0.6578996665542692,
      "grad_norm": 0.12375069409608841,
      "learning_rate": 8.019494286550438e-05,
      "loss": 0.094,
      "step": 17560
    },
    {
      "epoch": 0.6582743246787307,
      "grad_norm": 0.09215930849313736,
      "learning_rate": 8.017024872067788e-05,
      "loss": 0.0897,
      "step": 17570
    },
    {
      "epoch": 0.6586489828031921,
      "grad_norm": 0.09336814284324646,
      "learning_rate": 8.014554299767884e-05,
      "loss": 0.0914,
      "step": 17580
    },
    {
      "epoch": 0.6590236409276535,
      "grad_norm": 0.06951625645160675,
      "learning_rate": 8.01208257059884e-05,
      "loss": 0.0942,
      "step": 17590
    },
    {
      "epoch": 0.659398299052115,
      "grad_norm": 0.3943605124950409,
      "learning_rate": 8.009609685509208e-05,
      "loss": 0.096,
      "step": 17600
    },
    {
      "epoch": 0.6597729571765764,
      "grad_norm": 0.07787464559078217,
      "learning_rate": 8.007135645447982e-05,
      "loss": 0.0885,
      "step": 17610
    },
    {
      "epoch": 0.6601476153010378,
      "grad_norm": 0.07681814581155777,
      "learning_rate": 8.004660451364605e-05,
      "loss": 0.0848,
      "step": 17620
    },
    {
      "epoch": 0.6605222734254992,
      "grad_norm": 0.07022731006145477,
      "learning_rate": 8.002184104208964e-05,
      "loss": 0.0882,
      "step": 17630
    },
    {
      "epoch": 0.6608969315499607,
      "grad_norm": 0.21914264559745789,
      "learning_rate": 7.999706604931379e-05,
      "loss": 0.0926,
      "step": 17640
    },
    {
      "epoch": 0.6612715896744221,
      "grad_norm": 0.08918234705924988,
      "learning_rate": 7.997227954482622e-05,
      "loss": 0.0915,
      "step": 17650
    },
    {
      "epoch": 0.6616462477988835,
      "grad_norm": 0.12836779654026031,
      "learning_rate": 7.994748153813902e-05,
      "loss": 0.0891,
      "step": 17660
    },
    {
      "epoch": 0.662020905923345,
      "grad_norm": 0.08339924365282059,
      "learning_rate": 7.992267203876872e-05,
      "loss": 0.0919,
      "step": 17670
    },
    {
      "epoch": 0.6623955640478064,
      "grad_norm": 0.117747962474823,
      "learning_rate": 7.989785105623622e-05,
      "loss": 0.0872,
      "step": 17680
    },
    {
      "epoch": 0.6627702221722678,
      "grad_norm": 0.07066742330789566,
      "learning_rate": 7.987301860006687e-05,
      "loss": 0.0946,
      "step": 17690
    },
    {
      "epoch": 0.6631448802967292,
      "grad_norm": 0.08252795040607452,
      "learning_rate": 7.984817467979039e-05,
      "loss": 0.0969,
      "step": 17700
    },
    {
      "epoch": 0.6635195384211907,
      "grad_norm": 0.08875001221895218,
      "learning_rate": 7.982331930494092e-05,
      "loss": 0.0914,
      "step": 17710
    },
    {
      "epoch": 0.6638941965456521,
      "grad_norm": 0.09031330794095993,
      "learning_rate": 7.9798452485057e-05,
      "loss": 0.0904,
      "step": 17720
    },
    {
      "epoch": 0.6642688546701135,
      "grad_norm": 0.07486645877361298,
      "learning_rate": 7.977357422968153e-05,
      "loss": 0.0928,
      "step": 17730
    },
    {
      "epoch": 0.6646435127945749,
      "grad_norm": 0.08744975924491882,
      "learning_rate": 7.974868454836184e-05,
      "loss": 0.0895,
      "step": 17740
    },
    {
      "epoch": 0.6650181709190364,
      "grad_norm": 0.08961834013462067,
      "learning_rate": 7.972378345064962e-05,
      "loss": 0.0922,
      "step": 17750
    },
    {
      "epoch": 0.6653928290434978,
      "grad_norm": 0.08629605174064636,
      "learning_rate": 7.969887094610093e-05,
      "loss": 0.0916,
      "step": 17760
    },
    {
      "epoch": 0.6657674871679592,
      "grad_norm": 0.09212782979011536,
      "learning_rate": 7.967394704427626e-05,
      "loss": 0.0937,
      "step": 17770
    },
    {
      "epoch": 0.6661421452924207,
      "grad_norm": 0.09018470346927643,
      "learning_rate": 7.964901175474042e-05,
      "loss": 0.0919,
      "step": 17780
    },
    {
      "epoch": 0.6665168034168821,
      "grad_norm": 0.07132963091135025,
      "learning_rate": 7.962406508706259e-05,
      "loss": 0.0906,
      "step": 17790
    },
    {
      "epoch": 0.6668914615413435,
      "grad_norm": 0.11455772817134857,
      "learning_rate": 7.959910705081637e-05,
      "loss": 0.092,
      "step": 17800
    },
    {
      "epoch": 0.667266119665805,
      "grad_norm": 0.29551956057548523,
      "learning_rate": 7.957413765557964e-05,
      "loss": 0.0935,
      "step": 17810
    },
    {
      "epoch": 0.6676407777902664,
      "grad_norm": 0.08896352350711823,
      "learning_rate": 7.954915691093472e-05,
      "loss": 0.0873,
      "step": 17820
    },
    {
      "epoch": 0.6680154359147278,
      "grad_norm": 0.07617591321468353,
      "learning_rate": 7.952416482646825e-05,
      "loss": 0.0889,
      "step": 17830
    },
    {
      "epoch": 0.6683900940391893,
      "grad_norm": 0.06909539550542831,
      "learning_rate": 7.949916141177123e-05,
      "loss": 0.0864,
      "step": 17840
    },
    {
      "epoch": 0.6687647521636507,
      "grad_norm": 0.10290297865867615,
      "learning_rate": 7.947414667643897e-05,
      "loss": 0.089,
      "step": 17850
    },
    {
      "epoch": 0.6691394102881121,
      "grad_norm": 0.1685328185558319,
      "learning_rate": 7.944912063007119e-05,
      "loss": 0.0918,
      "step": 17860
    },
    {
      "epoch": 0.6695140684125735,
      "grad_norm": 0.2076161652803421,
      "learning_rate": 7.942408328227191e-05,
      "loss": 0.0974,
      "step": 17870
    },
    {
      "epoch": 0.6698887265370349,
      "grad_norm": 0.09655579924583435,
      "learning_rate": 7.939903464264948e-05,
      "loss": 0.0942,
      "step": 17880
    },
    {
      "epoch": 0.6702633846614964,
      "grad_norm": 0.07074611634016037,
      "learning_rate": 7.93739747208166e-05,
      "loss": 0.0915,
      "step": 17890
    },
    {
      "epoch": 0.6706380427859578,
      "grad_norm": 0.07721392810344696,
      "learning_rate": 7.934890352639032e-05,
      "loss": 0.0921,
      "step": 17900
    },
    {
      "epoch": 0.6710127009104192,
      "grad_norm": 0.08259755373001099,
      "learning_rate": 7.932382106899197e-05,
      "loss": 0.0922,
      "step": 17910
    },
    {
      "epoch": 0.6713873590348807,
      "grad_norm": 0.07250659912824631,
      "learning_rate": 7.929872735824723e-05,
      "loss": 0.0888,
      "step": 17920
    },
    {
      "epoch": 0.6717620171593421,
      "grad_norm": 0.08461538702249527,
      "learning_rate": 7.927362240378609e-05,
      "loss": 0.0903,
      "step": 17930
    },
    {
      "epoch": 0.6721366752838035,
      "grad_norm": 0.07542063295841217,
      "learning_rate": 7.924850621524286e-05,
      "loss": 0.0938,
      "step": 17940
    },
    {
      "epoch": 0.672511333408265,
      "grad_norm": 0.07151507586240768,
      "learning_rate": 7.922337880225616e-05,
      "loss": 0.0912,
      "step": 17950
    },
    {
      "epoch": 0.6728859915327264,
      "grad_norm": 0.09247763454914093,
      "learning_rate": 7.919824017446893e-05,
      "loss": 0.0917,
      "step": 17960
    },
    {
      "epoch": 0.6732606496571878,
      "grad_norm": 0.06738470494747162,
      "learning_rate": 7.91730903415284e-05,
      "loss": 0.0914,
      "step": 17970
    },
    {
      "epoch": 0.6736353077816493,
      "grad_norm": 0.16461002826690674,
      "learning_rate": 7.914792931308607e-05,
      "loss": 0.0929,
      "step": 17980
    },
    {
      "epoch": 0.6740099659061106,
      "grad_norm": 0.09462891519069672,
      "learning_rate": 7.912275709879783e-05,
      "loss": 0.0905,
      "step": 17990
    },
    {
      "epoch": 0.6743846240305721,
      "grad_norm": 0.10850405693054199,
      "learning_rate": 7.909757370832373e-05,
      "loss": 0.089,
      "step": 18000
    },
    {
      "epoch": 0.6747592821550336,
      "grad_norm": 0.0926852896809578,
      "learning_rate": 7.907237915132823e-05,
      "loss": 0.0935,
      "step": 18010
    },
    {
      "epoch": 0.6751339402794949,
      "grad_norm": 0.08075936138629913,
      "learning_rate": 7.904717343748e-05,
      "loss": 0.0904,
      "step": 18020
    },
    {
      "epoch": 0.6755085984039564,
      "grad_norm": 0.15915493667125702,
      "learning_rate": 7.902195657645203e-05,
      "loss": 0.09,
      "step": 18030
    },
    {
      "epoch": 0.6758832565284179,
      "grad_norm": 0.06959390640258789,
      "learning_rate": 7.899672857792156e-05,
      "loss": 0.0902,
      "step": 18040
    },
    {
      "epoch": 0.6762579146528792,
      "grad_norm": 0.08733821660280228,
      "learning_rate": 7.897148945157013e-05,
      "loss": 0.0921,
      "step": 18050
    },
    {
      "epoch": 0.6766325727773407,
      "grad_norm": 0.09321776777505875,
      "learning_rate": 7.894623920708353e-05,
      "loss": 0.0907,
      "step": 18060
    },
    {
      "epoch": 0.6770072309018021,
      "grad_norm": 0.09247303009033203,
      "learning_rate": 7.892097785415183e-05,
      "loss": 0.0885,
      "step": 18070
    },
    {
      "epoch": 0.6773818890262635,
      "grad_norm": 0.14906840026378632,
      "learning_rate": 7.889570540246937e-05,
      "loss": 0.0936,
      "step": 18080
    },
    {
      "epoch": 0.677756547150725,
      "grad_norm": 0.06603995710611343,
      "learning_rate": 7.887042186173472e-05,
      "loss": 0.0932,
      "step": 18090
    },
    {
      "epoch": 0.6781312052751864,
      "grad_norm": 0.08021026104688644,
      "learning_rate": 7.884512724165072e-05,
      "loss": 0.0937,
      "step": 18100
    },
    {
      "epoch": 0.6785058633996478,
      "grad_norm": 0.09069515019655228,
      "learning_rate": 7.881982155192449e-05,
      "loss": 0.0918,
      "step": 18110
    },
    {
      "epoch": 0.6788805215241093,
      "grad_norm": 0.0796542763710022,
      "learning_rate": 7.879450480226736e-05,
      "loss": 0.0903,
      "step": 18120
    },
    {
      "epoch": 0.6792551796485706,
      "grad_norm": 0.10582773387432098,
      "learning_rate": 7.87691770023949e-05,
      "loss": 0.0937,
      "step": 18130
    },
    {
      "epoch": 0.6796298377730321,
      "grad_norm": 0.0755147859454155,
      "learning_rate": 7.874383816202697e-05,
      "loss": 0.0945,
      "step": 18140
    },
    {
      "epoch": 0.6800044958974936,
      "grad_norm": 0.0626816600561142,
      "learning_rate": 7.871848829088761e-05,
      "loss": 0.0964,
      "step": 18150
    },
    {
      "epoch": 0.6803791540219549,
      "grad_norm": 0.11815783381462097,
      "learning_rate": 7.869312739870514e-05,
      "loss": 0.0946,
      "step": 18160
    },
    {
      "epoch": 0.6807538121464164,
      "grad_norm": 0.08760679513216019,
      "learning_rate": 7.866775549521206e-05,
      "loss": 0.0908,
      "step": 18170
    },
    {
      "epoch": 0.6811284702708779,
      "grad_norm": 0.08689704537391663,
      "learning_rate": 7.864237259014514e-05,
      "loss": 0.0893,
      "step": 18180
    },
    {
      "epoch": 0.6815031283953392,
      "grad_norm": 0.1863769143819809,
      "learning_rate": 7.861697869324536e-05,
      "loss": 0.0909,
      "step": 18190
    },
    {
      "epoch": 0.6818777865198007,
      "grad_norm": 0.06857661157846451,
      "learning_rate": 7.859157381425792e-05,
      "loss": 0.0922,
      "step": 18200
    },
    {
      "epoch": 0.6822524446442622,
      "grad_norm": 0.09440770745277405,
      "learning_rate": 7.856615796293221e-05,
      "loss": 0.0885,
      "step": 18210
    },
    {
      "epoch": 0.6826271027687235,
      "grad_norm": 0.0778375044465065,
      "learning_rate": 7.854073114902185e-05,
      "loss": 0.0881,
      "step": 18220
    },
    {
      "epoch": 0.683001760893185,
      "grad_norm": 0.1447293907403946,
      "learning_rate": 7.851529338228467e-05,
      "loss": 0.0889,
      "step": 18230
    },
    {
      "epoch": 0.6833764190176465,
      "grad_norm": 0.07047969102859497,
      "learning_rate": 7.84898446724827e-05,
      "loss": 0.0908,
      "step": 18240
    },
    {
      "epoch": 0.6837510771421078,
      "grad_norm": 0.09324189275503159,
      "learning_rate": 7.846438502938218e-05,
      "loss": 0.0908,
      "step": 18250
    },
    {
      "epoch": 0.6841257352665693,
      "grad_norm": 0.05796809494495392,
      "learning_rate": 7.843891446275352e-05,
      "loss": 0.0889,
      "step": 18260
    },
    {
      "epoch": 0.6845003933910306,
      "grad_norm": 0.09634923189878464,
      "learning_rate": 7.841343298237133e-05,
      "loss": 0.0927,
      "step": 18270
    },
    {
      "epoch": 0.6848750515154921,
      "grad_norm": 0.07804077863693237,
      "learning_rate": 7.838794059801444e-05,
      "loss": 0.0951,
      "step": 18280
    },
    {
      "epoch": 0.6852497096399536,
      "grad_norm": 0.09084181487560272,
      "learning_rate": 7.836243731946584e-05,
      "loss": 0.093,
      "step": 18290
    },
    {
      "epoch": 0.6856243677644149,
      "grad_norm": 0.08514400571584702,
      "learning_rate": 7.833692315651266e-05,
      "loss": 0.0931,
      "step": 18300
    },
    {
      "epoch": 0.6859990258888764,
      "grad_norm": 0.09908369183540344,
      "learning_rate": 7.831139811894627e-05,
      "loss": 0.0905,
      "step": 18310
    },
    {
      "epoch": 0.6863736840133379,
      "grad_norm": 0.07730059325695038,
      "learning_rate": 7.828586221656223e-05,
      "loss": 0.0906,
      "step": 18320
    },
    {
      "epoch": 0.6867483421377992,
      "grad_norm": 0.09312129765748978,
      "learning_rate": 7.826031545916015e-05,
      "loss": 0.0894,
      "step": 18330
    },
    {
      "epoch": 0.6871230002622607,
      "grad_norm": 0.08154337108135223,
      "learning_rate": 7.823475785654396e-05,
      "loss": 0.0917,
      "step": 18340
    },
    {
      "epoch": 0.6874976583867222,
      "grad_norm": 0.08185885101556778,
      "learning_rate": 7.820918941852162e-05,
      "loss": 0.0915,
      "step": 18350
    },
    {
      "epoch": 0.6878723165111835,
      "grad_norm": 0.0675671249628067,
      "learning_rate": 7.818361015490537e-05,
      "loss": 0.0891,
      "step": 18360
    },
    {
      "epoch": 0.688246974635645,
      "grad_norm": 0.07670856267213821,
      "learning_rate": 7.815802007551148e-05,
      "loss": 0.0913,
      "step": 18370
    },
    {
      "epoch": 0.6886216327601064,
      "grad_norm": 0.08614492416381836,
      "learning_rate": 7.813241919016044e-05,
      "loss": 0.0918,
      "step": 18380
    },
    {
      "epoch": 0.6889962908845678,
      "grad_norm": 0.1278044581413269,
      "learning_rate": 7.810680750867691e-05,
      "loss": 0.0932,
      "step": 18390
    },
    {
      "epoch": 0.6893709490090293,
      "grad_norm": 0.07896668463945389,
      "learning_rate": 7.808118504088963e-05,
      "loss": 0.0904,
      "step": 18400
    },
    {
      "epoch": 0.6897456071334906,
      "grad_norm": 0.24004195630550385,
      "learning_rate": 7.805555179663154e-05,
      "loss": 0.09,
      "step": 18410
    },
    {
      "epoch": 0.6901202652579521,
      "grad_norm": 0.08014985173940659,
      "learning_rate": 7.802990778573965e-05,
      "loss": 0.0913,
      "step": 18420
    },
    {
      "epoch": 0.6904949233824136,
      "grad_norm": 0.07389505952596664,
      "learning_rate": 7.800425301805518e-05,
      "loss": 0.0928,
      "step": 18430
    },
    {
      "epoch": 0.6908695815068749,
      "grad_norm": 0.25677433609962463,
      "learning_rate": 7.79785875034234e-05,
      "loss": 0.088,
      "step": 18440
    },
    {
      "epoch": 0.6912442396313364,
      "grad_norm": 0.11654819548130035,
      "learning_rate": 7.795291125169375e-05,
      "loss": 0.0942,
      "step": 18450
    },
    {
      "epoch": 0.6916188977557979,
      "grad_norm": 0.08912783116102219,
      "learning_rate": 7.79272242727198e-05,
      "loss": 0.0925,
      "step": 18460
    },
    {
      "epoch": 0.6919935558802592,
      "grad_norm": 0.08138425648212433,
      "learning_rate": 7.790152657635919e-05,
      "loss": 0.095,
      "step": 18470
    },
    {
      "epoch": 0.6923682140047207,
      "grad_norm": 0.08967432379722595,
      "learning_rate": 7.78758181724737e-05,
      "loss": 0.091,
      "step": 18480
    },
    {
      "epoch": 0.6927428721291822,
      "grad_norm": 0.07038255035877228,
      "learning_rate": 7.785009907092924e-05,
      "loss": 0.0916,
      "step": 18490
    },
    {
      "epoch": 0.6931175302536435,
      "grad_norm": 0.08141007274389267,
      "learning_rate": 7.78243692815958e-05,
      "loss": 0.0955,
      "step": 18500
    },
    {
      "epoch": 0.693492188378105,
      "grad_norm": 0.07805538922548294,
      "learning_rate": 7.779862881434746e-05,
      "loss": 0.0873,
      "step": 18510
    },
    {
      "epoch": 0.6938668465025664,
      "grad_norm": 0.05677369609475136,
      "learning_rate": 7.777287767906244e-05,
      "loss": 0.0942,
      "step": 18520
    },
    {
      "epoch": 0.6942415046270278,
      "grad_norm": 0.08180143684148788,
      "learning_rate": 7.7747115885623e-05,
      "loss": 0.0929,
      "step": 18530
    },
    {
      "epoch": 0.6946161627514893,
      "grad_norm": 0.07156678289175034,
      "learning_rate": 7.772134344391554e-05,
      "loss": 0.0888,
      "step": 18540
    },
    {
      "epoch": 0.6949908208759507,
      "grad_norm": 0.06937718391418457,
      "learning_rate": 7.769556036383051e-05,
      "loss": 0.0913,
      "step": 18550
    },
    {
      "epoch": 0.6953654790004121,
      "grad_norm": 0.0867878794670105,
      "learning_rate": 7.766976665526247e-05,
      "loss": 0.0922,
      "step": 18560
    },
    {
      "epoch": 0.6957401371248736,
      "grad_norm": 0.07979292422533035,
      "learning_rate": 7.764396232811004e-05,
      "loss": 0.0897,
      "step": 18570
    },
    {
      "epoch": 0.696114795249335,
      "grad_norm": 0.07141793519258499,
      "learning_rate": 7.761814739227592e-05,
      "loss": 0.0908,
      "step": 18580
    },
    {
      "epoch": 0.6964894533737964,
      "grad_norm": 0.06935326009988785,
      "learning_rate": 7.759232185766689e-05,
      "loss": 0.09,
      "step": 18590
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 0.09066224843263626,
      "learning_rate": 7.756648573419376e-05,
      "loss": 0.0898,
      "step": 18600
    },
    {
      "epoch": 0.6972387696227192,
      "grad_norm": 0.10015896707773209,
      "learning_rate": 7.754063903177146e-05,
      "loss": 0.0924,
      "step": 18610
    },
    {
      "epoch": 0.6976134277471807,
      "grad_norm": 0.10962773859500885,
      "learning_rate": 7.751478176031897e-05,
      "loss": 0.0909,
      "step": 18620
    },
    {
      "epoch": 0.6979880858716421,
      "grad_norm": 0.10364118218421936,
      "learning_rate": 7.748891392975928e-05,
      "loss": 0.0926,
      "step": 18630
    },
    {
      "epoch": 0.6983627439961035,
      "grad_norm": 0.0691472515463829,
      "learning_rate": 7.746303555001946e-05,
      "loss": 0.0918,
      "step": 18640
    },
    {
      "epoch": 0.698737402120565,
      "grad_norm": 0.09821905195713043,
      "learning_rate": 7.743714663103064e-05,
      "loss": 0.0912,
      "step": 18650
    },
    {
      "epoch": 0.6991120602450264,
      "grad_norm": 0.09366057813167572,
      "learning_rate": 7.741124718272799e-05,
      "loss": 0.0912,
      "step": 18660
    },
    {
      "epoch": 0.6994867183694878,
      "grad_norm": 0.07295656949281693,
      "learning_rate": 7.738533721505069e-05,
      "loss": 0.0891,
      "step": 18670
    },
    {
      "epoch": 0.6998613764939493,
      "grad_norm": 0.15252891182899475,
      "learning_rate": 7.735941673794201e-05,
      "loss": 0.0956,
      "step": 18680
    },
    {
      "epoch": 0.7002360346184107,
      "grad_norm": 0.07282322645187378,
      "learning_rate": 7.73334857613492e-05,
      "loss": 0.0899,
      "step": 18690
    },
    {
      "epoch": 0.7006106927428721,
      "grad_norm": 0.09548067301511765,
      "learning_rate": 7.73075442952236e-05,
      "loss": 0.0904,
      "step": 18700
    },
    {
      "epoch": 0.7009853508673336,
      "grad_norm": 0.07998739928007126,
      "learning_rate": 7.72815923495205e-05,
      "loss": 0.0924,
      "step": 18710
    },
    {
      "epoch": 0.701360008991795,
      "grad_norm": 0.10007772594690323,
      "learning_rate": 7.725562993419928e-05,
      "loss": 0.0934,
      "step": 18720
    },
    {
      "epoch": 0.7017346671162564,
      "grad_norm": 0.0910729169845581,
      "learning_rate": 7.722965705922329e-05,
      "loss": 0.0912,
      "step": 18730
    },
    {
      "epoch": 0.7021093252407179,
      "grad_norm": 0.08530335128307343,
      "learning_rate": 7.720367373455992e-05,
      "loss": 0.0874,
      "step": 18740
    },
    {
      "epoch": 0.7024839833651793,
      "grad_norm": 0.06875311583280563,
      "learning_rate": 7.717767997018057e-05,
      "loss": 0.0915,
      "step": 18750
    },
    {
      "epoch": 0.7028586414896407,
      "grad_norm": 0.06951826065778732,
      "learning_rate": 7.715167577606063e-05,
      "loss": 0.0892,
      "step": 18760
    },
    {
      "epoch": 0.7032332996141021,
      "grad_norm": 0.0691346749663353,
      "learning_rate": 7.712566116217951e-05,
      "loss": 0.0943,
      "step": 18770
    },
    {
      "epoch": 0.7036079577385635,
      "grad_norm": 0.06732594221830368,
      "learning_rate": 7.70996361385206e-05,
      "loss": 0.0907,
      "step": 18780
    },
    {
      "epoch": 0.703982615863025,
      "grad_norm": 0.07993783056735992,
      "learning_rate": 7.70736007150713e-05,
      "loss": 0.0941,
      "step": 18790
    },
    {
      "epoch": 0.7043572739874864,
      "grad_norm": 0.08543748408555984,
      "learning_rate": 7.7047554901823e-05,
      "loss": 0.0877,
      "step": 18800
    },
    {
      "epoch": 0.7047319321119478,
      "grad_norm": 0.07578646391630173,
      "learning_rate": 7.702149870877109e-05,
      "loss": 0.0956,
      "step": 18810
    },
    {
      "epoch": 0.7051065902364093,
      "grad_norm": 0.12790469825267792,
      "learning_rate": 7.69954321459149e-05,
      "loss": 0.0933,
      "step": 18820
    },
    {
      "epoch": 0.7054812483608707,
      "grad_norm": 0.10592427104711533,
      "learning_rate": 7.696935522325777e-05,
      "loss": 0.0974,
      "step": 18830
    },
    {
      "epoch": 0.7058559064853321,
      "grad_norm": 0.07783590257167816,
      "learning_rate": 7.694326795080702e-05,
      "loss": 0.0892,
      "step": 18840
    },
    {
      "epoch": 0.7062305646097936,
      "grad_norm": 0.09041160345077515,
      "learning_rate": 7.691717033857396e-05,
      "loss": 0.0941,
      "step": 18850
    },
    {
      "epoch": 0.706605222734255,
      "grad_norm": 0.09037995338439941,
      "learning_rate": 7.689106239657378e-05,
      "loss": 0.0877,
      "step": 18860
    },
    {
      "epoch": 0.7069798808587164,
      "grad_norm": 0.10449404269456863,
      "learning_rate": 7.686494413482574e-05,
      "loss": 0.091,
      "step": 18870
    },
    {
      "epoch": 0.7073545389831779,
      "grad_norm": 0.09566602110862732,
      "learning_rate": 7.683881556335303e-05,
      "loss": 0.0931,
      "step": 18880
    },
    {
      "epoch": 0.7077291971076393,
      "grad_norm": 0.06455706804990768,
      "learning_rate": 7.681267669218277e-05,
      "loss": 0.0903,
      "step": 18890
    },
    {
      "epoch": 0.7081038552321007,
      "grad_norm": 0.055193450301885605,
      "learning_rate": 7.6786527531346e-05,
      "loss": 0.0906,
      "step": 18900
    },
    {
      "epoch": 0.7084785133565621,
      "grad_norm": 0.07889726012945175,
      "learning_rate": 7.676036809087784e-05,
      "loss": 0.0914,
      "step": 18910
    },
    {
      "epoch": 0.7088531714810236,
      "grad_norm": 0.07544142752885818,
      "learning_rate": 7.673419838081719e-05,
      "loss": 0.096,
      "step": 18920
    },
    {
      "epoch": 0.709227829605485,
      "grad_norm": 0.07173798233270645,
      "learning_rate": 7.670801841120702e-05,
      "loss": 0.0935,
      "step": 18930
    },
    {
      "epoch": 0.7096024877299464,
      "grad_norm": 0.096489317715168,
      "learning_rate": 7.668182819209418e-05,
      "loss": 0.0913,
      "step": 18940
    },
    {
      "epoch": 0.7099771458544079,
      "grad_norm": 0.08763432502746582,
      "learning_rate": 7.665562773352944e-05,
      "loss": 0.0936,
      "step": 18950
    },
    {
      "epoch": 0.7103518039788693,
      "grad_norm": 0.06857288628816605,
      "learning_rate": 7.662941704556754e-05,
      "loss": 0.0939,
      "step": 18960
    },
    {
      "epoch": 0.7107264621033307,
      "grad_norm": 0.06461270898580551,
      "learning_rate": 7.660319613826711e-05,
      "loss": 0.0916,
      "step": 18970
    },
    {
      "epoch": 0.7111011202277921,
      "grad_norm": 0.10116583853960037,
      "learning_rate": 7.657696502169075e-05,
      "loss": 0.0918,
      "step": 18980
    },
    {
      "epoch": 0.7114757783522536,
      "grad_norm": 0.14619562029838562,
      "learning_rate": 7.655072370590493e-05,
      "loss": 0.0909,
      "step": 18990
    },
    {
      "epoch": 0.711850436476715,
      "grad_norm": 0.07403599470853806,
      "learning_rate": 7.652447220098002e-05,
      "loss": 0.0885,
      "step": 19000
    },
    {
      "epoch": 0.7122250946011764,
      "grad_norm": 0.08876659721136093,
      "learning_rate": 7.649821051699038e-05,
      "loss": 0.0957,
      "step": 19010
    },
    {
      "epoch": 0.7125997527256378,
      "grad_norm": 0.08255782723426819,
      "learning_rate": 7.647193866401419e-05,
      "loss": 0.0897,
      "step": 19020
    },
    {
      "epoch": 0.7129744108500993,
      "grad_norm": 0.12056943029165268,
      "learning_rate": 7.64456566521336e-05,
      "loss": 0.0944,
      "step": 19030
    },
    {
      "epoch": 0.7133490689745607,
      "grad_norm": 0.0927157998085022,
      "learning_rate": 7.64193644914346e-05,
      "loss": 0.0906,
      "step": 19040
    },
    {
      "epoch": 0.7137237270990221,
      "grad_norm": 0.0787065327167511,
      "learning_rate": 7.639306219200712e-05,
      "loss": 0.0943,
      "step": 19050
    },
    {
      "epoch": 0.7140983852234836,
      "grad_norm": 0.09293581545352936,
      "learning_rate": 7.636674976394495e-05,
      "loss": 0.0926,
      "step": 19060
    },
    {
      "epoch": 0.714473043347945,
      "grad_norm": 0.06517034769058228,
      "learning_rate": 7.634042721734577e-05,
      "loss": 0.0894,
      "step": 19070
    },
    {
      "epoch": 0.7148477014724064,
      "grad_norm": 0.07839565724134445,
      "learning_rate": 7.631409456231122e-05,
      "loss": 0.0905,
      "step": 19080
    },
    {
      "epoch": 0.7152223595968679,
      "grad_norm": 0.06837870925664902,
      "learning_rate": 7.628775180894665e-05,
      "loss": 0.0884,
      "step": 19090
    },
    {
      "epoch": 0.7155970177213293,
      "grad_norm": 0.11186716705560684,
      "learning_rate": 7.626139896736149e-05,
      "loss": 0.0885,
      "step": 19100
    },
    {
      "epoch": 0.7159716758457907,
      "grad_norm": 0.08769136667251587,
      "learning_rate": 7.623503604766886e-05,
      "loss": 0.089,
      "step": 19110
    },
    {
      "epoch": 0.7163463339702522,
      "grad_norm": 0.08632951229810715,
      "learning_rate": 7.620866305998586e-05,
      "loss": 0.0947,
      "step": 19120
    },
    {
      "epoch": 0.7167209920947136,
      "grad_norm": 0.08736089617013931,
      "learning_rate": 7.618228001443343e-05,
      "loss": 0.0926,
      "step": 19130
    },
    {
      "epoch": 0.717095650219175,
      "grad_norm": 0.0616341307759285,
      "learning_rate": 7.615588692113634e-05,
      "loss": 0.0876,
      "step": 19140
    },
    {
      "epoch": 0.7174703083436365,
      "grad_norm": 0.06737249344587326,
      "learning_rate": 7.612948379022327e-05,
      "loss": 0.0916,
      "step": 19150
    },
    {
      "epoch": 0.7178449664680978,
      "grad_norm": 0.0681602880358696,
      "learning_rate": 7.610307063182667e-05,
      "loss": 0.0908,
      "step": 19160
    },
    {
      "epoch": 0.7182196245925593,
      "grad_norm": 0.08212166279554367,
      "learning_rate": 7.607664745608292e-05,
      "loss": 0.0891,
      "step": 19170
    },
    {
      "epoch": 0.7185942827170207,
      "grad_norm": 0.08955121785402298,
      "learning_rate": 7.605021427313222e-05,
      "loss": 0.0917,
      "step": 19180
    },
    {
      "epoch": 0.7189689408414821,
      "grad_norm": 0.08621536195278168,
      "learning_rate": 7.602377109311856e-05,
      "loss": 0.0903,
      "step": 19190
    },
    {
      "epoch": 0.7193435989659436,
      "grad_norm": 0.07648324966430664,
      "learning_rate": 7.599731792618983e-05,
      "loss": 0.0903,
      "step": 19200
    },
    {
      "epoch": 0.719718257090405,
      "grad_norm": 0.08146399259567261,
      "learning_rate": 7.597085478249775e-05,
      "loss": 0.089,
      "step": 19210
    },
    {
      "epoch": 0.7200929152148664,
      "grad_norm": 0.08609850704669952,
      "learning_rate": 7.594438167219784e-05,
      "loss": 0.0891,
      "step": 19220
    },
    {
      "epoch": 0.7204675733393279,
      "grad_norm": 0.10465993732213974,
      "learning_rate": 7.591789860544945e-05,
      "loss": 0.0903,
      "step": 19230
    },
    {
      "epoch": 0.7208422314637893,
      "grad_norm": 0.06277689337730408,
      "learning_rate": 7.589140559241571e-05,
      "loss": 0.0957,
      "step": 19240
    },
    {
      "epoch": 0.7212168895882507,
      "grad_norm": 0.08218152076005936,
      "learning_rate": 7.586490264326369e-05,
      "loss": 0.0905,
      "step": 19250
    },
    {
      "epoch": 0.7215915477127122,
      "grad_norm": 0.08511675149202347,
      "learning_rate": 7.583838976816416e-05,
      "loss": 0.0919,
      "step": 19260
    },
    {
      "epoch": 0.7219662058371735,
      "grad_norm": 0.2823728024959564,
      "learning_rate": 7.581186697729173e-05,
      "loss": 0.0907,
      "step": 19270
    },
    {
      "epoch": 0.722340863961635,
      "grad_norm": 0.09481555968523026,
      "learning_rate": 7.578533428082482e-05,
      "loss": 0.0924,
      "step": 19280
    },
    {
      "epoch": 0.7227155220860965,
      "grad_norm": 0.06684687733650208,
      "learning_rate": 7.575879168894564e-05,
      "loss": 0.0878,
      "step": 19290
    },
    {
      "epoch": 0.7230901802105578,
      "grad_norm": 0.0865212231874466,
      "learning_rate": 7.573223921184022e-05,
      "loss": 0.0921,
      "step": 19300
    },
    {
      "epoch": 0.7234648383350193,
      "grad_norm": 0.08054880052804947,
      "learning_rate": 7.570567685969838e-05,
      "loss": 0.0916,
      "step": 19310
    },
    {
      "epoch": 0.7238394964594808,
      "grad_norm": 0.11533549427986145,
      "learning_rate": 7.567910464271373e-05,
      "loss": 0.0906,
      "step": 19320
    },
    {
      "epoch": 0.7242141545839421,
      "grad_norm": 0.07515716552734375,
      "learning_rate": 7.565252257108363e-05,
      "loss": 0.0941,
      "step": 19330
    },
    {
      "epoch": 0.7245888127084036,
      "grad_norm": 0.13119211792945862,
      "learning_rate": 7.562593065500927e-05,
      "loss": 0.0892,
      "step": 19340
    },
    {
      "epoch": 0.724963470832865,
      "grad_norm": 0.08848565816879272,
      "learning_rate": 7.559932890469559e-05,
      "loss": 0.0915,
      "step": 19350
    },
    {
      "epoch": 0.7253381289573264,
      "grad_norm": 0.07973569631576538,
      "learning_rate": 7.557271733035129e-05,
      "loss": 0.0913,
      "step": 19360
    },
    {
      "epoch": 0.7257127870817879,
      "grad_norm": 0.08058837801218033,
      "learning_rate": 7.55460959421889e-05,
      "loss": 0.0956,
      "step": 19370
    },
    {
      "epoch": 0.7260874452062493,
      "grad_norm": 0.07878609746694565,
      "learning_rate": 7.551946475042465e-05,
      "loss": 0.0915,
      "step": 19380
    },
    {
      "epoch": 0.7264621033307107,
      "grad_norm": 0.07069487869739532,
      "learning_rate": 7.549282376527858e-05,
      "loss": 0.0895,
      "step": 19390
    },
    {
      "epoch": 0.7268367614551722,
      "grad_norm": 0.0828559473156929,
      "learning_rate": 7.546617299697444e-05,
      "loss": 0.0947,
      "step": 19400
    },
    {
      "epoch": 0.7272114195796335,
      "grad_norm": 0.06837161630392075,
      "learning_rate": 7.54395124557398e-05,
      "loss": 0.094,
      "step": 19410
    },
    {
      "epoch": 0.727586077704095,
      "grad_norm": 0.07566257566213608,
      "learning_rate": 7.54128421518059e-05,
      "loss": 0.0905,
      "step": 19420
    },
    {
      "epoch": 0.7279607358285565,
      "grad_norm": 0.08302978426218033,
      "learning_rate": 7.538616209540779e-05,
      "loss": 0.0922,
      "step": 19430
    },
    {
      "epoch": 0.7283353939530178,
      "grad_norm": 0.09003382921218872,
      "learning_rate": 7.535947229678425e-05,
      "loss": 0.0912,
      "step": 19440
    },
    {
      "epoch": 0.7287100520774793,
      "grad_norm": 0.0883028581738472,
      "learning_rate": 7.533277276617778e-05,
      "loss": 0.0941,
      "step": 19450
    },
    {
      "epoch": 0.7290847102019408,
      "grad_norm": 0.07378305494785309,
      "learning_rate": 7.530606351383464e-05,
      "loss": 0.0908,
      "step": 19460
    },
    {
      "epoch": 0.7294593683264021,
      "grad_norm": 0.09234495460987091,
      "learning_rate": 7.527934455000479e-05,
      "loss": 0.0958,
      "step": 19470
    },
    {
      "epoch": 0.7298340264508636,
      "grad_norm": 0.06395429372787476,
      "learning_rate": 7.525261588494193e-05,
      "loss": 0.0927,
      "step": 19480
    },
    {
      "epoch": 0.7302086845753251,
      "grad_norm": 0.08907691389322281,
      "learning_rate": 7.52258775289035e-05,
      "loss": 0.0919,
      "step": 19490
    },
    {
      "epoch": 0.7305833426997864,
      "grad_norm": 0.09780888259410858,
      "learning_rate": 7.519912949215063e-05,
      "loss": 0.09,
      "step": 19500
    },
    {
      "epoch": 0.7309580008242479,
      "grad_norm": 0.1055045872926712,
      "learning_rate": 7.517237178494821e-05,
      "loss": 0.0909,
      "step": 19510
    },
    {
      "epoch": 0.7313326589487094,
      "grad_norm": 0.09860001504421234,
      "learning_rate": 7.514560441756476e-05,
      "loss": 0.0918,
      "step": 19520
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.1031242236495018,
      "learning_rate": 7.511882740027261e-05,
      "loss": 0.0901,
      "step": 19530
    },
    {
      "epoch": 0.7320819751976322,
      "grad_norm": 0.07340331375598907,
      "learning_rate": 7.509204074334772e-05,
      "loss": 0.0884,
      "step": 19540
    },
    {
      "epoch": 0.7324566333220935,
      "grad_norm": 0.0863306075334549,
      "learning_rate": 7.506524445706975e-05,
      "loss": 0.0904,
      "step": 19550
    },
    {
      "epoch": 0.732831291446555,
      "grad_norm": 0.09311990439891815,
      "learning_rate": 7.503843855172213e-05,
      "loss": 0.0965,
      "step": 19560
    },
    {
      "epoch": 0.7332059495710165,
      "grad_norm": 0.07958944886922836,
      "learning_rate": 7.501162303759189e-05,
      "loss": 0.0947,
      "step": 19570
    },
    {
      "epoch": 0.7335806076954778,
      "grad_norm": 0.06508338451385498,
      "learning_rate": 7.498479792496978e-05,
      "loss": 0.0936,
      "step": 19580
    },
    {
      "epoch": 0.7339552658199393,
      "grad_norm": 0.08702550083398819,
      "learning_rate": 7.495796322415026e-05,
      "loss": 0.0903,
      "step": 19590
    },
    {
      "epoch": 0.7343299239444008,
      "grad_norm": 0.057861946523189545,
      "learning_rate": 7.493111894543142e-05,
      "loss": 0.0929,
      "step": 19600
    },
    {
      "epoch": 0.7347045820688621,
      "grad_norm": 0.06910468637943268,
      "learning_rate": 7.490426509911508e-05,
      "loss": 0.0896,
      "step": 19610
    },
    {
      "epoch": 0.7350792401933236,
      "grad_norm": 0.14021070301532745,
      "learning_rate": 7.48774016955067e-05,
      "loss": 0.0917,
      "step": 19620
    },
    {
      "epoch": 0.7354538983177851,
      "grad_norm": 0.08689232915639877,
      "learning_rate": 7.485052874491542e-05,
      "loss": 0.0898,
      "step": 19630
    },
    {
      "epoch": 0.7358285564422464,
      "grad_norm": 0.10964890569448471,
      "learning_rate": 7.482364625765402e-05,
      "loss": 0.091,
      "step": 19640
    },
    {
      "epoch": 0.7362032145667079,
      "grad_norm": 0.06699368357658386,
      "learning_rate": 7.479675424403898e-05,
      "loss": 0.0907,
      "step": 19650
    },
    {
      "epoch": 0.7365778726911693,
      "grad_norm": 0.0777892917394638,
      "learning_rate": 7.476985271439039e-05,
      "loss": 0.097,
      "step": 19660
    },
    {
      "epoch": 0.7369525308156307,
      "grad_norm": 0.08893447369337082,
      "learning_rate": 7.474294167903202e-05,
      "loss": 0.0852,
      "step": 19670
    },
    {
      "epoch": 0.7373271889400922,
      "grad_norm": 0.07443879544734955,
      "learning_rate": 7.471602114829131e-05,
      "loss": 0.0943,
      "step": 19680
    },
    {
      "epoch": 0.7377018470645536,
      "grad_norm": 0.07428549230098724,
      "learning_rate": 7.468909113249927e-05,
      "loss": 0.0933,
      "step": 19690
    },
    {
      "epoch": 0.738076505189015,
      "grad_norm": 0.08302003145217896,
      "learning_rate": 7.466215164199062e-05,
      "loss": 0.0915,
      "step": 19700
    },
    {
      "epoch": 0.7384511633134765,
      "grad_norm": 0.06095844507217407,
      "learning_rate": 7.46352026871037e-05,
      "loss": 0.0913,
      "step": 19710
    },
    {
      "epoch": 0.7388258214379378,
      "grad_norm": 0.08628316223621368,
      "learning_rate": 7.460824427818046e-05,
      "loss": 0.0913,
      "step": 19720
    },
    {
      "epoch": 0.7392004795623993,
      "grad_norm": 0.07503144443035126,
      "learning_rate": 7.458127642556651e-05,
      "loss": 0.093,
      "step": 19730
    },
    {
      "epoch": 0.7395751376868608,
      "grad_norm": 0.08685299009084702,
      "learning_rate": 7.455429913961105e-05,
      "loss": 0.0945,
      "step": 19740
    },
    {
      "epoch": 0.7399497958113221,
      "grad_norm": 0.09944410622119904,
      "learning_rate": 7.452731243066694e-05,
      "loss": 0.0945,
      "step": 19750
    },
    {
      "epoch": 0.7403244539357836,
      "grad_norm": 0.07135646790266037,
      "learning_rate": 7.450031630909061e-05,
      "loss": 0.0934,
      "step": 19760
    },
    {
      "epoch": 0.7406991120602451,
      "grad_norm": 0.08758818358182907,
      "learning_rate": 7.447331078524214e-05,
      "loss": 0.0883,
      "step": 19770
    },
    {
      "epoch": 0.7410737701847064,
      "grad_norm": 0.10539815574884415,
      "learning_rate": 7.444629586948517e-05,
      "loss": 0.0937,
      "step": 19780
    },
    {
      "epoch": 0.7414484283091679,
      "grad_norm": 0.06103668734431267,
      "learning_rate": 7.441927157218703e-05,
      "loss": 0.0935,
      "step": 19790
    },
    {
      "epoch": 0.7418230864336293,
      "grad_norm": 0.08277114480733871,
      "learning_rate": 7.439223790371858e-05,
      "loss": 0.0902,
      "step": 19800
    },
    {
      "epoch": 0.7421977445580907,
      "grad_norm": 0.0780414417386055,
      "learning_rate": 7.436519487445427e-05,
      "loss": 0.0872,
      "step": 19810
    },
    {
      "epoch": 0.7425724026825522,
      "grad_norm": 0.06995760649442673,
      "learning_rate": 7.433814249477219e-05,
      "loss": 0.0904,
      "step": 19820
    },
    {
      "epoch": 0.7429470608070136,
      "grad_norm": 0.08057300001382828,
      "learning_rate": 7.431108077505399e-05,
      "loss": 0.0906,
      "step": 19830
    },
    {
      "epoch": 0.743321718931475,
      "grad_norm": 0.08079800754785538,
      "learning_rate": 7.42840097256849e-05,
      "loss": 0.091,
      "step": 19840
    },
    {
      "epoch": 0.7436963770559365,
      "grad_norm": 0.07934053242206573,
      "learning_rate": 7.425692935705376e-05,
      "loss": 0.0882,
      "step": 19850
    },
    {
      "epoch": 0.7440710351803979,
      "grad_norm": 0.08009999245405197,
      "learning_rate": 7.422983967955296e-05,
      "loss": 0.0885,
      "step": 19860
    },
    {
      "epoch": 0.7444456933048593,
      "grad_norm": 0.0637548491358757,
      "learning_rate": 7.420274070357846e-05,
      "loss": 0.0928,
      "step": 19870
    },
    {
      "epoch": 0.7448203514293208,
      "grad_norm": 0.10771340876817703,
      "learning_rate": 7.417563243952981e-05,
      "loss": 0.0878,
      "step": 19880
    },
    {
      "epoch": 0.7451950095537822,
      "grad_norm": 0.07358645647764206,
      "learning_rate": 7.41485148978101e-05,
      "loss": 0.0904,
      "step": 19890
    },
    {
      "epoch": 0.7455696676782436,
      "grad_norm": 0.07595568895339966,
      "learning_rate": 7.412138808882599e-05,
      "loss": 0.0939,
      "step": 19900
    },
    {
      "epoch": 0.745944325802705,
      "grad_norm": 0.06195395812392235,
      "learning_rate": 7.409425202298768e-05,
      "loss": 0.093,
      "step": 19910
    },
    {
      "epoch": 0.7463189839271664,
      "grad_norm": 0.07389448583126068,
      "learning_rate": 7.4067106710709e-05,
      "loss": 0.089,
      "step": 19920
    },
    {
      "epoch": 0.7466936420516279,
      "grad_norm": 0.09936723858118057,
      "learning_rate": 7.403995216240723e-05,
      "loss": 0.0906,
      "step": 19930
    },
    {
      "epoch": 0.7470683001760893,
      "grad_norm": 0.07707352191209793,
      "learning_rate": 7.401278838850321e-05,
      "loss": 0.0941,
      "step": 19940
    },
    {
      "epoch": 0.7474429583005507,
      "grad_norm": 0.14888988435268402,
      "learning_rate": 7.39856153994214e-05,
      "loss": 0.0929,
      "step": 19950
    },
    {
      "epoch": 0.7478176164250122,
      "grad_norm": 0.05993533879518509,
      "learning_rate": 7.395843320558969e-05,
      "loss": 0.0886,
      "step": 19960
    },
    {
      "epoch": 0.7481922745494736,
      "grad_norm": 0.07572004944086075,
      "learning_rate": 7.393124181743957e-05,
      "loss": 0.0931,
      "step": 19970
    },
    {
      "epoch": 0.748566932673935,
      "grad_norm": 0.07690247148275375,
      "learning_rate": 7.390404124540606e-05,
      "loss": 0.0914,
      "step": 19980
    },
    {
      "epoch": 0.7489415907983965,
      "grad_norm": 0.08364386856555939,
      "learning_rate": 7.387683149992766e-05,
      "loss": 0.0915,
      "step": 19990
    },
    {
      "epoch": 0.7493162489228579,
      "grad_norm": 0.06407427787780762,
      "learning_rate": 7.384961259144645e-05,
      "loss": 0.0945,
      "step": 20000
    },
    {
      "epoch": 0.7496909070473193,
      "grad_norm": 0.07794804871082306,
      "learning_rate": 7.382238453040793e-05,
      "loss": 0.0916,
      "step": 20010
    },
    {
      "epoch": 0.7500655651717808,
      "grad_norm": 0.09662311524152756,
      "learning_rate": 7.379514732726125e-05,
      "loss": 0.0894,
      "step": 20020
    },
    {
      "epoch": 0.7504402232962422,
      "grad_norm": 0.06453961879014969,
      "learning_rate": 7.376790099245893e-05,
      "loss": 0.0903,
      "step": 20030
    },
    {
      "epoch": 0.7508148814207036,
      "grad_norm": 0.08318469673395157,
      "learning_rate": 7.374064553645712e-05,
      "loss": 0.0897,
      "step": 20040
    },
    {
      "epoch": 0.751189539545165,
      "grad_norm": 0.1375633031129837,
      "learning_rate": 7.371338096971535e-05,
      "loss": 0.0895,
      "step": 20050
    },
    {
      "epoch": 0.7515641976696265,
      "grad_norm": 0.07050494849681854,
      "learning_rate": 7.368610730269672e-05,
      "loss": 0.0887,
      "step": 20060
    },
    {
      "epoch": 0.7519388557940879,
      "grad_norm": 0.1076117679476738,
      "learning_rate": 7.365882454586783e-05,
      "loss": 0.0929,
      "step": 20070
    },
    {
      "epoch": 0.7523135139185493,
      "grad_norm": 0.10019554942846298,
      "learning_rate": 7.363153270969871e-05,
      "loss": 0.0928,
      "step": 20080
    },
    {
      "epoch": 0.7526881720430108,
      "grad_norm": 0.08783075213432312,
      "learning_rate": 7.360423180466294e-05,
      "loss": 0.0942,
      "step": 20090
    },
    {
      "epoch": 0.7530628301674722,
      "grad_norm": 0.05692106857895851,
      "learning_rate": 7.357692184123753e-05,
      "loss": 0.0916,
      "step": 20100
    },
    {
      "epoch": 0.7534374882919336,
      "grad_norm": 0.05741089954972267,
      "learning_rate": 7.3549602829903e-05,
      "loss": 0.0897,
      "step": 20110
    },
    {
      "epoch": 0.753812146416395,
      "grad_norm": 0.09454898536205292,
      "learning_rate": 7.352227478114335e-05,
      "loss": 0.0932,
      "step": 20120
    },
    {
      "epoch": 0.7541868045408565,
      "grad_norm": 0.0668368935585022,
      "learning_rate": 7.349493770544595e-05,
      "loss": 0.0921,
      "step": 20130
    },
    {
      "epoch": 0.7545614626653179,
      "grad_norm": 0.07830271124839783,
      "learning_rate": 7.346759161330178e-05,
      "loss": 0.0911,
      "step": 20140
    },
    {
      "epoch": 0.7549361207897793,
      "grad_norm": 0.09488959610462189,
      "learning_rate": 7.34402365152052e-05,
      "loss": 0.0927,
      "step": 20150
    },
    {
      "epoch": 0.7553107789142408,
      "grad_norm": 0.09224332869052887,
      "learning_rate": 7.341287242165401e-05,
      "loss": 0.0925,
      "step": 20160
    },
    {
      "epoch": 0.7556854370387022,
      "grad_norm": 0.08558519184589386,
      "learning_rate": 7.338549934314953e-05,
      "loss": 0.0977,
      "step": 20170
    },
    {
      "epoch": 0.7560600951631636,
      "grad_norm": 0.0656370297074318,
      "learning_rate": 7.335811729019642e-05,
      "loss": 0.0904,
      "step": 20180
    },
    {
      "epoch": 0.756434753287625,
      "grad_norm": 0.06864118576049805,
      "learning_rate": 7.333072627330294e-05,
      "loss": 0.0893,
      "step": 20190
    },
    {
      "epoch": 0.7568094114120865,
      "grad_norm": 0.07947153598070145,
      "learning_rate": 7.33033263029806e-05,
      "loss": 0.0912,
      "step": 20200
    },
    {
      "epoch": 0.7571840695365479,
      "grad_norm": 0.0766872689127922,
      "learning_rate": 7.327591738974454e-05,
      "loss": 0.0892,
      "step": 20210
    },
    {
      "epoch": 0.7575587276610093,
      "grad_norm": 0.0749504417181015,
      "learning_rate": 7.324849954411319e-05,
      "loss": 0.0924,
      "step": 20220
    },
    {
      "epoch": 0.7579333857854708,
      "grad_norm": 0.06866791099309921,
      "learning_rate": 7.322107277660847e-05,
      "loss": 0.0908,
      "step": 20230
    },
    {
      "epoch": 0.7583080439099322,
      "grad_norm": 0.08157777786254883,
      "learning_rate": 7.319363709775572e-05,
      "loss": 0.0933,
      "step": 20240
    },
    {
      "epoch": 0.7586827020343936,
      "grad_norm": 0.0778791680932045,
      "learning_rate": 7.316619251808364e-05,
      "loss": 0.0922,
      "step": 20250
    },
    {
      "epoch": 0.759057360158855,
      "grad_norm": 0.14782650768756866,
      "learning_rate": 7.313873904812447e-05,
      "loss": 0.094,
      "step": 20260
    },
    {
      "epoch": 0.7594320182833165,
      "grad_norm": 0.06639184057712555,
      "learning_rate": 7.311127669841375e-05,
      "loss": 0.0889,
      "step": 20270
    },
    {
      "epoch": 0.7598066764077779,
      "grad_norm": 0.09273198992013931,
      "learning_rate": 7.308380547949046e-05,
      "loss": 0.0896,
      "step": 20280
    },
    {
      "epoch": 0.7601813345322394,
      "grad_norm": 0.07246661931276321,
      "learning_rate": 7.3056325401897e-05,
      "loss": 0.0881,
      "step": 20290
    },
    {
      "epoch": 0.7605559926567007,
      "grad_norm": 0.07462230324745178,
      "learning_rate": 7.302883647617914e-05,
      "loss": 0.0892,
      "step": 20300
    },
    {
      "epoch": 0.7609306507811622,
      "grad_norm": 0.07308671623468399,
      "learning_rate": 7.300133871288611e-05,
      "loss": 0.0872,
      "step": 20310
    },
    {
      "epoch": 0.7613053089056236,
      "grad_norm": 0.06991637498140335,
      "learning_rate": 7.297383212257043e-05,
      "loss": 0.0893,
      "step": 20320
    },
    {
      "epoch": 0.761679967030085,
      "grad_norm": 0.07000578939914703,
      "learning_rate": 7.29463167157881e-05,
      "loss": 0.0892,
      "step": 20330
    },
    {
      "epoch": 0.7620546251545465,
      "grad_norm": 0.10321998596191406,
      "learning_rate": 7.291879250309847e-05,
      "loss": 0.0895,
      "step": 20340
    },
    {
      "epoch": 0.7624292832790079,
      "grad_norm": 0.10167286545038223,
      "learning_rate": 7.289125949506425e-05,
      "loss": 0.091,
      "step": 20350
    },
    {
      "epoch": 0.7628039414034693,
      "grad_norm": 0.08368685841560364,
      "learning_rate": 7.286371770225155e-05,
      "loss": 0.0888,
      "step": 20360
    },
    {
      "epoch": 0.7631785995279308,
      "grad_norm": 0.10717805474996567,
      "learning_rate": 7.283616713522983e-05,
      "loss": 0.0903,
      "step": 20370
    },
    {
      "epoch": 0.7635532576523922,
      "grad_norm": 0.09854093194007874,
      "learning_rate": 7.280860780457194e-05,
      "loss": 0.0902,
      "step": 20380
    },
    {
      "epoch": 0.7639279157768536,
      "grad_norm": 0.07766222953796387,
      "learning_rate": 7.27810397208541e-05,
      "loss": 0.0901,
      "step": 20390
    },
    {
      "epoch": 0.7643025739013151,
      "grad_norm": 0.07309078425168991,
      "learning_rate": 7.275346289465583e-05,
      "loss": 0.088,
      "step": 20400
    },
    {
      "epoch": 0.7646772320257765,
      "grad_norm": 0.06529876589775085,
      "learning_rate": 7.272587733656008e-05,
      "loss": 0.0917,
      "step": 20410
    },
    {
      "epoch": 0.7650518901502379,
      "grad_norm": 0.05745694786310196,
      "learning_rate": 7.269828305715313e-05,
      "loss": 0.0927,
      "step": 20420
    },
    {
      "epoch": 0.7654265482746994,
      "grad_norm": 0.0814453586935997,
      "learning_rate": 7.267068006702457e-05,
      "loss": 0.0922,
      "step": 20430
    },
    {
      "epoch": 0.7658012063991607,
      "grad_norm": 0.08199599385261536,
      "learning_rate": 7.264306837676734e-05,
      "loss": 0.0917,
      "step": 20440
    },
    {
      "epoch": 0.7661758645236222,
      "grad_norm": 0.06236840784549713,
      "learning_rate": 7.26154479969778e-05,
      "loss": 0.0877,
      "step": 20450
    },
    {
      "epoch": 0.7665505226480837,
      "grad_norm": 0.09850043803453445,
      "learning_rate": 7.258781893825551e-05,
      "loss": 0.0899,
      "step": 20460
    },
    {
      "epoch": 0.766925180772545,
      "grad_norm": 0.06812414526939392,
      "learning_rate": 7.25601812112035e-05,
      "loss": 0.0955,
      "step": 20470
    },
    {
      "epoch": 0.7672998388970065,
      "grad_norm": 0.08239863812923431,
      "learning_rate": 7.2532534826428e-05,
      "loss": 0.0933,
      "step": 20480
    },
    {
      "epoch": 0.767674497021468,
      "grad_norm": 0.25792235136032104,
      "learning_rate": 7.250487979453864e-05,
      "loss": 0.0909,
      "step": 20490
    },
    {
      "epoch": 0.7680491551459293,
      "grad_norm": 0.07725075632333755,
      "learning_rate": 7.247721612614837e-05,
      "loss": 0.09,
      "step": 20500
    },
    {
      "epoch": 0.7684238132703908,
      "grad_norm": 0.10177318751811981,
      "learning_rate": 7.244954383187341e-05,
      "loss": 0.0901,
      "step": 20510
    },
    {
      "epoch": 0.7687984713948522,
      "grad_norm": 0.13277408480644226,
      "learning_rate": 7.242186292233332e-05,
      "loss": 0.0892,
      "step": 20520
    },
    {
      "epoch": 0.7691731295193136,
      "grad_norm": 0.0847436934709549,
      "learning_rate": 7.239417340815098e-05,
      "loss": 0.0928,
      "step": 20530
    },
    {
      "epoch": 0.7695477876437751,
      "grad_norm": 0.14173352718353271,
      "learning_rate": 7.236647529995254e-05,
      "loss": 0.0887,
      "step": 20540
    },
    {
      "epoch": 0.7699224457682364,
      "grad_norm": 0.06968716531991959,
      "learning_rate": 7.233876860836745e-05,
      "loss": 0.088,
      "step": 20550
    },
    {
      "epoch": 0.7702971038926979,
      "grad_norm": 0.08312284201383591,
      "learning_rate": 7.231105334402847e-05,
      "loss": 0.0928,
      "step": 20560
    },
    {
      "epoch": 0.7706717620171594,
      "grad_norm": 0.08088024705648422,
      "learning_rate": 7.228332951757163e-05,
      "loss": 0.0864,
      "step": 20570
    },
    {
      "epoch": 0.7710464201416207,
      "grad_norm": 0.09761992841959,
      "learning_rate": 7.225559713963633e-05,
      "loss": 0.0899,
      "step": 20580
    },
    {
      "epoch": 0.7714210782660822,
      "grad_norm": 0.11355628818273544,
      "learning_rate": 7.222785622086508e-05,
      "loss": 0.0906,
      "step": 20590
    },
    {
      "epoch": 0.7717957363905437,
      "grad_norm": 0.07995535433292389,
      "learning_rate": 7.220010677190385e-05,
      "loss": 0.0915,
      "step": 20600
    },
    {
      "epoch": 0.772170394515005,
      "grad_norm": 0.07643836736679077,
      "learning_rate": 7.217234880340176e-05,
      "loss": 0.0949,
      "step": 20610
    },
    {
      "epoch": 0.7725450526394665,
      "grad_norm": 0.08074576407670975,
      "learning_rate": 7.214458232601126e-05,
      "loss": 0.0905,
      "step": 20620
    },
    {
      "epoch": 0.772919710763928,
      "grad_norm": 0.06806594133377075,
      "learning_rate": 7.211680735038804e-05,
      "loss": 0.0949,
      "step": 20630
    },
    {
      "epoch": 0.7732943688883893,
      "grad_norm": 0.13367722928524017,
      "learning_rate": 7.208902388719106e-05,
      "loss": 0.0892,
      "step": 20640
    },
    {
      "epoch": 0.7736690270128508,
      "grad_norm": 0.07907279580831528,
      "learning_rate": 7.206123194708255e-05,
      "loss": 0.0888,
      "step": 20650
    },
    {
      "epoch": 0.7740436851373123,
      "grad_norm": 0.07627899944782257,
      "learning_rate": 7.203343154072796e-05,
      "loss": 0.0921,
      "step": 20660
    },
    {
      "epoch": 0.7744183432617736,
      "grad_norm": 0.08619575202465057,
      "learning_rate": 7.200562267879603e-05,
      "loss": 0.0929,
      "step": 20670
    },
    {
      "epoch": 0.7747930013862351,
      "grad_norm": 0.10035918653011322,
      "learning_rate": 7.197780537195872e-05,
      "loss": 0.0928,
      "step": 20680
    },
    {
      "epoch": 0.7751676595106964,
      "grad_norm": 0.07546637207269669,
      "learning_rate": 7.19499796308912e-05,
      "loss": 0.09,
      "step": 20690
    },
    {
      "epoch": 0.7755423176351579,
      "grad_norm": 0.08406699448823929,
      "learning_rate": 7.192214546627198e-05,
      "loss": 0.0932,
      "step": 20700
    },
    {
      "epoch": 0.7759169757596194,
      "grad_norm": 0.084261454641819,
      "learning_rate": 7.189430288878266e-05,
      "loss": 0.0919,
      "step": 20710
    },
    {
      "epoch": 0.7762916338840807,
      "grad_norm": 0.08481290191411972,
      "learning_rate": 7.186645190910819e-05,
      "loss": 0.0919,
      "step": 20720
    },
    {
      "epoch": 0.7766662920085422,
      "grad_norm": 0.09591469168663025,
      "learning_rate": 7.183859253793667e-05,
      "loss": 0.0912,
      "step": 20730
    },
    {
      "epoch": 0.7770409501330037,
      "grad_norm": 0.09152097254991531,
      "learning_rate": 7.181072478595947e-05,
      "loss": 0.0969,
      "step": 20740
    },
    {
      "epoch": 0.777415608257465,
      "grad_norm": 0.08632723242044449,
      "learning_rate": 7.178284866387115e-05,
      "loss": 0.0921,
      "step": 20750
    },
    {
      "epoch": 0.7777902663819265,
      "grad_norm": 0.15933315455913544,
      "learning_rate": 7.175496418236946e-05,
      "loss": 0.0897,
      "step": 20760
    },
    {
      "epoch": 0.778164924506388,
      "grad_norm": 0.07710634917020798,
      "learning_rate": 7.172707135215541e-05,
      "loss": 0.0929,
      "step": 20770
    },
    {
      "epoch": 0.7785395826308493,
      "grad_norm": 0.08478546142578125,
      "learning_rate": 7.169917018393318e-05,
      "loss": 0.089,
      "step": 20780
    },
    {
      "epoch": 0.7789142407553108,
      "grad_norm": 0.061604179441928864,
      "learning_rate": 7.167126068841017e-05,
      "loss": 0.0917,
      "step": 20790
    },
    {
      "epoch": 0.7792888988797723,
      "grad_norm": 0.05908080190420151,
      "learning_rate": 7.164334287629694e-05,
      "loss": 0.0908,
      "step": 20800
    },
    {
      "epoch": 0.7796635570042336,
      "grad_norm": 0.12834739685058594,
      "learning_rate": 7.161541675830728e-05,
      "loss": 0.0907,
      "step": 20810
    },
    {
      "epoch": 0.7800382151286951,
      "grad_norm": 0.09522559493780136,
      "learning_rate": 7.158748234515814e-05,
      "loss": 0.0916,
      "step": 20820
    },
    {
      "epoch": 0.7804128732531564,
      "grad_norm": 0.08484911173582077,
      "learning_rate": 7.155953964756966e-05,
      "loss": 0.0894,
      "step": 20830
    },
    {
      "epoch": 0.7807875313776179,
      "grad_norm": 0.10229932516813278,
      "learning_rate": 7.153158867626521e-05,
      "loss": 0.094,
      "step": 20840
    },
    {
      "epoch": 0.7811621895020794,
      "grad_norm": 0.07163921743631363,
      "learning_rate": 7.150362944197124e-05,
      "loss": 0.0927,
      "step": 20850
    },
    {
      "epoch": 0.7815368476265407,
      "grad_norm": 0.10392168909311295,
      "learning_rate": 7.147566195541743e-05,
      "loss": 0.0881,
      "step": 20860
    },
    {
      "epoch": 0.7819115057510022,
      "grad_norm": 0.07272560149431229,
      "learning_rate": 7.144768622733663e-05,
      "loss": 0.0971,
      "step": 20870
    },
    {
      "epoch": 0.7822861638754637,
      "grad_norm": 0.07200714200735092,
      "learning_rate": 7.141970226846485e-05,
      "loss": 0.0901,
      "step": 20880
    },
    {
      "epoch": 0.782660821999925,
      "grad_norm": 0.0817316547036171,
      "learning_rate": 7.139171008954123e-05,
      "loss": 0.0912,
      "step": 20890
    },
    {
      "epoch": 0.7830354801243865,
      "grad_norm": 0.08274246007204056,
      "learning_rate": 7.136370970130806e-05,
      "loss": 0.0921,
      "step": 20900
    },
    {
      "epoch": 0.783410138248848,
      "grad_norm": 0.06122176721692085,
      "learning_rate": 7.133570111451087e-05,
      "loss": 0.0922,
      "step": 20910
    },
    {
      "epoch": 0.7837847963733093,
      "grad_norm": 0.0789487436413765,
      "learning_rate": 7.130768433989821e-05,
      "loss": 0.0909,
      "step": 20920
    },
    {
      "epoch": 0.7841594544977708,
      "grad_norm": 0.08312708139419556,
      "learning_rate": 7.127965938822186e-05,
      "loss": 0.0911,
      "step": 20930
    },
    {
      "epoch": 0.7845341126222322,
      "grad_norm": 0.09932591021060944,
      "learning_rate": 7.125162627023673e-05,
      "loss": 0.0909,
      "step": 20940
    },
    {
      "epoch": 0.7849087707466936,
      "grad_norm": 0.10001455247402191,
      "learning_rate": 7.122358499670079e-05,
      "loss": 0.0902,
      "step": 20950
    },
    {
      "epoch": 0.7852834288711551,
      "grad_norm": 0.0823216363787651,
      "learning_rate": 7.119553557837525e-05,
      "loss": 0.0921,
      "step": 20960
    },
    {
      "epoch": 0.7856580869956165,
      "grad_norm": 0.11203616112470627,
      "learning_rate": 7.116747802602435e-05,
      "loss": 0.0915,
      "step": 20970
    },
    {
      "epoch": 0.7860327451200779,
      "grad_norm": 0.06707880645990372,
      "learning_rate": 7.113941235041553e-05,
      "loss": 0.0898,
      "step": 20980
    },
    {
      "epoch": 0.7864074032445394,
      "grad_norm": 0.08528637140989304,
      "learning_rate": 7.111133856231926e-05,
      "loss": 0.0973,
      "step": 20990
    },
    {
      "epoch": 0.7867820613690008,
      "grad_norm": 0.0772915929555893,
      "learning_rate": 7.10832566725092e-05,
      "loss": 0.0948,
      "step": 21000
    },
    {
      "epoch": 0.7871567194934622,
      "grad_norm": 0.07097893953323364,
      "learning_rate": 7.105516669176211e-05,
      "loss": 0.0926,
      "step": 21010
    },
    {
      "epoch": 0.7875313776179237,
      "grad_norm": 0.07246841490268707,
      "learning_rate": 7.10270686308578e-05,
      "loss": 0.0913,
      "step": 21020
    },
    {
      "epoch": 0.787906035742385,
      "grad_norm": 0.06781567633152008,
      "learning_rate": 7.099896250057924e-05,
      "loss": 0.0933,
      "step": 21030
    },
    {
      "epoch": 0.7882806938668465,
      "grad_norm": 0.07376068830490112,
      "learning_rate": 7.097084831171245e-05,
      "loss": 0.092,
      "step": 21040
    },
    {
      "epoch": 0.788655351991308,
      "grad_norm": 0.08887004107236862,
      "learning_rate": 7.09427260750466e-05,
      "loss": 0.0896,
      "step": 21050
    },
    {
      "epoch": 0.7890300101157693,
      "grad_norm": 0.07990957051515579,
      "learning_rate": 7.09145958013739e-05,
      "loss": 0.0929,
      "step": 21060
    },
    {
      "epoch": 0.7894046682402308,
      "grad_norm": 0.0809868797659874,
      "learning_rate": 7.088645750148965e-05,
      "loss": 0.0927,
      "step": 21070
    },
    {
      "epoch": 0.7897793263646922,
      "grad_norm": 0.07667560130357742,
      "learning_rate": 7.085831118619226e-05,
      "loss": 0.0934,
      "step": 21080
    },
    {
      "epoch": 0.7901539844891536,
      "grad_norm": 0.089698925614357,
      "learning_rate": 7.083015686628316e-05,
      "loss": 0.094,
      "step": 21090
    },
    {
      "epoch": 0.7905286426136151,
      "grad_norm": 0.062224749475717545,
      "learning_rate": 7.080199455256693e-05,
      "loss": 0.0915,
      "step": 21100
    },
    {
      "epoch": 0.7909033007380765,
      "grad_norm": 0.06614404916763306,
      "learning_rate": 7.077382425585115e-05,
      "loss": 0.0923,
      "step": 21110
    },
    {
      "epoch": 0.7912779588625379,
      "grad_norm": 0.09600694477558136,
      "learning_rate": 7.074564598694649e-05,
      "loss": 0.094,
      "step": 21120
    },
    {
      "epoch": 0.7916526169869994,
      "grad_norm": 0.08106165379285812,
      "learning_rate": 7.071745975666666e-05,
      "loss": 0.092,
      "step": 21130
    },
    {
      "epoch": 0.7920272751114608,
      "grad_norm": 0.08064374327659607,
      "learning_rate": 7.068926557582849e-05,
      "loss": 0.0944,
      "step": 21140
    },
    {
      "epoch": 0.7924019332359222,
      "grad_norm": 0.06595557183027267,
      "learning_rate": 7.066106345525179e-05,
      "loss": 0.0887,
      "step": 21150
    },
    {
      "epoch": 0.7927765913603837,
      "grad_norm": 0.0640721544623375,
      "learning_rate": 7.063285340575943e-05,
      "loss": 0.0914,
      "step": 21160
    },
    {
      "epoch": 0.7931512494848451,
      "grad_norm": 0.07568007707595825,
      "learning_rate": 7.060463543817733e-05,
      "loss": 0.0904,
      "step": 21170
    },
    {
      "epoch": 0.7935259076093065,
      "grad_norm": 0.07448912411928177,
      "learning_rate": 7.057640956333447e-05,
      "loss": 0.0904,
      "step": 21180
    },
    {
      "epoch": 0.7939005657337679,
      "grad_norm": 0.1309007704257965,
      "learning_rate": 7.054817579206285e-05,
      "loss": 0.0928,
      "step": 21190
    },
    {
      "epoch": 0.7942752238582294,
      "grad_norm": 0.06590208411216736,
      "learning_rate": 7.051993413519748e-05,
      "loss": 0.0935,
      "step": 21200
    },
    {
      "epoch": 0.7946498819826908,
      "grad_norm": 0.08498929440975189,
      "learning_rate": 7.049168460357644e-05,
      "loss": 0.0916,
      "step": 21210
    },
    {
      "epoch": 0.7950245401071522,
      "grad_norm": 0.06218115985393524,
      "learning_rate": 7.046342720804077e-05,
      "loss": 0.0905,
      "step": 21220
    },
    {
      "epoch": 0.7953991982316136,
      "grad_norm": 0.11189509928226471,
      "learning_rate": 7.043516195943462e-05,
      "loss": 0.0945,
      "step": 21230
    },
    {
      "epoch": 0.7957738563560751,
      "grad_norm": 0.098488949239254,
      "learning_rate": 7.040688886860503e-05,
      "loss": 0.0893,
      "step": 21240
    },
    {
      "epoch": 0.7961485144805365,
      "grad_norm": 0.08795275539159775,
      "learning_rate": 7.037860794640215e-05,
      "loss": 0.0867,
      "step": 21250
    },
    {
      "epoch": 0.7965231726049979,
      "grad_norm": 0.21040137112140656,
      "learning_rate": 7.035031920367909e-05,
      "loss": 0.0925,
      "step": 21260
    },
    {
      "epoch": 0.7968978307294594,
      "grad_norm": 0.09807908535003662,
      "learning_rate": 7.0322022651292e-05,
      "loss": 0.0892,
      "step": 21270
    },
    {
      "epoch": 0.7972724888539208,
      "grad_norm": 0.07060177624225616,
      "learning_rate": 7.029371830009998e-05,
      "loss": 0.0901,
      "step": 21280
    },
    {
      "epoch": 0.7976471469783822,
      "grad_norm": 0.09803438186645508,
      "learning_rate": 7.026540616096514e-05,
      "loss": 0.0904,
      "step": 21290
    },
    {
      "epoch": 0.7980218051028437,
      "grad_norm": 0.08741461485624313,
      "learning_rate": 7.02370862447526e-05,
      "loss": 0.0902,
      "step": 21300
    },
    {
      "epoch": 0.7983964632273051,
      "grad_norm": 0.06707589328289032,
      "learning_rate": 7.020875856233044e-05,
      "loss": 0.0923,
      "step": 21310
    },
    {
      "epoch": 0.7987711213517665,
      "grad_norm": 0.08934725821018219,
      "learning_rate": 7.01804231245697e-05,
      "loss": 0.0922,
      "step": 21320
    },
    {
      "epoch": 0.7991457794762279,
      "grad_norm": 0.07462835311889648,
      "learning_rate": 7.015207994234447e-05,
      "loss": 0.0905,
      "step": 21330
    },
    {
      "epoch": 0.7995204376006894,
      "grad_norm": 0.0728408545255661,
      "learning_rate": 7.012372902653172e-05,
      "loss": 0.0923,
      "step": 21340
    },
    {
      "epoch": 0.7998950957251508,
      "grad_norm": 0.08778762817382812,
      "learning_rate": 7.009537038801147e-05,
      "loss": 0.0948,
      "step": 21350
    },
    {
      "epoch": 0.8002697538496122,
      "grad_norm": 0.08439876139163971,
      "learning_rate": 7.006700403766664e-05,
      "loss": 0.0912,
      "step": 21360
    },
    {
      "epoch": 0.8006444119740737,
      "grad_norm": 0.0674394965171814,
      "learning_rate": 7.003862998638315e-05,
      "loss": 0.0961,
      "step": 21370
    },
    {
      "epoch": 0.8010190700985351,
      "grad_norm": 0.08384725451469421,
      "learning_rate": 7.001024824504986e-05,
      "loss": 0.0903,
      "step": 21380
    },
    {
      "epoch": 0.8013937282229965,
      "grad_norm": 0.07854747027158737,
      "learning_rate": 6.998185882455858e-05,
      "loss": 0.0903,
      "step": 21390
    },
    {
      "epoch": 0.801768386347458,
      "grad_norm": 0.10381359606981277,
      "learning_rate": 6.995346173580408e-05,
      "loss": 0.0918,
      "step": 21400
    },
    {
      "epoch": 0.8021430444719194,
      "grad_norm": 0.07897297292947769,
      "learning_rate": 6.992505698968403e-05,
      "loss": 0.0931,
      "step": 21410
    },
    {
      "epoch": 0.8025177025963808,
      "grad_norm": 0.06651302427053452,
      "learning_rate": 6.989664459709911e-05,
      "loss": 0.0934,
      "step": 21420
    },
    {
      "epoch": 0.8028923607208422,
      "grad_norm": 0.06212130934000015,
      "learning_rate": 6.986822456895287e-05,
      "loss": 0.0872,
      "step": 21430
    },
    {
      "epoch": 0.8032670188453037,
      "grad_norm": 0.05608522519469261,
      "learning_rate": 6.983979691615184e-05,
      "loss": 0.0911,
      "step": 21440
    },
    {
      "epoch": 0.8036416769697651,
      "grad_norm": 0.10118521004915237,
      "learning_rate": 6.981136164960542e-05,
      "loss": 0.0919,
      "step": 21450
    },
    {
      "epoch": 0.8040163350942265,
      "grad_norm": 0.06854983419179916,
      "learning_rate": 6.978291878022599e-05,
      "loss": 0.0921,
      "step": 21460
    },
    {
      "epoch": 0.8043909932186879,
      "grad_norm": 0.0830930694937706,
      "learning_rate": 6.975446831892883e-05,
      "loss": 0.0909,
      "step": 21470
    },
    {
      "epoch": 0.8047656513431494,
      "grad_norm": 0.07885522395372391,
      "learning_rate": 6.972601027663206e-05,
      "loss": 0.0907,
      "step": 21480
    },
    {
      "epoch": 0.8051403094676108,
      "grad_norm": 0.09454511851072311,
      "learning_rate": 6.969754466425686e-05,
      "loss": 0.0914,
      "step": 21490
    },
    {
      "epoch": 0.8055149675920722,
      "grad_norm": 0.07078757882118225,
      "learning_rate": 6.966907149272718e-05,
      "loss": 0.0904,
      "step": 21500
    },
    {
      "epoch": 0.8058896257165337,
      "grad_norm": 0.0768183246254921,
      "learning_rate": 6.964059077296992e-05,
      "loss": 0.0918,
      "step": 21510
    },
    {
      "epoch": 0.8062642838409951,
      "grad_norm": 0.07897443324327469,
      "learning_rate": 6.96121025159149e-05,
      "loss": 0.0939,
      "step": 21520
    },
    {
      "epoch": 0.8066389419654565,
      "grad_norm": 0.09206687659025192,
      "learning_rate": 6.958360673249479e-05,
      "loss": 0.0906,
      "step": 21530
    },
    {
      "epoch": 0.807013600089918,
      "grad_norm": 0.0899527445435524,
      "learning_rate": 6.955510343364517e-05,
      "loss": 0.0905,
      "step": 21540
    },
    {
      "epoch": 0.8073882582143794,
      "grad_norm": 0.07954670488834381,
      "learning_rate": 6.95265926303045e-05,
      "loss": 0.0894,
      "step": 21550
    },
    {
      "epoch": 0.8077629163388408,
      "grad_norm": 0.0807860866189003,
      "learning_rate": 6.949807433341416e-05,
      "loss": 0.0941,
      "step": 21560
    },
    {
      "epoch": 0.8081375744633023,
      "grad_norm": 0.07517953962087631,
      "learning_rate": 6.94695485539183e-05,
      "loss": 0.0922,
      "step": 21570
    },
    {
      "epoch": 0.8085122325877636,
      "grad_norm": 0.08368255198001862,
      "learning_rate": 6.944101530276405e-05,
      "loss": 0.0918,
      "step": 21580
    },
    {
      "epoch": 0.8088868907122251,
      "grad_norm": 0.07088499516248703,
      "learning_rate": 6.941247459090136e-05,
      "loss": 0.088,
      "step": 21590
    },
    {
      "epoch": 0.8092615488366866,
      "grad_norm": 0.12216727435588837,
      "learning_rate": 6.938392642928306e-05,
      "loss": 0.0877,
      "step": 21600
    },
    {
      "epoch": 0.8096362069611479,
      "grad_norm": 0.09365583956241608,
      "learning_rate": 6.935537082886481e-05,
      "loss": 0.0901,
      "step": 21610
    },
    {
      "epoch": 0.8100108650856094,
      "grad_norm": 0.06708602607250214,
      "learning_rate": 6.932680780060514e-05,
      "loss": 0.0923,
      "step": 21620
    },
    {
      "epoch": 0.8103855232100708,
      "grad_norm": 0.08161749690771103,
      "learning_rate": 6.929823735546545e-05,
      "loss": 0.091,
      "step": 21630
    },
    {
      "epoch": 0.8107601813345322,
      "grad_norm": 0.07528103142976761,
      "learning_rate": 6.926965950440998e-05,
      "loss": 0.0881,
      "step": 21640
    },
    {
      "epoch": 0.8111348394589937,
      "grad_norm": 0.07987048476934433,
      "learning_rate": 6.924107425840576e-05,
      "loss": 0.0903,
      "step": 21650
    },
    {
      "epoch": 0.8115094975834551,
      "grad_norm": 0.06294495612382889,
      "learning_rate": 6.921248162842276e-05,
      "loss": 0.0965,
      "step": 21660
    },
    {
      "epoch": 0.8118841557079165,
      "grad_norm": 0.06160343810915947,
      "learning_rate": 6.918388162543366e-05,
      "loss": 0.0941,
      "step": 21670
    },
    {
      "epoch": 0.812258813832378,
      "grad_norm": 0.07571294903755188,
      "learning_rate": 6.915527426041407e-05,
      "loss": 0.0942,
      "step": 21680
    },
    {
      "epoch": 0.8126334719568394,
      "grad_norm": 0.0672832503914833,
      "learning_rate": 6.912665954434237e-05,
      "loss": 0.0916,
      "step": 21690
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 0.13614694774150848,
      "learning_rate": 6.909803748819981e-05,
      "loss": 0.0887,
      "step": 21700
    },
    {
      "epoch": 0.8133827882057623,
      "grad_norm": 0.1355019211769104,
      "learning_rate": 6.90694081029704e-05,
      "loss": 0.0903,
      "step": 21710
    },
    {
      "epoch": 0.8137574463302236,
      "grad_norm": 0.09514465928077698,
      "learning_rate": 6.904077139964099e-05,
      "loss": 0.093,
      "step": 21720
    },
    {
      "epoch": 0.8141321044546851,
      "grad_norm": 0.0917874127626419,
      "learning_rate": 6.901212738920125e-05,
      "loss": 0.0935,
      "step": 21730
    },
    {
      "epoch": 0.8145067625791466,
      "grad_norm": 0.06879323720932007,
      "learning_rate": 6.898347608264363e-05,
      "loss": 0.0902,
      "step": 21740
    },
    {
      "epoch": 0.8148814207036079,
      "grad_norm": 0.09903791546821594,
      "learning_rate": 6.89548174909634e-05,
      "loss": 0.0912,
      "step": 21750
    },
    {
      "epoch": 0.8152560788280694,
      "grad_norm": 0.07234148681163788,
      "learning_rate": 6.892615162515859e-05,
      "loss": 0.0896,
      "step": 21760
    },
    {
      "epoch": 0.8156307369525309,
      "grad_norm": 0.1386818289756775,
      "learning_rate": 6.88974784962301e-05,
      "loss": 0.0895,
      "step": 21770
    },
    {
      "epoch": 0.8160053950769922,
      "grad_norm": 0.07860395312309265,
      "learning_rate": 6.886879811518152e-05,
      "loss": 0.0897,
      "step": 21780
    },
    {
      "epoch": 0.8163800532014537,
      "grad_norm": 0.0593227818608284,
      "learning_rate": 6.884011049301929e-05,
      "loss": 0.0911,
      "step": 21790
    },
    {
      "epoch": 0.8167547113259152,
      "grad_norm": 0.09693314135074615,
      "learning_rate": 6.881141564075261e-05,
      "loss": 0.0923,
      "step": 21800
    },
    {
      "epoch": 0.8171293694503765,
      "grad_norm": 0.06989426165819168,
      "learning_rate": 6.878271356939343e-05,
      "loss": 0.0925,
      "step": 21810
    },
    {
      "epoch": 0.817504027574838,
      "grad_norm": 0.06324973702430725,
      "learning_rate": 6.875400428995652e-05,
      "loss": 0.0898,
      "step": 21820
    },
    {
      "epoch": 0.8178786856992993,
      "grad_norm": 0.06501514464616776,
      "learning_rate": 6.872528781345939e-05,
      "loss": 0.0913,
      "step": 21830
    },
    {
      "epoch": 0.8182533438237608,
      "grad_norm": 0.11967466026544571,
      "learning_rate": 6.869656415092227e-05,
      "loss": 0.0862,
      "step": 21840
    },
    {
      "epoch": 0.8186280019482223,
      "grad_norm": 0.06633415818214417,
      "learning_rate": 6.866783331336823e-05,
      "loss": 0.0899,
      "step": 21850
    },
    {
      "epoch": 0.8190026600726836,
      "grad_norm": 0.13939493894577026,
      "learning_rate": 6.863909531182307e-05,
      "loss": 0.0904,
      "step": 21860
    },
    {
      "epoch": 0.8193773181971451,
      "grad_norm": 0.12896709144115448,
      "learning_rate": 6.861035015731526e-05,
      "loss": 0.0918,
      "step": 21870
    },
    {
      "epoch": 0.8197519763216066,
      "grad_norm": 0.06306254863739014,
      "learning_rate": 6.858159786087611e-05,
      "loss": 0.0893,
      "step": 21880
    },
    {
      "epoch": 0.8201266344460679,
      "grad_norm": 0.07470034062862396,
      "learning_rate": 6.855283843353964e-05,
      "loss": 0.0862,
      "step": 21890
    },
    {
      "epoch": 0.8205012925705294,
      "grad_norm": 0.10154815018177032,
      "learning_rate": 6.85240718863426e-05,
      "loss": 0.0916,
      "step": 21900
    },
    {
      "epoch": 0.8208759506949909,
      "grad_norm": 0.08065658062696457,
      "learning_rate": 6.849529823032448e-05,
      "loss": 0.0892,
      "step": 21910
    },
    {
      "epoch": 0.8212506088194522,
      "grad_norm": 0.0953092947602272,
      "learning_rate": 6.84665174765275e-05,
      "loss": 0.0947,
      "step": 21920
    },
    {
      "epoch": 0.8216252669439137,
      "grad_norm": 0.06763654947280884,
      "learning_rate": 6.843772963599658e-05,
      "loss": 0.0861,
      "step": 21930
    },
    {
      "epoch": 0.8219999250683752,
      "grad_norm": 0.09006186574697495,
      "learning_rate": 6.840893471977939e-05,
      "loss": 0.0907,
      "step": 21940
    },
    {
      "epoch": 0.8223745831928365,
      "grad_norm": 0.07760478556156158,
      "learning_rate": 6.838013273892629e-05,
      "loss": 0.0857,
      "step": 21950
    },
    {
      "epoch": 0.822749241317298,
      "grad_norm": 0.09218832850456238,
      "learning_rate": 6.835132370449037e-05,
      "loss": 0.0884,
      "step": 21960
    },
    {
      "epoch": 0.8231238994417593,
      "grad_norm": 0.08446613699197769,
      "learning_rate": 6.832250762752744e-05,
      "loss": 0.0895,
      "step": 21970
    },
    {
      "epoch": 0.8234985575662208,
      "grad_norm": 0.08273495733737946,
      "learning_rate": 6.829368451909598e-05,
      "loss": 0.0903,
      "step": 21980
    },
    {
      "epoch": 0.8238732156906823,
      "grad_norm": 0.07943511009216309,
      "learning_rate": 6.826485439025716e-05,
      "loss": 0.093,
      "step": 21990
    },
    {
      "epoch": 0.8242478738151436,
      "grad_norm": 0.07223434001207352,
      "learning_rate": 6.823601725207492e-05,
      "loss": 0.0879,
      "step": 22000
    },
    {
      "epoch": 0.8246225319396051,
      "grad_norm": 0.07639728486537933,
      "learning_rate": 6.820717311561577e-05,
      "loss": 0.0902,
      "step": 22010
    },
    {
      "epoch": 0.8249971900640666,
      "grad_norm": 0.08174213021993637,
      "learning_rate": 6.817832199194904e-05,
      "loss": 0.0904,
      "step": 22020
    },
    {
      "epoch": 0.8253718481885279,
      "grad_norm": 0.08177726715803146,
      "learning_rate": 6.814946389214662e-05,
      "loss": 0.089,
      "step": 22030
    },
    {
      "epoch": 0.8257465063129894,
      "grad_norm": 0.06845899671316147,
      "learning_rate": 6.812059882728316e-05,
      "loss": 0.0908,
      "step": 22040
    },
    {
      "epoch": 0.8261211644374509,
      "grad_norm": 0.07769034057855606,
      "learning_rate": 6.809172680843594e-05,
      "loss": 0.0927,
      "step": 22050
    },
    {
      "epoch": 0.8264958225619122,
      "grad_norm": 0.07945239543914795,
      "learning_rate": 6.806284784668491e-05,
      "loss": 0.0877,
      "step": 22060
    },
    {
      "epoch": 0.8268704806863737,
      "grad_norm": 0.06850060075521469,
      "learning_rate": 6.803396195311274e-05,
      "loss": 0.0924,
      "step": 22070
    },
    {
      "epoch": 0.8272451388108352,
      "grad_norm": 0.09442773461341858,
      "learning_rate": 6.800506913880468e-05,
      "loss": 0.0935,
      "step": 22080
    },
    {
      "epoch": 0.8276197969352965,
      "grad_norm": 0.06643017381429672,
      "learning_rate": 6.797616941484867e-05,
      "loss": 0.0917,
      "step": 22090
    },
    {
      "epoch": 0.827994455059758,
      "grad_norm": 0.05736301839351654,
      "learning_rate": 6.794726279233534e-05,
      "loss": 0.0897,
      "step": 22100
    },
    {
      "epoch": 0.8283691131842194,
      "grad_norm": 0.058332014828920364,
      "learning_rate": 6.791834928235787e-05,
      "loss": 0.0898,
      "step": 22110
    },
    {
      "epoch": 0.8287437713086808,
      "grad_norm": 0.07820878177881241,
      "learning_rate": 6.788942889601223e-05,
      "loss": 0.0943,
      "step": 22120
    },
    {
      "epoch": 0.8291184294331423,
      "grad_norm": 0.086579829454422,
      "learning_rate": 6.786050164439686e-05,
      "loss": 0.0908,
      "step": 22130
    },
    {
      "epoch": 0.8294930875576036,
      "grad_norm": 0.08256789296865463,
      "learning_rate": 6.783156753861297e-05,
      "loss": 0.095,
      "step": 22140
    },
    {
      "epoch": 0.8298677456820651,
      "grad_norm": 0.13061873614788055,
      "learning_rate": 6.780262658976434e-05,
      "loss": 0.092,
      "step": 22150
    },
    {
      "epoch": 0.8302424038065266,
      "grad_norm": 0.07068584114313126,
      "learning_rate": 6.777367880895737e-05,
      "loss": 0.0917,
      "step": 22160
    },
    {
      "epoch": 0.8306170619309879,
      "grad_norm": 0.086100734770298,
      "learning_rate": 6.774472420730111e-05,
      "loss": 0.0922,
      "step": 22170
    },
    {
      "epoch": 0.8309917200554494,
      "grad_norm": 0.10115056484937668,
      "learning_rate": 6.77157627959072e-05,
      "loss": 0.0933,
      "step": 22180
    },
    {
      "epoch": 0.8313663781799109,
      "grad_norm": 0.06999876350164413,
      "learning_rate": 6.768679458588994e-05,
      "loss": 0.0904,
      "step": 22190
    },
    {
      "epoch": 0.8317410363043722,
      "grad_norm": 0.18037694692611694,
      "learning_rate": 6.765781958836616e-05,
      "loss": 0.0919,
      "step": 22200
    },
    {
      "epoch": 0.8321156944288337,
      "grad_norm": 0.07138287276029587,
      "learning_rate": 6.76288378144554e-05,
      "loss": 0.09,
      "step": 22210
    },
    {
      "epoch": 0.8324903525532951,
      "grad_norm": 0.07801002264022827,
      "learning_rate": 6.759984927527967e-05,
      "loss": 0.0888,
      "step": 22220
    },
    {
      "epoch": 0.8328650106777565,
      "grad_norm": 0.09075973927974701,
      "learning_rate": 6.75708539819637e-05,
      "loss": 0.091,
      "step": 22230
    },
    {
      "epoch": 0.833239668802218,
      "grad_norm": 0.07298250496387482,
      "learning_rate": 6.754185194563475e-05,
      "loss": 0.0955,
      "step": 22240
    },
    {
      "epoch": 0.8336143269266794,
      "grad_norm": 0.1056855320930481,
      "learning_rate": 6.751284317742266e-05,
      "loss": 0.0926,
      "step": 22250
    },
    {
      "epoch": 0.8339889850511408,
      "grad_norm": 0.07642974704504013,
      "learning_rate": 6.748382768845992e-05,
      "loss": 0.0899,
      "step": 22260
    },
    {
      "epoch": 0.8343636431756023,
      "grad_norm": 0.07167141884565353,
      "learning_rate": 6.745480548988148e-05,
      "loss": 0.0927,
      "step": 22270
    },
    {
      "epoch": 0.8347383013000637,
      "grad_norm": 0.07586034387350082,
      "learning_rate": 6.742577659282497e-05,
      "loss": 0.0909,
      "step": 22280
    },
    {
      "epoch": 0.8351129594245251,
      "grad_norm": 0.08726714551448822,
      "learning_rate": 6.739674100843056e-05,
      "loss": 0.0928,
      "step": 22290
    },
    {
      "epoch": 0.8354876175489866,
      "grad_norm": 0.06032827869057655,
      "learning_rate": 6.7367698747841e-05,
      "loss": 0.0901,
      "step": 22300
    },
    {
      "epoch": 0.835862275673448,
      "grad_norm": 0.06547341495752335,
      "learning_rate": 6.733864982220157e-05,
      "loss": 0.0903,
      "step": 22310
    },
    {
      "epoch": 0.8362369337979094,
      "grad_norm": 0.07315035909414291,
      "learning_rate": 6.730959424266007e-05,
      "loss": 0.0915,
      "step": 22320
    },
    {
      "epoch": 0.8366115919223709,
      "grad_norm": 0.08927521109580994,
      "learning_rate": 6.728053202036699e-05,
      "loss": 0.0882,
      "step": 22330
    },
    {
      "epoch": 0.8369862500468322,
      "grad_norm": 0.06990256160497665,
      "learning_rate": 6.725146316647524e-05,
      "loss": 0.0936,
      "step": 22340
    },
    {
      "epoch": 0.8373609081712937,
      "grad_norm": 0.08307313174009323,
      "learning_rate": 6.722238769214031e-05,
      "loss": 0.0901,
      "step": 22350
    },
    {
      "epoch": 0.8377355662957551,
      "grad_norm": 0.11812963336706161,
      "learning_rate": 6.719330560852028e-05,
      "loss": 0.0928,
      "step": 22360
    },
    {
      "epoch": 0.8381102244202165,
      "grad_norm": 0.08254256844520569,
      "learning_rate": 6.716421692677568e-05,
      "loss": 0.0906,
      "step": 22370
    },
    {
      "epoch": 0.838484882544678,
      "grad_norm": 0.06261740624904633,
      "learning_rate": 6.713512165806967e-05,
      "loss": 0.0904,
      "step": 22380
    },
    {
      "epoch": 0.8388595406691394,
      "grad_norm": 0.08060703426599503,
      "learning_rate": 6.710601981356785e-05,
      "loss": 0.0965,
      "step": 22390
    },
    {
      "epoch": 0.8392341987936008,
      "grad_norm": 0.07408925145864487,
      "learning_rate": 6.707691140443838e-05,
      "loss": 0.0937,
      "step": 22400
    },
    {
      "epoch": 0.8396088569180623,
      "grad_norm": 0.07803215831518173,
      "learning_rate": 6.704779644185195e-05,
      "loss": 0.0908,
      "step": 22410
    },
    {
      "epoch": 0.8399835150425237,
      "grad_norm": 0.06549310684204102,
      "learning_rate": 6.701867493698175e-05,
      "loss": 0.0894,
      "step": 22420
    },
    {
      "epoch": 0.8403581731669851,
      "grad_norm": 0.09124711155891418,
      "learning_rate": 6.69895469010035e-05,
      "loss": 0.0943,
      "step": 22430
    },
    {
      "epoch": 0.8407328312914466,
      "grad_norm": 0.10606853663921356,
      "learning_rate": 6.696041234509542e-05,
      "loss": 0.0932,
      "step": 22440
    },
    {
      "epoch": 0.841107489415908,
      "grad_norm": 0.08117399364709854,
      "learning_rate": 6.693127128043816e-05,
      "loss": 0.0927,
      "step": 22450
    },
    {
      "epoch": 0.8414821475403694,
      "grad_norm": 0.06440163403749466,
      "learning_rate": 6.690212371821501e-05,
      "loss": 0.0891,
      "step": 22460
    },
    {
      "epoch": 0.8418568056648308,
      "grad_norm": 0.06650439649820328,
      "learning_rate": 6.687296966961163e-05,
      "loss": 0.0923,
      "step": 22470
    },
    {
      "epoch": 0.8422314637892923,
      "grad_norm": 0.054346468299627304,
      "learning_rate": 6.684380914581622e-05,
      "loss": 0.0911,
      "step": 22480
    },
    {
      "epoch": 0.8426061219137537,
      "grad_norm": 0.06892228871583939,
      "learning_rate": 6.68146421580195e-05,
      "loss": 0.0921,
      "step": 22490
    },
    {
      "epoch": 0.8429807800382151,
      "grad_norm": 0.08349832147359848,
      "learning_rate": 6.678546871741455e-05,
      "loss": 0.0903,
      "step": 22500
    },
    {
      "epoch": 0.8433554381626766,
      "grad_norm": 0.07935231924057007,
      "learning_rate": 6.675628883519708e-05,
      "loss": 0.0913,
      "step": 22510
    },
    {
      "epoch": 0.843730096287138,
      "grad_norm": 0.07379020750522614,
      "learning_rate": 6.672710252256516e-05,
      "loss": 0.0877,
      "step": 22520
    },
    {
      "epoch": 0.8441047544115994,
      "grad_norm": 0.058250781148672104,
      "learning_rate": 6.669790979071937e-05,
      "loss": 0.0936,
      "step": 22530
    },
    {
      "epoch": 0.8444794125360608,
      "grad_norm": 0.09714391082525253,
      "learning_rate": 6.666871065086276e-05,
      "loss": 0.0903,
      "step": 22540
    },
    {
      "epoch": 0.8448540706605223,
      "grad_norm": 0.07679028809070587,
      "learning_rate": 6.663950511420083e-05,
      "loss": 0.0874,
      "step": 22550
    },
    {
      "epoch": 0.8452287287849837,
      "grad_norm": 0.08391616493463516,
      "learning_rate": 6.661029319194152e-05,
      "loss": 0.0919,
      "step": 22560
    },
    {
      "epoch": 0.8456033869094451,
      "grad_norm": 0.058660708367824554,
      "learning_rate": 6.658107489529523e-05,
      "loss": 0.0898,
      "step": 22570
    },
    {
      "epoch": 0.8459780450339066,
      "grad_norm": 0.1235472708940506,
      "learning_rate": 6.655185023547483e-05,
      "loss": 0.0872,
      "step": 22580
    },
    {
      "epoch": 0.846352703158368,
      "grad_norm": 0.06574665755033493,
      "learning_rate": 6.652261922369562e-05,
      "loss": 0.091,
      "step": 22590
    },
    {
      "epoch": 0.8467273612828294,
      "grad_norm": 0.11572814732789993,
      "learning_rate": 6.649338187117528e-05,
      "loss": 0.0926,
      "step": 22600
    },
    {
      "epoch": 0.8471020194072908,
      "grad_norm": 0.2642761766910553,
      "learning_rate": 6.646413818913402e-05,
      "loss": 0.0912,
      "step": 22610
    },
    {
      "epoch": 0.8474766775317523,
      "grad_norm": 0.0641535073518753,
      "learning_rate": 6.643488818879443e-05,
      "loss": 0.0876,
      "step": 22620
    },
    {
      "epoch": 0.8478513356562137,
      "grad_norm": 0.06865214556455612,
      "learning_rate": 6.640563188138151e-05,
      "loss": 0.093,
      "step": 22630
    },
    {
      "epoch": 0.8482259937806751,
      "grad_norm": 0.09954291582107544,
      "learning_rate": 6.637636927812269e-05,
      "loss": 0.0922,
      "step": 22640
    },
    {
      "epoch": 0.8486006519051366,
      "grad_norm": 0.0746060386300087,
      "learning_rate": 6.634710039024785e-05,
      "loss": 0.0917,
      "step": 22650
    },
    {
      "epoch": 0.848975310029598,
      "grad_norm": 0.20425398647785187,
      "learning_rate": 6.631782522898924e-05,
      "loss": 0.0908,
      "step": 22660
    },
    {
      "epoch": 0.8493499681540594,
      "grad_norm": 0.05542251840233803,
      "learning_rate": 6.628854380558152e-05,
      "loss": 0.0881,
      "step": 22670
    },
    {
      "epoch": 0.8497246262785209,
      "grad_norm": 0.08157424628734589,
      "learning_rate": 6.625925613126179e-05,
      "loss": 0.0895,
      "step": 22680
    },
    {
      "epoch": 0.8500992844029823,
      "grad_norm": 0.12317785620689392,
      "learning_rate": 6.622996221726952e-05,
      "loss": 0.0925,
      "step": 22690
    },
    {
      "epoch": 0.8504739425274437,
      "grad_norm": 0.10061201453208923,
      "learning_rate": 6.620066207484656e-05,
      "loss": 0.0911,
      "step": 22700
    },
    {
      "epoch": 0.8508486006519052,
      "grad_norm": 0.08234810084104538,
      "learning_rate": 6.617135571523719e-05,
      "loss": 0.0922,
      "step": 22710
    },
    {
      "epoch": 0.8512232587763666,
      "grad_norm": 0.07607966661453247,
      "learning_rate": 6.614204314968804e-05,
      "loss": 0.0898,
      "step": 22720
    },
    {
      "epoch": 0.851597916900828,
      "grad_norm": 0.08814870566129684,
      "learning_rate": 6.611272438944816e-05,
      "loss": 0.0912,
      "step": 22730
    },
    {
      "epoch": 0.8519725750252894,
      "grad_norm": 0.07117690145969391,
      "learning_rate": 6.608339944576891e-05,
      "loss": 0.0917,
      "step": 22740
    },
    {
      "epoch": 0.8523472331497508,
      "grad_norm": 0.08904584497213364,
      "learning_rate": 6.605406832990413e-05,
      "loss": 0.0924,
      "step": 22750
    },
    {
      "epoch": 0.8527218912742123,
      "grad_norm": 0.07863716036081314,
      "learning_rate": 6.602473105310991e-05,
      "loss": 0.0928,
      "step": 22760
    },
    {
      "epoch": 0.8530965493986737,
      "grad_norm": 0.08667559176683426,
      "learning_rate": 6.59953876266448e-05,
      "loss": 0.0905,
      "step": 22770
    },
    {
      "epoch": 0.8534712075231351,
      "grad_norm": 0.062125857919454575,
      "learning_rate": 6.596603806176962e-05,
      "loss": 0.0896,
      "step": 22780
    },
    {
      "epoch": 0.8538458656475966,
      "grad_norm": 0.07982704043388367,
      "learning_rate": 6.593668236974766e-05,
      "loss": 0.0894,
      "step": 22790
    },
    {
      "epoch": 0.854220523772058,
      "grad_norm": 0.05945631489157677,
      "learning_rate": 6.590732056184445e-05,
      "loss": 0.0887,
      "step": 22800
    },
    {
      "epoch": 0.8545951818965194,
      "grad_norm": 0.07912231236696243,
      "learning_rate": 6.587795264932795e-05,
      "loss": 0.087,
      "step": 22810
    },
    {
      "epoch": 0.8549698400209809,
      "grad_norm": 0.06011611595749855,
      "learning_rate": 6.58485786434684e-05,
      "loss": 0.0884,
      "step": 22820
    },
    {
      "epoch": 0.8553444981454423,
      "grad_norm": 0.06363451480865479,
      "learning_rate": 6.581919855553839e-05,
      "loss": 0.0953,
      "step": 22830
    },
    {
      "epoch": 0.8557191562699037,
      "grad_norm": 0.09523376077413559,
      "learning_rate": 6.57898123968129e-05,
      "loss": 0.0945,
      "step": 22840
    },
    {
      "epoch": 0.8560938143943652,
      "grad_norm": 0.09771724790334702,
      "learning_rate": 6.576042017856918e-05,
      "loss": 0.089,
      "step": 22850
    },
    {
      "epoch": 0.8564684725188265,
      "grad_norm": 0.1277390867471695,
      "learning_rate": 6.573102191208682e-05,
      "loss": 0.0934,
      "step": 22860
    },
    {
      "epoch": 0.856843130643288,
      "grad_norm": 0.0617545060813427,
      "learning_rate": 6.570161760864775e-05,
      "loss": 0.0875,
      "step": 22870
    },
    {
      "epoch": 0.8572177887677495,
      "grad_norm": 0.12854471802711487,
      "learning_rate": 6.567220727953618e-05,
      "loss": 0.0918,
      "step": 22880
    },
    {
      "epoch": 0.8575924468922108,
      "grad_norm": 0.06347968429327011,
      "learning_rate": 6.564279093603868e-05,
      "loss": 0.0879,
      "step": 22890
    },
    {
      "epoch": 0.8579671050166723,
      "grad_norm": 0.07196825742721558,
      "learning_rate": 6.561336858944408e-05,
      "loss": 0.0896,
      "step": 22900
    },
    {
      "epoch": 0.8583417631411338,
      "grad_norm": 0.05892505124211311,
      "learning_rate": 6.558394025104357e-05,
      "loss": 0.0912,
      "step": 22910
    },
    {
      "epoch": 0.8587164212655951,
      "grad_norm": 0.07337994873523712,
      "learning_rate": 6.555450593213054e-05,
      "loss": 0.0907,
      "step": 22920
    },
    {
      "epoch": 0.8590910793900566,
      "grad_norm": 0.09521708637475967,
      "learning_rate": 6.552506564400082e-05,
      "loss": 0.0902,
      "step": 22930
    },
    {
      "epoch": 0.859465737514518,
      "grad_norm": 0.0852898433804512,
      "learning_rate": 6.549561939795241e-05,
      "loss": 0.0952,
      "step": 22940
    },
    {
      "epoch": 0.8598403956389794,
      "grad_norm": 0.06718186289072037,
      "learning_rate": 6.546616720528564e-05,
      "loss": 0.0923,
      "step": 22950
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 0.06338592618703842,
      "learning_rate": 6.543670907730316e-05,
      "loss": 0.0905,
      "step": 22960
    },
    {
      "epoch": 0.8605897118879023,
      "grad_norm": 0.060870423913002014,
      "learning_rate": 6.540724502530978e-05,
      "loss": 0.0889,
      "step": 22970
    },
    {
      "epoch": 0.8609643700123637,
      "grad_norm": 0.11446349322795868,
      "learning_rate": 6.537777506061272e-05,
      "loss": 0.0946,
      "step": 22980
    },
    {
      "epoch": 0.8613390281368252,
      "grad_norm": 0.09200353175401688,
      "learning_rate": 6.53482991945214e-05,
      "loss": 0.0918,
      "step": 22990
    },
    {
      "epoch": 0.8617136862612865,
      "grad_norm": 0.20462395250797272,
      "learning_rate": 6.531881743834751e-05,
      "loss": 0.0972,
      "step": 23000
    },
    {
      "epoch": 0.862088344385748,
      "grad_norm": 0.0852227658033371,
      "learning_rate": 6.5289329803405e-05,
      "loss": 0.0906,
      "step": 23010
    },
    {
      "epoch": 0.8624630025102095,
      "grad_norm": 0.07788845896720886,
      "learning_rate": 6.525983630101008e-05,
      "loss": 0.0892,
      "step": 23020
    },
    {
      "epoch": 0.8628376606346708,
      "grad_norm": 0.058999065309762955,
      "learning_rate": 6.523033694248126e-05,
      "loss": 0.0906,
      "step": 23030
    },
    {
      "epoch": 0.8632123187591323,
      "grad_norm": 0.10355635732412338,
      "learning_rate": 6.520083173913919e-05,
      "loss": 0.0916,
      "step": 23040
    },
    {
      "epoch": 0.8635869768835938,
      "grad_norm": 0.061764512211084366,
      "learning_rate": 6.517132070230685e-05,
      "loss": 0.0884,
      "step": 23050
    },
    {
      "epoch": 0.8639616350080551,
      "grad_norm": 0.06979638338088989,
      "learning_rate": 6.514180384330943e-05,
      "loss": 0.0879,
      "step": 23060
    },
    {
      "epoch": 0.8643362931325166,
      "grad_norm": 0.055544935166835785,
      "learning_rate": 6.511228117347434e-05,
      "loss": 0.0907,
      "step": 23070
    },
    {
      "epoch": 0.864710951256978,
      "grad_norm": 0.07194089144468307,
      "learning_rate": 6.508275270413128e-05,
      "loss": 0.0876,
      "step": 23080
    },
    {
      "epoch": 0.8650856093814394,
      "grad_norm": 0.07031130790710449,
      "learning_rate": 6.505321844661209e-05,
      "loss": 0.0911,
      "step": 23090
    },
    {
      "epoch": 0.8654602675059009,
      "grad_norm": 0.07282184064388275,
      "learning_rate": 6.50236784122509e-05,
      "loss": 0.0885,
      "step": 23100
    },
    {
      "epoch": 0.8658349256303622,
      "grad_norm": 0.07943782955408096,
      "learning_rate": 6.499413261238401e-05,
      "loss": 0.0944,
      "step": 23110
    },
    {
      "epoch": 0.8662095837548237,
      "grad_norm": 0.07872919738292694,
      "learning_rate": 6.496458105834998e-05,
      "loss": 0.0918,
      "step": 23120
    },
    {
      "epoch": 0.8665842418792852,
      "grad_norm": 0.06518007069826126,
      "learning_rate": 6.493502376148952e-05,
      "loss": 0.0904,
      "step": 23130
    },
    {
      "epoch": 0.8669589000037465,
      "grad_norm": 0.08588606119155884,
      "learning_rate": 6.49054607331456e-05,
      "loss": 0.0925,
      "step": 23140
    },
    {
      "epoch": 0.867333558128208,
      "grad_norm": 0.11672822386026382,
      "learning_rate": 6.487589198466336e-05,
      "loss": 0.0918,
      "step": 23150
    },
    {
      "epoch": 0.8677082162526695,
      "grad_norm": 0.08170342445373535,
      "learning_rate": 6.484631752739012e-05,
      "loss": 0.0879,
      "step": 23160
    },
    {
      "epoch": 0.8680828743771308,
      "grad_norm": 0.08526390790939331,
      "learning_rate": 6.481673737267544e-05,
      "loss": 0.0952,
      "step": 23170
    },
    {
      "epoch": 0.8684575325015923,
      "grad_norm": 0.06686793267726898,
      "learning_rate": 6.478715153187101e-05,
      "loss": 0.0917,
      "step": 23180
    },
    {
      "epoch": 0.8688321906260538,
      "grad_norm": 0.057044483721256256,
      "learning_rate": 6.475756001633074e-05,
      "loss": 0.0915,
      "step": 23190
    },
    {
      "epoch": 0.8692068487505151,
      "grad_norm": 0.06714979559183121,
      "learning_rate": 6.472796283741073e-05,
      "loss": 0.0916,
      "step": 23200
    },
    {
      "epoch": 0.8695815068749766,
      "grad_norm": 0.06125131994485855,
      "learning_rate": 6.469836000646921e-05,
      "loss": 0.0899,
      "step": 23210
    },
    {
      "epoch": 0.8699561649994381,
      "grad_norm": 0.0674491673707962,
      "learning_rate": 6.466875153486658e-05,
      "loss": 0.0912,
      "step": 23220
    },
    {
      "epoch": 0.8703308231238994,
      "grad_norm": 0.06413903087377548,
      "learning_rate": 6.463913743396546e-05,
      "loss": 0.089,
      "step": 23230
    },
    {
      "epoch": 0.8707054812483609,
      "grad_norm": 0.08653085678815842,
      "learning_rate": 6.460951771513058e-05,
      "loss": 0.0873,
      "step": 23240
    },
    {
      "epoch": 0.8710801393728222,
      "grad_norm": 0.06807275861501694,
      "learning_rate": 6.457989238972882e-05,
      "loss": 0.0897,
      "step": 23250
    },
    {
      "epoch": 0.8714547974972837,
      "grad_norm": 0.11293654888868332,
      "learning_rate": 6.455026146912926e-05,
      "loss": 0.0935,
      "step": 23260
    },
    {
      "epoch": 0.8718294556217452,
      "grad_norm": 0.07615258544683456,
      "learning_rate": 6.45206249647031e-05,
      "loss": 0.0906,
      "step": 23270
    },
    {
      "epoch": 0.8722041137462065,
      "grad_norm": 0.06927767395973206,
      "learning_rate": 6.449098288782369e-05,
      "loss": 0.0903,
      "step": 23280
    },
    {
      "epoch": 0.872578771870668,
      "grad_norm": 0.10223713517189026,
      "learning_rate": 6.446133524986646e-05,
      "loss": 0.092,
      "step": 23290
    },
    {
      "epoch": 0.8729534299951295,
      "grad_norm": 0.09252870082855225,
      "learning_rate": 6.443168206220909e-05,
      "loss": 0.0896,
      "step": 23300
    },
    {
      "epoch": 0.8733280881195908,
      "grad_norm": 0.07111760973930359,
      "learning_rate": 6.440202333623126e-05,
      "loss": 0.0924,
      "step": 23310
    },
    {
      "epoch": 0.8737027462440523,
      "grad_norm": 0.06014160439372063,
      "learning_rate": 6.43723590833149e-05,
      "loss": 0.087,
      "step": 23320
    },
    {
      "epoch": 0.8740774043685138,
      "grad_norm": 0.06766383349895477,
      "learning_rate": 6.434268931484398e-05,
      "loss": 0.0914,
      "step": 23330
    },
    {
      "epoch": 0.8744520624929751,
      "grad_norm": 0.11536013334989548,
      "learning_rate": 6.431301404220455e-05,
      "loss": 0.088,
      "step": 23340
    },
    {
      "epoch": 0.8748267206174366,
      "grad_norm": 0.08452217280864716,
      "learning_rate": 6.428333327678493e-05,
      "loss": 0.0895,
      "step": 23350
    },
    {
      "epoch": 0.8752013787418981,
      "grad_norm": 0.09161075949668884,
      "learning_rate": 6.425364702997538e-05,
      "loss": 0.0913,
      "step": 23360
    },
    {
      "epoch": 0.8755760368663594,
      "grad_norm": 0.11588023602962494,
      "learning_rate": 6.422395531316836e-05,
      "loss": 0.094,
      "step": 23370
    },
    {
      "epoch": 0.8759506949908209,
      "grad_norm": 0.09166617691516876,
      "learning_rate": 6.419425813775839e-05,
      "loss": 0.09,
      "step": 23380
    },
    {
      "epoch": 0.8763253531152823,
      "grad_norm": 0.07400862127542496,
      "learning_rate": 6.416455551514208e-05,
      "loss": 0.0869,
      "step": 23390
    },
    {
      "epoch": 0.8767000112397437,
      "grad_norm": 0.07584413141012192,
      "learning_rate": 6.41348474567182e-05,
      "loss": 0.0931,
      "step": 23400
    },
    {
      "epoch": 0.8770746693642052,
      "grad_norm": 0.17209263145923615,
      "learning_rate": 6.410513397388748e-05,
      "loss": 0.0894,
      "step": 23410
    },
    {
      "epoch": 0.8774493274886666,
      "grad_norm": 0.10492925345897675,
      "learning_rate": 6.407541507805286e-05,
      "loss": 0.0904,
      "step": 23420
    },
    {
      "epoch": 0.877823985613128,
      "grad_norm": 0.07359525561332703,
      "learning_rate": 6.404569078061926e-05,
      "loss": 0.089,
      "step": 23430
    },
    {
      "epoch": 0.8781986437375895,
      "grad_norm": 0.0659569799900055,
      "learning_rate": 6.401596109299373e-05,
      "loss": 0.0925,
      "step": 23440
    },
    {
      "epoch": 0.8785733018620508,
      "grad_norm": 0.057339832186698914,
      "learning_rate": 6.39862260265854e-05,
      "loss": 0.0882,
      "step": 23450
    },
    {
      "epoch": 0.8789479599865123,
      "grad_norm": 0.07918734103441238,
      "learning_rate": 6.395648559280537e-05,
      "loss": 0.0872,
      "step": 23460
    },
    {
      "epoch": 0.8793226181109738,
      "grad_norm": 0.07635358721017838,
      "learning_rate": 6.392673980306694e-05,
      "loss": 0.0906,
      "step": 23470
    },
    {
      "epoch": 0.8796972762354351,
      "grad_norm": 0.06672636419534683,
      "learning_rate": 6.389698866878533e-05,
      "loss": 0.0903,
      "step": 23480
    },
    {
      "epoch": 0.8800719343598966,
      "grad_norm": 0.19021357595920563,
      "learning_rate": 6.386723220137793e-05,
      "loss": 0.0889,
      "step": 23490
    },
    {
      "epoch": 0.880446592484358,
      "grad_norm": 0.07984480261802673,
      "learning_rate": 6.383747041226408e-05,
      "loss": 0.0897,
      "step": 23500
    },
    {
      "epoch": 0.8808212506088194,
      "grad_norm": 0.16799037158489227,
      "learning_rate": 6.38077033128652e-05,
      "loss": 0.092,
      "step": 23510
    },
    {
      "epoch": 0.8811959087332809,
      "grad_norm": 0.11664331704378128,
      "learning_rate": 6.377793091460478e-05,
      "loss": 0.092,
      "step": 23520
    },
    {
      "epoch": 0.8815705668577423,
      "grad_norm": 0.08652559667825699,
      "learning_rate": 6.374815322890828e-05,
      "loss": 0.0909,
      "step": 23530
    },
    {
      "epoch": 0.8819452249822037,
      "grad_norm": 0.0657506138086319,
      "learning_rate": 6.371837026720325e-05,
      "loss": 0.0873,
      "step": 23540
    },
    {
      "epoch": 0.8823198831066652,
      "grad_norm": 0.09700269252061844,
      "learning_rate": 6.368858204091922e-05,
      "loss": 0.0912,
      "step": 23550
    },
    {
      "epoch": 0.8826945412311266,
      "grad_norm": 0.06396584212779999,
      "learning_rate": 6.365878856148775e-05,
      "loss": 0.0873,
      "step": 23560
    },
    {
      "epoch": 0.883069199355588,
      "grad_norm": 0.08703426271677017,
      "learning_rate": 6.362898984034244e-05,
      "loss": 0.0945,
      "step": 23570
    },
    {
      "epoch": 0.8834438574800495,
      "grad_norm": 0.06819450110197067,
      "learning_rate": 6.359918588891888e-05,
      "loss": 0.0962,
      "step": 23580
    },
    {
      "epoch": 0.8838185156045109,
      "grad_norm": 0.09393388032913208,
      "learning_rate": 6.356937671865466e-05,
      "loss": 0.0896,
      "step": 23590
    },
    {
      "epoch": 0.8841931737289723,
      "grad_norm": 0.06675161421298981,
      "learning_rate": 6.35395623409894e-05,
      "loss": 0.091,
      "step": 23600
    },
    {
      "epoch": 0.8845678318534338,
      "grad_norm": 0.061099957674741745,
      "learning_rate": 6.350974276736471e-05,
      "loss": 0.0906,
      "step": 23610
    },
    {
      "epoch": 0.8849424899778952,
      "grad_norm": 0.09207332134246826,
      "learning_rate": 6.347991800922416e-05,
      "loss": 0.0935,
      "step": 23620
    },
    {
      "epoch": 0.8853171481023566,
      "grad_norm": 0.07987770438194275,
      "learning_rate": 6.345008807801336e-05,
      "loss": 0.0889,
      "step": 23630
    },
    {
      "epoch": 0.885691806226818,
      "grad_norm": 0.07919831573963165,
      "learning_rate": 6.342025298517986e-05,
      "loss": 0.0927,
      "step": 23640
    },
    {
      "epoch": 0.8860664643512794,
      "grad_norm": 0.08039917796850204,
      "learning_rate": 6.339041274217323e-05,
      "loss": 0.0912,
      "step": 23650
    },
    {
      "epoch": 0.8864411224757409,
      "grad_norm": 0.0784306600689888,
      "learning_rate": 6.3360567360445e-05,
      "loss": 0.0949,
      "step": 23660
    },
    {
      "epoch": 0.8868157806002023,
      "grad_norm": 0.10480841249227524,
      "learning_rate": 6.333071685144866e-05,
      "loss": 0.0908,
      "step": 23670
    },
    {
      "epoch": 0.8871904387246637,
      "grad_norm": 0.07227126508951187,
      "learning_rate": 6.33008612266397e-05,
      "loss": 0.0889,
      "step": 23680
    },
    {
      "epoch": 0.8875650968491252,
      "grad_norm": 0.08781939744949341,
      "learning_rate": 6.327100049747551e-05,
      "loss": 0.0899,
      "step": 23690
    },
    {
      "epoch": 0.8879397549735866,
      "grad_norm": 0.0841536670923233,
      "learning_rate": 6.324113467541553e-05,
      "loss": 0.0913,
      "step": 23700
    },
    {
      "epoch": 0.888314413098048,
      "grad_norm": 0.07424595206975937,
      "learning_rate": 6.321126377192107e-05,
      "loss": 0.0937,
      "step": 23710
    },
    {
      "epoch": 0.8886890712225095,
      "grad_norm": 0.08749480545520782,
      "learning_rate": 6.318138779845543e-05,
      "loss": 0.0892,
      "step": 23720
    },
    {
      "epoch": 0.8890637293469709,
      "grad_norm": 0.10739374160766602,
      "learning_rate": 6.315150676648388e-05,
      "loss": 0.0962,
      "step": 23730
    },
    {
      "epoch": 0.8894383874714323,
      "grad_norm": 0.0966254472732544,
      "learning_rate": 6.312162068747358e-05,
      "loss": 0.0887,
      "step": 23740
    },
    {
      "epoch": 0.8898130455958937,
      "grad_norm": 0.07846809923648834,
      "learning_rate": 6.309172957289364e-05,
      "loss": 0.0872,
      "step": 23750
    },
    {
      "epoch": 0.8901877037203552,
      "grad_norm": 0.06501045823097229,
      "learning_rate": 6.306183343421514e-05,
      "loss": 0.0907,
      "step": 23760
    },
    {
      "epoch": 0.8905623618448166,
      "grad_norm": 0.08690787851810455,
      "learning_rate": 6.303193228291103e-05,
      "loss": 0.0911,
      "step": 23770
    },
    {
      "epoch": 0.890937019969278,
      "grad_norm": 0.0992605909705162,
      "learning_rate": 6.300202613045622e-05,
      "loss": 0.0927,
      "step": 23780
    },
    {
      "epoch": 0.8913116780937395,
      "grad_norm": 0.11031991243362427,
      "learning_rate": 6.297211498832754e-05,
      "loss": 0.0914,
      "step": 23790
    },
    {
      "epoch": 0.8916863362182009,
      "grad_norm": 0.0847238302230835,
      "learning_rate": 6.294219886800372e-05,
      "loss": 0.0925,
      "step": 23800
    },
    {
      "epoch": 0.8920609943426623,
      "grad_norm": 0.06892501562833786,
      "learning_rate": 6.291227778096545e-05,
      "loss": 0.092,
      "step": 23810
    },
    {
      "epoch": 0.8924356524671238,
      "grad_norm": 0.09688354283571243,
      "learning_rate": 6.288235173869522e-05,
      "loss": 0.0905,
      "step": 23820
    },
    {
      "epoch": 0.8928103105915852,
      "grad_norm": 0.1425483524799347,
      "learning_rate": 6.28524207526775e-05,
      "loss": 0.0919,
      "step": 23830
    },
    {
      "epoch": 0.8931849687160466,
      "grad_norm": 0.06775310635566711,
      "learning_rate": 6.282248483439868e-05,
      "loss": 0.0915,
      "step": 23840
    },
    {
      "epoch": 0.893559626840508,
      "grad_norm": 0.08392152190208435,
      "learning_rate": 6.279254399534697e-05,
      "loss": 0.0909,
      "step": 23850
    },
    {
      "epoch": 0.8939342849649695,
      "grad_norm": 0.07830940932035446,
      "learning_rate": 6.276259824701252e-05,
      "loss": 0.0889,
      "step": 23860
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 0.0690249353647232,
      "learning_rate": 6.273264760088733e-05,
      "loss": 0.0877,
      "step": 23870
    },
    {
      "epoch": 0.8946836012138923,
      "grad_norm": 0.07012181729078293,
      "learning_rate": 6.270269206846533e-05,
      "loss": 0.0878,
      "step": 23880
    },
    {
      "epoch": 0.8950582593383537,
      "grad_norm": 0.07984007149934769,
      "learning_rate": 6.267273166124227e-05,
      "loss": 0.0933,
      "step": 23890
    },
    {
      "epoch": 0.8954329174628152,
      "grad_norm": 0.07708524912595749,
      "learning_rate": 6.26427663907158e-05,
      "loss": 0.0882,
      "step": 23900
    },
    {
      "epoch": 0.8958075755872766,
      "grad_norm": 0.06736760586500168,
      "learning_rate": 6.261279626838542e-05,
      "loss": 0.0904,
      "step": 23910
    },
    {
      "epoch": 0.896182233711738,
      "grad_norm": 0.05993080511689186,
      "learning_rate": 6.25828213057525e-05,
      "loss": 0.0947,
      "step": 23920
    },
    {
      "epoch": 0.8965568918361995,
      "grad_norm": 0.09006904065608978,
      "learning_rate": 6.25528415143203e-05,
      "loss": 0.0901,
      "step": 23930
    },
    {
      "epoch": 0.8969315499606609,
      "grad_norm": 0.07070045173168182,
      "learning_rate": 6.252285690559385e-05,
      "loss": 0.091,
      "step": 23940
    },
    {
      "epoch": 0.8973062080851223,
      "grad_norm": 0.07774205505847931,
      "learning_rate": 6.249286749108015e-05,
      "loss": 0.0911,
      "step": 23950
    },
    {
      "epoch": 0.8976808662095838,
      "grad_norm": 0.07222796231508255,
      "learning_rate": 6.24628732822879e-05,
      "loss": 0.0885,
      "step": 23960
    },
    {
      "epoch": 0.8980555243340452,
      "grad_norm": 0.08492938429117203,
      "learning_rate": 6.243287429072777e-05,
      "loss": 0.093,
      "step": 23970
    },
    {
      "epoch": 0.8984301824585066,
      "grad_norm": 0.13431261479854584,
      "learning_rate": 6.240287052791221e-05,
      "loss": 0.0926,
      "step": 23980
    },
    {
      "epoch": 0.8988048405829681,
      "grad_norm": 0.07742443680763245,
      "learning_rate": 6.237286200535545e-05,
      "loss": 0.0909,
      "step": 23990
    },
    {
      "epoch": 0.8991794987074295,
      "grad_norm": 0.059351421892642975,
      "learning_rate": 6.234284873457365e-05,
      "loss": 0.0889,
      "step": 24000
    },
    {
      "epoch": 0.8995541568318909,
      "grad_norm": 0.06623125821352005,
      "learning_rate": 6.231283072708472e-05,
      "loss": 0.0935,
      "step": 24010
    },
    {
      "epoch": 0.8999288149563524,
      "grad_norm": 0.07216385006904602,
      "learning_rate": 6.228280799440843e-05,
      "loss": 0.091,
      "step": 24020
    },
    {
      "epoch": 0.9003034730808137,
      "grad_norm": 0.10645867139101028,
      "learning_rate": 6.225278054806631e-05,
      "loss": 0.092,
      "step": 24030
    },
    {
      "epoch": 0.9006781312052752,
      "grad_norm": 0.06790327280759811,
      "learning_rate": 6.222274839958173e-05,
      "loss": 0.0893,
      "step": 24040
    },
    {
      "epoch": 0.9010527893297366,
      "grad_norm": 0.08140029013156891,
      "learning_rate": 6.21927115604799e-05,
      "loss": 0.0923,
      "step": 24050
    },
    {
      "epoch": 0.901427447454198,
      "grad_norm": 0.06299365311861038,
      "learning_rate": 6.216267004228775e-05,
      "loss": 0.0902,
      "step": 24060
    },
    {
      "epoch": 0.9018021055786595,
      "grad_norm": 0.06617631018161774,
      "learning_rate": 6.21326238565341e-05,
      "loss": 0.089,
      "step": 24070
    },
    {
      "epoch": 0.9021767637031209,
      "grad_norm": 0.08587883412837982,
      "learning_rate": 6.210257301474948e-05,
      "loss": 0.093,
      "step": 24080
    },
    {
      "epoch": 0.9025514218275823,
      "grad_norm": 0.07798627763986588,
      "learning_rate": 6.207251752846623e-05,
      "loss": 0.0913,
      "step": 24090
    },
    {
      "epoch": 0.9029260799520438,
      "grad_norm": 0.07264664769172668,
      "learning_rate": 6.204245740921852e-05,
      "loss": 0.0916,
      "step": 24100
    },
    {
      "epoch": 0.9033007380765052,
      "grad_norm": 0.07594019919633865,
      "learning_rate": 6.20123926685422e-05,
      "loss": 0.0876,
      "step": 24110
    },
    {
      "epoch": 0.9036753962009666,
      "grad_norm": 0.06480644643306732,
      "learning_rate": 6.198232331797503e-05,
      "loss": 0.0881,
      "step": 24120
    },
    {
      "epoch": 0.9040500543254281,
      "grad_norm": 0.08315308392047882,
      "learning_rate": 6.19522493690564e-05,
      "loss": 0.0897,
      "step": 24130
    },
    {
      "epoch": 0.9044247124498894,
      "grad_norm": 0.08067085593938828,
      "learning_rate": 6.192217083332754e-05,
      "loss": 0.0929,
      "step": 24140
    },
    {
      "epoch": 0.9047993705743509,
      "grad_norm": 0.12400149554014206,
      "learning_rate": 6.189208772233144e-05,
      "loss": 0.0941,
      "step": 24150
    },
    {
      "epoch": 0.9051740286988124,
      "grad_norm": 0.08312802016735077,
      "learning_rate": 6.186200004761282e-05,
      "loss": 0.0904,
      "step": 24160
    },
    {
      "epoch": 0.9055486868232737,
      "grad_norm": 0.07883057743310928,
      "learning_rate": 6.183190782071819e-05,
      "loss": 0.09,
      "step": 24170
    },
    {
      "epoch": 0.9059233449477352,
      "grad_norm": 0.058290135115385056,
      "learning_rate": 6.180181105319574e-05,
      "loss": 0.087,
      "step": 24180
    },
    {
      "epoch": 0.9062980030721967,
      "grad_norm": 0.06454671919345856,
      "learning_rate": 6.177170975659547e-05,
      "loss": 0.0932,
      "step": 24190
    },
    {
      "epoch": 0.906672661196658,
      "grad_norm": 0.06497494131326675,
      "learning_rate": 6.174160394246908e-05,
      "loss": 0.0911,
      "step": 24200
    },
    {
      "epoch": 0.9070473193211195,
      "grad_norm": 0.07311037182807922,
      "learning_rate": 6.171149362237002e-05,
      "loss": 0.0866,
      "step": 24210
    },
    {
      "epoch": 0.907421977445581,
      "grad_norm": 0.06451747566461563,
      "learning_rate": 6.168137880785347e-05,
      "loss": 0.0886,
      "step": 24220
    },
    {
      "epoch": 0.9077966355700423,
      "grad_norm": 0.11153014749288559,
      "learning_rate": 6.165125951047631e-05,
      "loss": 0.0925,
      "step": 24230
    },
    {
      "epoch": 0.9081712936945038,
      "grad_norm": 0.08393078297376633,
      "learning_rate": 6.162113574179718e-05,
      "loss": 0.0941,
      "step": 24240
    },
    {
      "epoch": 0.9085459518189652,
      "grad_norm": 0.1221308708190918,
      "learning_rate": 6.159100751337642e-05,
      "loss": 0.0905,
      "step": 24250
    },
    {
      "epoch": 0.9089206099434266,
      "grad_norm": 0.09155933558940887,
      "learning_rate": 6.156087483677606e-05,
      "loss": 0.0945,
      "step": 24260
    },
    {
      "epoch": 0.9092952680678881,
      "grad_norm": 0.06801284104585648,
      "learning_rate": 6.153073772355985e-05,
      "loss": 0.0921,
      "step": 24270
    },
    {
      "epoch": 0.9096699261923494,
      "grad_norm": 0.07638470083475113,
      "learning_rate": 6.150059618529325e-05,
      "loss": 0.0924,
      "step": 24280
    },
    {
      "epoch": 0.9100445843168109,
      "grad_norm": 0.09923775494098663,
      "learning_rate": 6.147045023354342e-05,
      "loss": 0.0913,
      "step": 24290
    },
    {
      "epoch": 0.9104192424412724,
      "grad_norm": 0.07151620835065842,
      "learning_rate": 6.144029987987921e-05,
      "loss": 0.0901,
      "step": 24300
    },
    {
      "epoch": 0.9107939005657337,
      "grad_norm": 0.09754384309053421,
      "learning_rate": 6.141014513587117e-05,
      "loss": 0.094,
      "step": 24310
    },
    {
      "epoch": 0.9111685586901952,
      "grad_norm": 0.05115291476249695,
      "learning_rate": 6.13799860130915e-05,
      "loss": 0.0893,
      "step": 24320
    },
    {
      "epoch": 0.9115432168146567,
      "grad_norm": 0.06090127304196358,
      "learning_rate": 6.13498225231141e-05,
      "loss": 0.0883,
      "step": 24330
    },
    {
      "epoch": 0.911917874939118,
      "grad_norm": 0.17074337601661682,
      "learning_rate": 6.131965467751457e-05,
      "loss": 0.0904,
      "step": 24340
    },
    {
      "epoch": 0.9122925330635795,
      "grad_norm": 0.06093444302678108,
      "learning_rate": 6.128948248787013e-05,
      "loss": 0.0907,
      "step": 24350
    },
    {
      "epoch": 0.912667191188041,
      "grad_norm": 0.07608393579721451,
      "learning_rate": 6.125930596575974e-05,
      "loss": 0.09,
      "step": 24360
    },
    {
      "epoch": 0.9130418493125023,
      "grad_norm": 0.11478690057992935,
      "learning_rate": 6.122912512276394e-05,
      "loss": 0.0917,
      "step": 24370
    },
    {
      "epoch": 0.9134165074369638,
      "grad_norm": 0.13398198783397675,
      "learning_rate": 6.1198939970465e-05,
      "loss": 0.0914,
      "step": 24380
    },
    {
      "epoch": 0.9137911655614251,
      "grad_norm": 0.06130187213420868,
      "learning_rate": 6.11687505204468e-05,
      "loss": 0.0903,
      "step": 24390
    },
    {
      "epoch": 0.9141658236858866,
      "grad_norm": 0.0711129754781723,
      "learning_rate": 6.113855678429485e-05,
      "loss": 0.0905,
      "step": 24400
    },
    {
      "epoch": 0.9145404818103481,
      "grad_norm": 0.07760405540466309,
      "learning_rate": 6.110835877359639e-05,
      "loss": 0.0889,
      "step": 24410
    },
    {
      "epoch": 0.9149151399348094,
      "grad_norm": 0.08342026174068451,
      "learning_rate": 6.10781564999402e-05,
      "loss": 0.0934,
      "step": 24420
    },
    {
      "epoch": 0.9152897980592709,
      "grad_norm": 0.07285699248313904,
      "learning_rate": 6.104794997491678e-05,
      "loss": 0.0872,
      "step": 24430
    },
    {
      "epoch": 0.9156644561837324,
      "grad_norm": 0.07521215081214905,
      "learning_rate": 6.101773921011821e-05,
      "loss": 0.0908,
      "step": 24440
    },
    {
      "epoch": 0.9160391143081937,
      "grad_norm": 0.08031070977449417,
      "learning_rate": 6.0987524217138194e-05,
      "loss": 0.0923,
      "step": 24450
    },
    {
      "epoch": 0.9164137724326552,
      "grad_norm": 0.08113542199134827,
      "learning_rate": 6.0957305007572095e-05,
      "loss": 0.0908,
      "step": 24460
    },
    {
      "epoch": 0.9167884305571167,
      "grad_norm": 0.07349041104316711,
      "learning_rate": 6.0927081593016877e-05,
      "loss": 0.0941,
      "step": 24470
    },
    {
      "epoch": 0.917163088681578,
      "grad_norm": 0.06477569788694382,
      "learning_rate": 6.08968539850711e-05,
      "loss": 0.086,
      "step": 24480
    },
    {
      "epoch": 0.9175377468060395,
      "grad_norm": 0.08723331242799759,
      "learning_rate": 6.086662219533496e-05,
      "loss": 0.0856,
      "step": 24490
    },
    {
      "epoch": 0.917912404930501,
      "grad_norm": 0.09143921732902527,
      "learning_rate": 6.083638623541024e-05,
      "loss": 0.0895,
      "step": 24500
    },
    {
      "epoch": 0.9182870630549623,
      "grad_norm": 0.09396552294492722,
      "learning_rate": 6.0806146116900345e-05,
      "loss": 0.096,
      "step": 24510
    },
    {
      "epoch": 0.9186617211794238,
      "grad_norm": 0.09820953756570816,
      "learning_rate": 6.077590185141022e-05,
      "loss": 0.0909,
      "step": 24520
    },
    {
      "epoch": 0.9190363793038852,
      "grad_norm": 0.07622024416923523,
      "learning_rate": 6.07456534505465e-05,
      "loss": 0.0915,
      "step": 24530
    },
    {
      "epoch": 0.9194110374283466,
      "grad_norm": 0.0631960928440094,
      "learning_rate": 6.071540092591732e-05,
      "loss": 0.0902,
      "step": 24540
    },
    {
      "epoch": 0.9197856955528081,
      "grad_norm": 0.10124729573726654,
      "learning_rate": 6.068514428913241e-05,
      "loss": 0.0888,
      "step": 24550
    },
    {
      "epoch": 0.9201603536772694,
      "grad_norm": 0.0648416057229042,
      "learning_rate": 6.065488355180313e-05,
      "loss": 0.0894,
      "step": 24560
    },
    {
      "epoch": 0.9205350118017309,
      "grad_norm": 0.08057951182126999,
      "learning_rate": 6.0624618725542346e-05,
      "loss": 0.0931,
      "step": 24570
    },
    {
      "epoch": 0.9209096699261924,
      "grad_norm": 0.06098553538322449,
      "learning_rate": 6.059434982196456e-05,
      "loss": 0.0905,
      "step": 24580
    },
    {
      "epoch": 0.9212843280506537,
      "grad_norm": 0.08614767342805862,
      "learning_rate": 6.056407685268576e-05,
      "loss": 0.0903,
      "step": 24590
    },
    {
      "epoch": 0.9216589861751152,
      "grad_norm": 0.09682881087064743,
      "learning_rate": 6.053379982932359e-05,
      "loss": 0.0917,
      "step": 24600
    },
    {
      "epoch": 0.9220336442995767,
      "grad_norm": 0.06435126066207886,
      "learning_rate": 6.050351876349716e-05,
      "loss": 0.0891,
      "step": 24610
    },
    {
      "epoch": 0.922408302424038,
      "grad_norm": 0.07478626072406769,
      "learning_rate": 6.0473233666827175e-05,
      "loss": 0.0924,
      "step": 24620
    },
    {
      "epoch": 0.9227829605484995,
      "grad_norm": 0.07569906115531921,
      "learning_rate": 6.044294455093591e-05,
      "loss": 0.0912,
      "step": 24630
    },
    {
      "epoch": 0.923157618672961,
      "grad_norm": 0.08345537632703781,
      "learning_rate": 6.041265142744711e-05,
      "loss": 0.0916,
      "step": 24640
    },
    {
      "epoch": 0.9235322767974223,
      "grad_norm": 0.06908706575632095,
      "learning_rate": 6.038235430798615e-05,
      "loss": 0.0932,
      "step": 24650
    },
    {
      "epoch": 0.9239069349218838,
      "grad_norm": 0.07580840587615967,
      "learning_rate": 6.0352053204179846e-05,
      "loss": 0.091,
      "step": 24660
    },
    {
      "epoch": 0.9242815930463452,
      "grad_norm": 0.10604081302881241,
      "learning_rate": 6.032174812765661e-05,
      "loss": 0.0893,
      "step": 24670
    },
    {
      "epoch": 0.9246562511708066,
      "grad_norm": 0.06198468431830406,
      "learning_rate": 6.029143909004636e-05,
      "loss": 0.0894,
      "step": 24680
    },
    {
      "epoch": 0.9250309092952681,
      "grad_norm": 0.05596288666129112,
      "learning_rate": 6.026112610298051e-05,
      "loss": 0.0906,
      "step": 24690
    },
    {
      "epoch": 0.9254055674197295,
      "grad_norm": 0.07877343147993088,
      "learning_rate": 6.023080917809205e-05,
      "loss": 0.0903,
      "step": 24700
    },
    {
      "epoch": 0.9257802255441909,
      "grad_norm": 0.06938827037811279,
      "learning_rate": 6.0200488327015394e-05,
      "loss": 0.093,
      "step": 24710
    },
    {
      "epoch": 0.9261548836686524,
      "grad_norm": 0.07014472782611847,
      "learning_rate": 6.0170163561386536e-05,
      "loss": 0.0895,
      "step": 24720
    },
    {
      "epoch": 0.9265295417931138,
      "grad_norm": 0.06434649974107742,
      "learning_rate": 6.013983489284294e-05,
      "loss": 0.0907,
      "step": 24730
    },
    {
      "epoch": 0.9269041999175752,
      "grad_norm": 0.07345777750015259,
      "learning_rate": 6.010950233302358e-05,
      "loss": 0.093,
      "step": 24740
    },
    {
      "epoch": 0.9272788580420367,
      "grad_norm": 0.06784012913703918,
      "learning_rate": 6.007916589356892e-05,
      "loss": 0.0919,
      "step": 24750
    },
    {
      "epoch": 0.927653516166498,
      "grad_norm": 0.06566067785024643,
      "learning_rate": 6.004882558612088e-05,
      "loss": 0.0925,
      "step": 24760
    },
    {
      "epoch": 0.9280281742909595,
      "grad_norm": 0.07461437582969666,
      "learning_rate": 6.001848142232294e-05,
      "loss": 0.094,
      "step": 24770
    },
    {
      "epoch": 0.9284028324154209,
      "grad_norm": 0.08897913992404938,
      "learning_rate": 5.998813341381998e-05,
      "loss": 0.0922,
      "step": 24780
    },
    {
      "epoch": 0.9287774905398823,
      "grad_norm": 0.06115860491991043,
      "learning_rate": 5.99577815722584e-05,
      "loss": 0.0868,
      "step": 24790
    },
    {
      "epoch": 0.9291521486643438,
      "grad_norm": 0.07116291671991348,
      "learning_rate": 5.992742590928606e-05,
      "loss": 0.09,
      "step": 24800
    },
    {
      "epoch": 0.9295268067888052,
      "grad_norm": 0.06476180255413055,
      "learning_rate": 5.98970664365523e-05,
      "loss": 0.09,
      "step": 24810
    },
    {
      "epoch": 0.9299014649132666,
      "grad_norm": 0.06814068555831909,
      "learning_rate": 5.98667031657079e-05,
      "loss": 0.0897,
      "step": 24820
    },
    {
      "epoch": 0.9302761230377281,
      "grad_norm": 0.06262686103582382,
      "learning_rate": 5.983633610840508e-05,
      "loss": 0.0898,
      "step": 24830
    },
    {
      "epoch": 0.9306507811621895,
      "grad_norm": 0.08864607661962509,
      "learning_rate": 5.9805965276297594e-05,
      "loss": 0.0916,
      "step": 24840
    },
    {
      "epoch": 0.9310254392866509,
      "grad_norm": 0.08508287370204926,
      "learning_rate": 5.977559068104055e-05,
      "loss": 0.0917,
      "step": 24850
    },
    {
      "epoch": 0.9314000974111124,
      "grad_norm": 0.06977374851703644,
      "learning_rate": 5.9745212334290556e-05,
      "loss": 0.0856,
      "step": 24860
    },
    {
      "epoch": 0.9317747555355738,
      "grad_norm": 0.07319151610136032,
      "learning_rate": 5.971483024770563e-05,
      "loss": 0.0873,
      "step": 24870
    },
    {
      "epoch": 0.9321494136600352,
      "grad_norm": 0.05574857443571091,
      "learning_rate": 5.9684444432945266e-05,
      "loss": 0.091,
      "step": 24880
    },
    {
      "epoch": 0.9325240717844967,
      "grad_norm": 0.09961998462677002,
      "learning_rate": 5.9654054901670356e-05,
      "loss": 0.0885,
      "step": 24890
    },
    {
      "epoch": 0.9328987299089581,
      "grad_norm": 0.06544371694326401,
      "learning_rate": 5.96236616655432e-05,
      "loss": 0.0902,
      "step": 24900
    },
    {
      "epoch": 0.9332733880334195,
      "grad_norm": 0.07434147596359253,
      "learning_rate": 5.959326473622756e-05,
      "loss": 0.0897,
      "step": 24910
    },
    {
      "epoch": 0.9336480461578809,
      "grad_norm": 0.06380265951156616,
      "learning_rate": 5.9562864125388605e-05,
      "loss": 0.0917,
      "step": 24920
    },
    {
      "epoch": 0.9340227042823424,
      "grad_norm": 0.08434002101421356,
      "learning_rate": 5.9532459844692915e-05,
      "loss": 0.0885,
      "step": 24930
    },
    {
      "epoch": 0.9343973624068038,
      "grad_norm": 0.12691813707351685,
      "learning_rate": 5.9502051905808455e-05,
      "loss": 0.0967,
      "step": 24940
    },
    {
      "epoch": 0.9347720205312652,
      "grad_norm": 0.07018198817968369,
      "learning_rate": 5.947164032040462e-05,
      "loss": 0.0887,
      "step": 24950
    },
    {
      "epoch": 0.9351466786557266,
      "grad_norm": 0.0833701565861702,
      "learning_rate": 5.944122510015223e-05,
      "loss": 0.0897,
      "step": 24960
    },
    {
      "epoch": 0.9355213367801881,
      "grad_norm": 0.20993278920650482,
      "learning_rate": 5.941080625672343e-05,
      "loss": 0.0891,
      "step": 24970
    },
    {
      "epoch": 0.9358959949046495,
      "grad_norm": 0.11335062980651855,
      "learning_rate": 5.93803838017918e-05,
      "loss": 0.093,
      "step": 24980
    },
    {
      "epoch": 0.9362706530291109,
      "grad_norm": 0.07042544335126877,
      "learning_rate": 5.93499577470323e-05,
      "loss": 0.0911,
      "step": 24990
    },
    {
      "epoch": 0.9366453111535724,
      "grad_norm": 0.06778290867805481,
      "learning_rate": 5.931952810412129e-05,
      "loss": 0.0908,
      "step": 25000
    },
    {
      "epoch": 0.9370199692780338,
      "grad_norm": 0.0831259936094284,
      "learning_rate": 5.928909488473646e-05,
      "loss": 0.0917,
      "step": 25010
    },
    {
      "epoch": 0.9373946274024952,
      "grad_norm": 0.07972556352615356,
      "learning_rate": 5.925865810055694e-05,
      "loss": 0.0871,
      "step": 25020
    },
    {
      "epoch": 0.9377692855269566,
      "grad_norm": 0.06216837465763092,
      "learning_rate": 5.922821776326314e-05,
      "loss": 0.0931,
      "step": 25030
    },
    {
      "epoch": 0.9381439436514181,
      "grad_norm": 0.06990624964237213,
      "learning_rate": 5.9197773884536914e-05,
      "loss": 0.0919,
      "step": 25040
    },
    {
      "epoch": 0.9385186017758795,
      "grad_norm": 0.06225414201617241,
      "learning_rate": 5.9167326476061434e-05,
      "loss": 0.0917,
      "step": 25050
    },
    {
      "epoch": 0.9388932599003409,
      "grad_norm": 0.0752716138958931,
      "learning_rate": 5.913687554952123e-05,
      "loss": 0.0924,
      "step": 25060
    },
    {
      "epoch": 0.9392679180248024,
      "grad_norm": 0.08715107291936874,
      "learning_rate": 5.910642111660221e-05,
      "loss": 0.0918,
      "step": 25070
    },
    {
      "epoch": 0.9396425761492638,
      "grad_norm": 0.0746690183877945,
      "learning_rate": 5.907596318899157e-05,
      "loss": 0.0877,
      "step": 25080
    },
    {
      "epoch": 0.9400172342737252,
      "grad_norm": 0.12068497389554977,
      "learning_rate": 5.9045501778377923e-05,
      "loss": 0.0879,
      "step": 25090
    },
    {
      "epoch": 0.9403918923981867,
      "grad_norm": 0.25488340854644775,
      "learning_rate": 5.901503689645113e-05,
      "loss": 0.0908,
      "step": 25100
    },
    {
      "epoch": 0.9407665505226481,
      "grad_norm": 0.06273698806762695,
      "learning_rate": 5.898456855490249e-05,
      "loss": 0.0923,
      "step": 25110
    },
    {
      "epoch": 0.9411412086471095,
      "grad_norm": 0.08416128903627396,
      "learning_rate": 5.8954096765424515e-05,
      "loss": 0.0897,
      "step": 25120
    },
    {
      "epoch": 0.941515866771571,
      "grad_norm": 0.1049966812133789,
      "learning_rate": 5.892362153971113e-05,
      "loss": 0.0899,
      "step": 25130
    },
    {
      "epoch": 0.9418905248960324,
      "grad_norm": 0.07139455527067184,
      "learning_rate": 5.8893142889457557e-05,
      "loss": 0.0932,
      "step": 25140
    },
    {
      "epoch": 0.9422651830204938,
      "grad_norm": 0.07342545688152313,
      "learning_rate": 5.886266082636027e-05,
      "loss": 0.0903,
      "step": 25150
    },
    {
      "epoch": 0.9426398411449552,
      "grad_norm": 0.0713609978556633,
      "learning_rate": 5.883217536211717e-05,
      "loss": 0.0922,
      "step": 25160
    },
    {
      "epoch": 0.9430144992694166,
      "grad_norm": 0.08302410691976547,
      "learning_rate": 5.8801686508427344e-05,
      "loss": 0.0905,
      "step": 25170
    },
    {
      "epoch": 0.9433891573938781,
      "grad_norm": 0.06744296103715897,
      "learning_rate": 5.877119427699127e-05,
      "loss": 0.0914,
      "step": 25180
    },
    {
      "epoch": 0.9437638155183395,
      "grad_norm": 0.07088305801153183,
      "learning_rate": 5.874069867951065e-05,
      "loss": 0.0883,
      "step": 25190
    },
    {
      "epoch": 0.9441384736428009,
      "grad_norm": 0.08356957882642746,
      "learning_rate": 5.871019972768854e-05,
      "loss": 0.0908,
      "step": 25200
    },
    {
      "epoch": 0.9445131317672624,
      "grad_norm": 0.06510134041309357,
      "learning_rate": 5.867969743322925e-05,
      "loss": 0.0906,
      "step": 25210
    },
    {
      "epoch": 0.9448877898917238,
      "grad_norm": 0.05766725912690163,
      "learning_rate": 5.8649191807838364e-05,
      "loss": 0.0876,
      "step": 25220
    },
    {
      "epoch": 0.9452624480161852,
      "grad_norm": 0.06680604815483093,
      "learning_rate": 5.8618682863222795e-05,
      "loss": 0.0887,
      "step": 25230
    },
    {
      "epoch": 0.9456371061406467,
      "grad_norm": 0.07480104267597198,
      "learning_rate": 5.858817061109065e-05,
      "loss": 0.0887,
      "step": 25240
    },
    {
      "epoch": 0.9460117642651081,
      "grad_norm": 0.06004331633448601,
      "learning_rate": 5.8557655063151375e-05,
      "loss": 0.0945,
      "step": 25250
    },
    {
      "epoch": 0.9463864223895695,
      "grad_norm": 0.13263662159442902,
      "learning_rate": 5.852713623111565e-05,
      "loss": 0.09,
      "step": 25260
    },
    {
      "epoch": 0.946761080514031,
      "grad_norm": 0.060562100261449814,
      "learning_rate": 5.8496614126695404e-05,
      "loss": 0.0918,
      "step": 25270
    },
    {
      "epoch": 0.9471357386384924,
      "grad_norm": 0.06298281252384186,
      "learning_rate": 5.8466088761603874e-05,
      "loss": 0.0844,
      "step": 25280
    },
    {
      "epoch": 0.9475103967629538,
      "grad_norm": 0.058367617428302765,
      "learning_rate": 5.843556014755548e-05,
      "loss": 0.0937,
      "step": 25290
    },
    {
      "epoch": 0.9478850548874153,
      "grad_norm": 0.0788491815328598,
      "learning_rate": 5.840502829626594e-05,
      "loss": 0.0914,
      "step": 25300
    },
    {
      "epoch": 0.9482597130118766,
      "grad_norm": 0.08363159745931625,
      "learning_rate": 5.8374493219452185e-05,
      "loss": 0.0874,
      "step": 25310
    },
    {
      "epoch": 0.9486343711363381,
      "grad_norm": 0.08961854875087738,
      "learning_rate": 5.83439549288324e-05,
      "loss": 0.0913,
      "step": 25320
    },
    {
      "epoch": 0.9490090292607996,
      "grad_norm": 0.08433286845684052,
      "learning_rate": 5.831341343612601e-05,
      "loss": 0.0878,
      "step": 25330
    },
    {
      "epoch": 0.9493836873852609,
      "grad_norm": 0.08448033779859543,
      "learning_rate": 5.828286875305361e-05,
      "loss": 0.0941,
      "step": 25340
    },
    {
      "epoch": 0.9497583455097224,
      "grad_norm": 0.09414584189653397,
      "learning_rate": 5.825232089133712e-05,
      "loss": 0.0905,
      "step": 25350
    },
    {
      "epoch": 0.9501330036341838,
      "grad_norm": 0.08427025377750397,
      "learning_rate": 5.82217698626996e-05,
      "loss": 0.0899,
      "step": 25360
    },
    {
      "epoch": 0.9505076617586452,
      "grad_norm": 0.08008498698472977,
      "learning_rate": 5.819121567886535e-05,
      "loss": 0.0916,
      "step": 25370
    },
    {
      "epoch": 0.9508823198831067,
      "grad_norm": 0.09330049157142639,
      "learning_rate": 5.816065835155989e-05,
      "loss": 0.086,
      "step": 25380
    },
    {
      "epoch": 0.9512569780075681,
      "grad_norm": 0.08786296099424362,
      "learning_rate": 5.8130097892509936e-05,
      "loss": 0.091,
      "step": 25390
    },
    {
      "epoch": 0.9516316361320295,
      "grad_norm": 0.09251812100410461,
      "learning_rate": 5.8099534313443425e-05,
      "loss": 0.0917,
      "step": 25400
    },
    {
      "epoch": 0.952006294256491,
      "grad_norm": 0.07163763046264648,
      "learning_rate": 5.806896762608942e-05,
      "loss": 0.0935,
      "step": 25410
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.07055855542421341,
      "learning_rate": 5.8038397842178305e-05,
      "loss": 0.0913,
      "step": 25420
    },
    {
      "epoch": 0.9527556105054138,
      "grad_norm": 0.14470158517360687,
      "learning_rate": 5.8007824973441525e-05,
      "loss": 0.0917,
      "step": 25430
    },
    {
      "epoch": 0.9531302686298753,
      "grad_norm": 0.060834851115942,
      "learning_rate": 5.797724903161178e-05,
      "loss": 0.0923,
      "step": 25440
    },
    {
      "epoch": 0.9535049267543366,
      "grad_norm": 0.07899033278226852,
      "learning_rate": 5.794667002842293e-05,
      "loss": 0.0912,
      "step": 25450
    },
    {
      "epoch": 0.9538795848787981,
      "grad_norm": 0.06972067058086395,
      "learning_rate": 5.791608797561001e-05,
      "loss": 0.0881,
      "step": 25460
    },
    {
      "epoch": 0.9542542430032596,
      "grad_norm": 0.0581338033080101,
      "learning_rate": 5.788550288490925e-05,
      "loss": 0.0892,
      "step": 25470
    },
    {
      "epoch": 0.9546289011277209,
      "grad_norm": 0.07030954211950302,
      "learning_rate": 5.7854914768057986e-05,
      "loss": 0.0944,
      "step": 25480
    },
    {
      "epoch": 0.9550035592521824,
      "grad_norm": 0.07540659606456757,
      "learning_rate": 5.7824323636794766e-05,
      "loss": 0.0897,
      "step": 25490
    },
    {
      "epoch": 0.9553782173766439,
      "grad_norm": 0.08364974707365036,
      "learning_rate": 5.7793729502859284e-05,
      "loss": 0.0913,
      "step": 25500
    },
    {
      "epoch": 0.9557528755011052,
      "grad_norm": 0.06329388916492462,
      "learning_rate": 5.776313237799239e-05,
      "loss": 0.091,
      "step": 25510
    },
    {
      "epoch": 0.9561275336255667,
      "grad_norm": 0.11500350385904312,
      "learning_rate": 5.773253227393607e-05,
      "loss": 0.0912,
      "step": 25520
    },
    {
      "epoch": 0.9565021917500282,
      "grad_norm": 0.0790955126285553,
      "learning_rate": 5.7701929202433444e-05,
      "loss": 0.0918,
      "step": 25530
    },
    {
      "epoch": 0.9568768498744895,
      "grad_norm": 0.058575280010700226,
      "learning_rate": 5.767132317522882e-05,
      "loss": 0.0882,
      "step": 25540
    },
    {
      "epoch": 0.957251507998951,
      "grad_norm": 0.0923948660492897,
      "learning_rate": 5.764071420406757e-05,
      "loss": 0.0882,
      "step": 25550
    },
    {
      "epoch": 0.9576261661234123,
      "grad_norm": 0.08580265194177628,
      "learning_rate": 5.7610102300696233e-05,
      "loss": 0.0924,
      "step": 25560
    },
    {
      "epoch": 0.9580008242478738,
      "grad_norm": 0.08247824758291245,
      "learning_rate": 5.757948747686249e-05,
      "loss": 0.0924,
      "step": 25570
    },
    {
      "epoch": 0.9583754823723353,
      "grad_norm": 0.06843249499797821,
      "learning_rate": 5.754886974431513e-05,
      "loss": 0.0886,
      "step": 25580
    },
    {
      "epoch": 0.9587501404967966,
      "grad_norm": 0.07340391725301743,
      "learning_rate": 5.751824911480402e-05,
      "loss": 0.09,
      "step": 25590
    },
    {
      "epoch": 0.9591247986212581,
      "grad_norm": 0.17087934911251068,
      "learning_rate": 5.748762560008021e-05,
      "loss": 0.09,
      "step": 25600
    },
    {
      "epoch": 0.9594994567457196,
      "grad_norm": 0.06983765214681625,
      "learning_rate": 5.745699921189579e-05,
      "loss": 0.0891,
      "step": 25610
    },
    {
      "epoch": 0.9598741148701809,
      "grad_norm": 0.07742927223443985,
      "learning_rate": 5.742636996200399e-05,
      "loss": 0.0902,
      "step": 25620
    },
    {
      "epoch": 0.9602487729946424,
      "grad_norm": 0.10474294424057007,
      "learning_rate": 5.7395737862159124e-05,
      "loss": 0.0919,
      "step": 25630
    },
    {
      "epoch": 0.9606234311191039,
      "grad_norm": 0.08028475195169449,
      "learning_rate": 5.736510292411662e-05,
      "loss": 0.0892,
      "step": 25640
    },
    {
      "epoch": 0.9609980892435652,
      "grad_norm": 0.06693142652511597,
      "learning_rate": 5.733446515963296e-05,
      "loss": 0.092,
      "step": 25650
    },
    {
      "epoch": 0.9613727473680267,
      "grad_norm": 0.07087256014347076,
      "learning_rate": 5.730382458046575e-05,
      "loss": 0.0907,
      "step": 25660
    },
    {
      "epoch": 0.961747405492488,
      "grad_norm": 0.0890178307890892,
      "learning_rate": 5.727318119837366e-05,
      "loss": 0.0886,
      "step": 25670
    },
    {
      "epoch": 0.9621220636169495,
      "grad_norm": 0.06310903280973434,
      "learning_rate": 5.7242535025116406e-05,
      "loss": 0.0927,
      "step": 25680
    },
    {
      "epoch": 0.962496721741411,
      "grad_norm": 0.09658374637365341,
      "learning_rate": 5.721188607245482e-05,
      "loss": 0.0901,
      "step": 25690
    },
    {
      "epoch": 0.9628713798658723,
      "grad_norm": 0.0858880877494812,
      "learning_rate": 5.7181234352150784e-05,
      "loss": 0.0907,
      "step": 25700
    },
    {
      "epoch": 0.9632460379903338,
      "grad_norm": 0.08177480101585388,
      "learning_rate": 5.715057987596724e-05,
      "loss": 0.0876,
      "step": 25710
    },
    {
      "epoch": 0.9636206961147953,
      "grad_norm": 0.08486223965883255,
      "learning_rate": 5.711992265566819e-05,
      "loss": 0.0937,
      "step": 25720
    },
    {
      "epoch": 0.9639953542392566,
      "grad_norm": 0.0794711783528328,
      "learning_rate": 5.7089262703018656e-05,
      "loss": 0.0919,
      "step": 25730
    },
    {
      "epoch": 0.9643700123637181,
      "grad_norm": 0.07639691233634949,
      "learning_rate": 5.705860002978479e-05,
      "loss": 0.0936,
      "step": 25740
    },
    {
      "epoch": 0.9647446704881796,
      "grad_norm": 0.07158742845058441,
      "learning_rate": 5.702793464773371e-05,
      "loss": 0.0908,
      "step": 25750
    },
    {
      "epoch": 0.9651193286126409,
      "grad_norm": 0.06493354588747025,
      "learning_rate": 5.699726656863361e-05,
      "loss": 0.0884,
      "step": 25760
    },
    {
      "epoch": 0.9654939867371024,
      "grad_norm": 0.07598762959241867,
      "learning_rate": 5.6966595804253695e-05,
      "loss": 0.0899,
      "step": 25770
    },
    {
      "epoch": 0.9658686448615639,
      "grad_norm": 0.11610203236341476,
      "learning_rate": 5.693592236636424e-05,
      "loss": 0.0881,
      "step": 25780
    },
    {
      "epoch": 0.9662433029860252,
      "grad_norm": 0.06789844483137131,
      "learning_rate": 5.6905246266736514e-05,
      "loss": 0.0914,
      "step": 25790
    },
    {
      "epoch": 0.9666179611104867,
      "grad_norm": 0.07849140465259552,
      "learning_rate": 5.687456751714278e-05,
      "loss": 0.0872,
      "step": 25800
    },
    {
      "epoch": 0.9669926192349481,
      "grad_norm": 0.07414271682500839,
      "learning_rate": 5.684388612935643e-05,
      "loss": 0.089,
      "step": 25810
    },
    {
      "epoch": 0.9673672773594095,
      "grad_norm": 0.06695695966482162,
      "learning_rate": 5.681320211515171e-05,
      "loss": 0.0895,
      "step": 25820
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 0.06374902278184891,
      "learning_rate": 5.678251548630401e-05,
      "loss": 0.0856,
      "step": 25830
    },
    {
      "epoch": 0.9681165936083324,
      "grad_norm": 0.07707621902227402,
      "learning_rate": 5.6751826254589656e-05,
      "loss": 0.0882,
      "step": 25840
    },
    {
      "epoch": 0.9684912517327938,
      "grad_norm": 0.08483823388814926,
      "learning_rate": 5.672113443178596e-05,
      "loss": 0.0927,
      "step": 25850
    },
    {
      "epoch": 0.9688659098572553,
      "grad_norm": 0.0975584089756012,
      "learning_rate": 5.66904400296713e-05,
      "loss": 0.0891,
      "step": 25860
    },
    {
      "epoch": 0.9692405679817166,
      "grad_norm": 0.0747906044125557,
      "learning_rate": 5.665974306002495e-05,
      "loss": 0.0886,
      "step": 25870
    },
    {
      "epoch": 0.9696152261061781,
      "grad_norm": 0.07471103966236115,
      "learning_rate": 5.662904353462727e-05,
      "loss": 0.0882,
      "step": 25880
    },
    {
      "epoch": 0.9699898842306396,
      "grad_norm": 0.062085479497909546,
      "learning_rate": 5.65983414652595e-05,
      "loss": 0.0837,
      "step": 25890
    },
    {
      "epoch": 0.970364542355101,
      "grad_norm": 0.06092758849263191,
      "learning_rate": 5.656763686370393e-05,
      "loss": 0.0877,
      "step": 25900
    },
    {
      "epoch": 0.9707392004795624,
      "grad_norm": 0.10077887773513794,
      "learning_rate": 5.653692974174381e-05,
      "loss": 0.0912,
      "step": 25910
    },
    {
      "epoch": 0.9711138586040239,
      "grad_norm": 0.09680423885583878,
      "learning_rate": 5.650622011116329e-05,
      "loss": 0.092,
      "step": 25920
    },
    {
      "epoch": 0.9714885167284852,
      "grad_norm": 0.07499566674232483,
      "learning_rate": 5.64755079837476e-05,
      "loss": 0.0893,
      "step": 25930
    },
    {
      "epoch": 0.9718631748529467,
      "grad_norm": 0.06572767347097397,
      "learning_rate": 5.6444793371282824e-05,
      "loss": 0.0894,
      "step": 25940
    },
    {
      "epoch": 0.9722378329774081,
      "grad_norm": 0.08786819130182266,
      "learning_rate": 5.641407628555605e-05,
      "loss": 0.0927,
      "step": 25950
    },
    {
      "epoch": 0.9726124911018695,
      "grad_norm": 0.08075881749391556,
      "learning_rate": 5.6383356738355306e-05,
      "loss": 0.0941,
      "step": 25960
    },
    {
      "epoch": 0.972987149226331,
      "grad_norm": 0.10688720643520355,
      "learning_rate": 5.635263474146958e-05,
      "loss": 0.0908,
      "step": 25970
    },
    {
      "epoch": 0.9733618073507924,
      "grad_norm": 0.0819649025797844,
      "learning_rate": 5.632191030668878e-05,
      "loss": 0.0923,
      "step": 25980
    },
    {
      "epoch": 0.9737364654752538,
      "grad_norm": 0.0531957633793354,
      "learning_rate": 5.629118344580371e-05,
      "loss": 0.0877,
      "step": 25990
    },
    {
      "epoch": 0.9741111235997153,
      "grad_norm": 0.0704246386885643,
      "learning_rate": 5.62604541706062e-05,
      "loss": 0.0884,
      "step": 26000
    },
    {
      "epoch": 0.9744857817241767,
      "grad_norm": 0.05966324731707573,
      "learning_rate": 5.6229722492888934e-05,
      "loss": 0.0881,
      "step": 26010
    },
    {
      "epoch": 0.9748604398486381,
      "grad_norm": 0.12345746904611588,
      "learning_rate": 5.619898842444554e-05,
      "loss": 0.0895,
      "step": 26020
    },
    {
      "epoch": 0.9752350979730996,
      "grad_norm": 0.0855979323387146,
      "learning_rate": 5.616825197707057e-05,
      "loss": 0.0907,
      "step": 26030
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.07099080830812454,
      "learning_rate": 5.6137513162559485e-05,
      "loss": 0.0906,
      "step": 26040
    },
    {
      "epoch": 0.9759844142220224,
      "grad_norm": 0.24212472140789032,
      "learning_rate": 5.610677199270864e-05,
      "loss": 0.0944,
      "step": 26050
    },
    {
      "epoch": 0.9763590723464838,
      "grad_norm": 0.07428694516420364,
      "learning_rate": 5.607602847931529e-05,
      "loss": 0.0906,
      "step": 26060
    },
    {
      "epoch": 0.9767337304709452,
      "grad_norm": 0.07133444398641586,
      "learning_rate": 5.6045282634177655e-05,
      "loss": 0.0931,
      "step": 26070
    },
    {
      "epoch": 0.9771083885954067,
      "grad_norm": 0.07437096536159515,
      "learning_rate": 5.601453446909475e-05,
      "loss": 0.0917,
      "step": 26080
    },
    {
      "epoch": 0.9774830467198681,
      "grad_norm": 0.09477550536394119,
      "learning_rate": 5.5983783995866534e-05,
      "loss": 0.0891,
      "step": 26090
    },
    {
      "epoch": 0.9778577048443295,
      "grad_norm": 0.07009521126747131,
      "learning_rate": 5.5953031226293874e-05,
      "loss": 0.0906,
      "step": 26100
    },
    {
      "epoch": 0.978232362968791,
      "grad_norm": 0.06579546630382538,
      "learning_rate": 5.592227617217847e-05,
      "loss": 0.0912,
      "step": 26110
    },
    {
      "epoch": 0.9786070210932524,
      "grad_norm": 0.06584831327199936,
      "learning_rate": 5.589151884532293e-05,
      "loss": 0.0899,
      "step": 26120
    },
    {
      "epoch": 0.9789816792177138,
      "grad_norm": 0.05862561613321304,
      "learning_rate": 5.5860759257530725e-05,
      "loss": 0.0885,
      "step": 26130
    },
    {
      "epoch": 0.9793563373421753,
      "grad_norm": 0.07482010126113892,
      "learning_rate": 5.582999742060618e-05,
      "loss": 0.0903,
      "step": 26140
    },
    {
      "epoch": 0.9797309954666367,
      "grad_norm": 0.06595966219902039,
      "learning_rate": 5.5799233346354506e-05,
      "loss": 0.0957,
      "step": 26150
    },
    {
      "epoch": 0.9801056535910981,
      "grad_norm": 0.07092498242855072,
      "learning_rate": 5.5768467046581773e-05,
      "loss": 0.0894,
      "step": 26160
    },
    {
      "epoch": 0.9804803117155596,
      "grad_norm": 0.07186070829629898,
      "learning_rate": 5.573769853309487e-05,
      "loss": 0.0882,
      "step": 26170
    },
    {
      "epoch": 0.980854969840021,
      "grad_norm": 0.06713789701461792,
      "learning_rate": 5.570692781770158e-05,
      "loss": 0.0935,
      "step": 26180
    },
    {
      "epoch": 0.9812296279644824,
      "grad_norm": 0.08680858463048935,
      "learning_rate": 5.567615491221052e-05,
      "loss": 0.0927,
      "step": 26190
    },
    {
      "epoch": 0.9816042860889438,
      "grad_norm": 0.0911741778254509,
      "learning_rate": 5.564537982843111e-05,
      "loss": 0.0895,
      "step": 26200
    },
    {
      "epoch": 0.9819789442134053,
      "grad_norm": 0.07803058624267578,
      "learning_rate": 5.5614602578173644e-05,
      "loss": 0.0911,
      "step": 26210
    },
    {
      "epoch": 0.9823536023378667,
      "grad_norm": 0.07487785816192627,
      "learning_rate": 5.558382317324925e-05,
      "loss": 0.0875,
      "step": 26220
    },
    {
      "epoch": 0.9827282604623281,
      "grad_norm": 0.08173295855522156,
      "learning_rate": 5.555304162546987e-05,
      "loss": 0.0878,
      "step": 26230
    },
    {
      "epoch": 0.9831029185867896,
      "grad_norm": 0.10621875524520874,
      "learning_rate": 5.552225794664826e-05,
      "loss": 0.09,
      "step": 26240
    },
    {
      "epoch": 0.983477576711251,
      "grad_norm": 0.09225030243396759,
      "learning_rate": 5.549147214859801e-05,
      "loss": 0.0874,
      "step": 26250
    },
    {
      "epoch": 0.9838522348357124,
      "grad_norm": 0.09937134385108948,
      "learning_rate": 5.546068424313351e-05,
      "loss": 0.0918,
      "step": 26260
    },
    {
      "epoch": 0.9842268929601738,
      "grad_norm": 0.08406046032905579,
      "learning_rate": 5.542989424206997e-05,
      "loss": 0.0869,
      "step": 26270
    },
    {
      "epoch": 0.9846015510846353,
      "grad_norm": 0.08762650936841965,
      "learning_rate": 5.539910215722339e-05,
      "loss": 0.0901,
      "step": 26280
    },
    {
      "epoch": 0.9849762092090967,
      "grad_norm": 0.07861081510782242,
      "learning_rate": 5.53683080004106e-05,
      "loss": 0.0934,
      "step": 26290
    },
    {
      "epoch": 0.9853508673335581,
      "grad_norm": 0.07345648109912872,
      "learning_rate": 5.533751178344918e-05,
      "loss": 0.092,
      "step": 26300
    },
    {
      "epoch": 0.9857255254580195,
      "grad_norm": 0.07638391852378845,
      "learning_rate": 5.530671351815755e-05,
      "loss": 0.0899,
      "step": 26310
    },
    {
      "epoch": 0.986100183582481,
      "grad_norm": 0.07106225937604904,
      "learning_rate": 5.527591321635488e-05,
      "loss": 0.0901,
      "step": 26320
    },
    {
      "epoch": 0.9864748417069424,
      "grad_norm": 0.0806843712925911,
      "learning_rate": 5.524511088986112e-05,
      "loss": 0.0875,
      "step": 26330
    },
    {
      "epoch": 0.9868494998314038,
      "grad_norm": 0.06354599446058273,
      "learning_rate": 5.521430655049702e-05,
      "loss": 0.0952,
      "step": 26340
    },
    {
      "epoch": 0.9872241579558653,
      "grad_norm": 0.06751538813114166,
      "learning_rate": 5.518350021008409e-05,
      "loss": 0.0854,
      "step": 26350
    },
    {
      "epoch": 0.9875988160803267,
      "grad_norm": 0.06583192199468613,
      "learning_rate": 5.5152691880444616e-05,
      "loss": 0.093,
      "step": 26360
    },
    {
      "epoch": 0.9879734742047881,
      "grad_norm": 0.08112289011478424,
      "learning_rate": 5.5121881573401643e-05,
      "loss": 0.0898,
      "step": 26370
    },
    {
      "epoch": 0.9883481323292496,
      "grad_norm": 0.07910194247961044,
      "learning_rate": 5.5091069300778954e-05,
      "loss": 0.0929,
      "step": 26380
    },
    {
      "epoch": 0.988722790453711,
      "grad_norm": 0.06943710893392563,
      "learning_rate": 5.5060255074401133e-05,
      "loss": 0.0877,
      "step": 26390
    },
    {
      "epoch": 0.9890974485781724,
      "grad_norm": 0.10916765034198761,
      "learning_rate": 5.502943890609347e-05,
      "loss": 0.0867,
      "step": 26400
    },
    {
      "epoch": 0.9894721067026339,
      "grad_norm": 0.07355522364377975,
      "learning_rate": 5.4998620807682e-05,
      "loss": 0.0894,
      "step": 26410
    },
    {
      "epoch": 0.9898467648270953,
      "grad_norm": 0.09420657157897949,
      "learning_rate": 5.496780079099355e-05,
      "loss": 0.0907,
      "step": 26420
    },
    {
      "epoch": 0.9902214229515567,
      "grad_norm": 0.08635549247264862,
      "learning_rate": 5.4936978867855625e-05,
      "loss": 0.0908,
      "step": 26430
    },
    {
      "epoch": 0.9905960810760182,
      "grad_norm": 0.07176639884710312,
      "learning_rate": 5.49061550500965e-05,
      "loss": 0.0927,
      "step": 26440
    },
    {
      "epoch": 0.9909707392004795,
      "grad_norm": 0.07506656646728516,
      "learning_rate": 5.487532934954513e-05,
      "loss": 0.0908,
      "step": 26450
    },
    {
      "epoch": 0.991345397324941,
      "grad_norm": 0.06997134536504745,
      "learning_rate": 5.4844501778031274e-05,
      "loss": 0.0897,
      "step": 26460
    },
    {
      "epoch": 0.9917200554494024,
      "grad_norm": 0.07737968116998672,
      "learning_rate": 5.4813672347385325e-05,
      "loss": 0.0943,
      "step": 26470
    },
    {
      "epoch": 0.9920947135738638,
      "grad_norm": 0.07722847908735275,
      "learning_rate": 5.478284106943843e-05,
      "loss": 0.0905,
      "step": 26480
    },
    {
      "epoch": 0.9924693716983253,
      "grad_norm": 0.08318343013525009,
      "learning_rate": 5.4752007956022445e-05,
      "loss": 0.0904,
      "step": 26490
    },
    {
      "epoch": 0.9928440298227867,
      "grad_norm": 0.06408271938562393,
      "learning_rate": 5.4721173018969906e-05,
      "loss": 0.0893,
      "step": 26500
    },
    {
      "epoch": 0.9932186879472481,
      "grad_norm": 0.060929298400878906,
      "learning_rate": 5.46903362701141e-05,
      "loss": 0.0873,
      "step": 26510
    },
    {
      "epoch": 0.9935933460717096,
      "grad_norm": 0.0692480206489563,
      "learning_rate": 5.465949772128894e-05,
      "loss": 0.0929,
      "step": 26520
    },
    {
      "epoch": 0.993968004196171,
      "grad_norm": 0.09425660967826843,
      "learning_rate": 5.462865738432911e-05,
      "loss": 0.093,
      "step": 26530
    },
    {
      "epoch": 0.9943426623206324,
      "grad_norm": 0.06684005260467529,
      "learning_rate": 5.45978152710699e-05,
      "loss": 0.0928,
      "step": 26540
    },
    {
      "epoch": 0.9947173204450939,
      "grad_norm": 0.06538473069667816,
      "learning_rate": 5.4566971393347324e-05,
      "loss": 0.0886,
      "step": 26550
    },
    {
      "epoch": 0.9950919785695553,
      "grad_norm": 0.10411527752876282,
      "learning_rate": 5.453612576299808e-05,
      "loss": 0.0947,
      "step": 26560
    },
    {
      "epoch": 0.9954666366940167,
      "grad_norm": 0.07061488926410675,
      "learning_rate": 5.4505278391859506e-05,
      "loss": 0.0894,
      "step": 26570
    },
    {
      "epoch": 0.9958412948184782,
      "grad_norm": 0.09024341404438019,
      "learning_rate": 5.4474429291769654e-05,
      "loss": 0.09,
      "step": 26580
    },
    {
      "epoch": 0.9962159529429395,
      "grad_norm": 0.09867697954177856,
      "learning_rate": 5.444357847456719e-05,
      "loss": 0.0923,
      "step": 26590
    },
    {
      "epoch": 0.996590611067401,
      "grad_norm": 0.12331444770097733,
      "learning_rate": 5.4412725952091484e-05,
      "loss": 0.0886,
      "step": 26600
    },
    {
      "epoch": 0.9969652691918625,
      "grad_norm": 0.07761655002832413,
      "learning_rate": 5.438187173618251e-05,
      "loss": 0.0914,
      "step": 26610
    },
    {
      "epoch": 0.9973399273163238,
      "grad_norm": 0.06503132730722427,
      "learning_rate": 5.435101583868094e-05,
      "loss": 0.0945,
      "step": 26620
    },
    {
      "epoch": 0.9977145854407853,
      "grad_norm": 0.06780931353569031,
      "learning_rate": 5.432015827142809e-05,
      "loss": 0.0884,
      "step": 26630
    },
    {
      "epoch": 0.9980892435652468,
      "grad_norm": 0.09661582857370377,
      "learning_rate": 5.428929904626584e-05,
      "loss": 0.0924,
      "step": 26640
    },
    {
      "epoch": 0.9984639016897081,
      "grad_norm": 0.11015281826257706,
      "learning_rate": 5.4258438175036844e-05,
      "loss": 0.0917,
      "step": 26650
    },
    {
      "epoch": 0.9988385598141696,
      "grad_norm": 0.0711621418595314,
      "learning_rate": 5.422757566958424e-05,
      "loss": 0.0939,
      "step": 26660
    },
    {
      "epoch": 0.999213217938631,
      "grad_norm": 0.12395332008600235,
      "learning_rate": 5.41967115417519e-05,
      "loss": 0.0922,
      "step": 26670
    },
    {
      "epoch": 0.9995878760630924,
      "grad_norm": 0.07608817517757416,
      "learning_rate": 5.416584580338426e-05,
      "loss": 0.0925,
      "step": 26680
    },
    {
      "epoch": 0.9999625341875539,
      "grad_norm": 0.08433928340673447,
      "learning_rate": 5.4134978466326416e-05,
      "loss": 0.0928,
      "step": 26690
    },
    {
      "epoch": 1.0003371923120152,
      "grad_norm": 0.14562147855758667,
      "learning_rate": 5.4104109542424044e-05,
      "loss": 0.0901,
      "step": 26700
    },
    {
      "epoch": 1.0007118504364767,
      "grad_norm": 0.0747261717915535,
      "learning_rate": 5.4073239043523435e-05,
      "loss": 0.0871,
      "step": 26710
    },
    {
      "epoch": 1.0010865085609382,
      "grad_norm": 0.07966229319572449,
      "learning_rate": 5.404236698147149e-05,
      "loss": 0.0884,
      "step": 26720
    },
    {
      "epoch": 1.0014611666853996,
      "grad_norm": 0.07391031086444855,
      "learning_rate": 5.401149336811573e-05,
      "loss": 0.0927,
      "step": 26730
    },
    {
      "epoch": 1.001835824809861,
      "grad_norm": 0.08560635894536972,
      "learning_rate": 5.3980618215304234e-05,
      "loss": 0.0942,
      "step": 26740
    },
    {
      "epoch": 1.0022104829343224,
      "grad_norm": 0.0699784979224205,
      "learning_rate": 5.394974153488569e-05,
      "loss": 0.0869,
      "step": 26750
    },
    {
      "epoch": 1.0025851410587838,
      "grad_norm": 0.0730871632695198,
      "learning_rate": 5.391886333870938e-05,
      "loss": 0.0919,
      "step": 26760
    },
    {
      "epoch": 1.0029597991832453,
      "grad_norm": 0.12796206772327423,
      "learning_rate": 5.388798363862517e-05,
      "loss": 0.0902,
      "step": 26770
    },
    {
      "epoch": 1.0033344573077068,
      "grad_norm": 0.0783357247710228,
      "learning_rate": 5.3857102446483455e-05,
      "loss": 0.0886,
      "step": 26780
    },
    {
      "epoch": 1.0037091154321682,
      "grad_norm": 0.08219611644744873,
      "learning_rate": 5.382621977413528e-05,
      "loss": 0.0929,
      "step": 26790
    },
    {
      "epoch": 1.0040837735566295,
      "grad_norm": 0.07578302174806595,
      "learning_rate": 5.379533563343219e-05,
      "loss": 0.092,
      "step": 26800
    },
    {
      "epoch": 1.004458431681091,
      "grad_norm": 0.09428597241640091,
      "learning_rate": 5.376445003622632e-05,
      "loss": 0.0971,
      "step": 26810
    },
    {
      "epoch": 1.0048330898055524,
      "grad_norm": 0.07215790450572968,
      "learning_rate": 5.3733562994370404e-05,
      "loss": 0.0905,
      "step": 26820
    },
    {
      "epoch": 1.0052077479300139,
      "grad_norm": 0.053001753985881805,
      "learning_rate": 5.370267451971766e-05,
      "loss": 0.0888,
      "step": 26830
    },
    {
      "epoch": 1.0055824060544754,
      "grad_norm": 0.06066783517599106,
      "learning_rate": 5.367178462412188e-05,
      "loss": 0.0926,
      "step": 26840
    },
    {
      "epoch": 1.0059570641789368,
      "grad_norm": 0.06418032199144363,
      "learning_rate": 5.3640893319437426e-05,
      "loss": 0.0873,
      "step": 26850
    },
    {
      "epoch": 1.006331722303398,
      "grad_norm": 0.07053912431001663,
      "learning_rate": 5.361000061751918e-05,
      "loss": 0.0923,
      "step": 26860
    },
    {
      "epoch": 1.0067063804278595,
      "grad_norm": 0.06727512925863266,
      "learning_rate": 5.357910653022258e-05,
      "loss": 0.0917,
      "step": 26870
    },
    {
      "epoch": 1.007081038552321,
      "grad_norm": 0.055687375366687775,
      "learning_rate": 5.354821106940354e-05,
      "loss": 0.0911,
      "step": 26880
    },
    {
      "epoch": 1.0074556966767825,
      "grad_norm": 0.06310471147298813,
      "learning_rate": 5.351731424691858e-05,
      "loss": 0.0912,
      "step": 26890
    },
    {
      "epoch": 1.007830354801244,
      "grad_norm": 0.10694124549627304,
      "learning_rate": 5.34864160746247e-05,
      "loss": 0.0884,
      "step": 26900
    },
    {
      "epoch": 1.0082050129257052,
      "grad_norm": 0.11670529097318649,
      "learning_rate": 5.3455516564379395e-05,
      "loss": 0.0911,
      "step": 26910
    },
    {
      "epoch": 1.0085796710501667,
      "grad_norm": 0.046123918145895004,
      "learning_rate": 5.342461572804072e-05,
      "loss": 0.0915,
      "step": 26920
    },
    {
      "epoch": 1.0089543291746281,
      "grad_norm": 0.08295959234237671,
      "learning_rate": 5.339371357746721e-05,
      "loss": 0.0904,
      "step": 26930
    },
    {
      "epoch": 1.0093289872990896,
      "grad_norm": 0.07400642335414886,
      "learning_rate": 5.336281012451791e-05,
      "loss": 0.0919,
      "step": 26940
    },
    {
      "epoch": 1.009703645423551,
      "grad_norm": 0.07758170366287231,
      "learning_rate": 5.333190538105239e-05,
      "loss": 0.0872,
      "step": 26950
    },
    {
      "epoch": 1.0100783035480125,
      "grad_norm": 0.08049286901950836,
      "learning_rate": 5.330099935893067e-05,
      "loss": 0.0898,
      "step": 26960
    },
    {
      "epoch": 1.0104529616724738,
      "grad_norm": 0.10126199573278427,
      "learning_rate": 5.32700920700133e-05,
      "loss": 0.0887,
      "step": 26970
    },
    {
      "epoch": 1.0108276197969353,
      "grad_norm": 0.0745682492852211,
      "learning_rate": 5.3239183526161285e-05,
      "loss": 0.0916,
      "step": 26980
    },
    {
      "epoch": 1.0112022779213967,
      "grad_norm": 0.0842355340719223,
      "learning_rate": 5.320827373923612e-05,
      "loss": 0.0892,
      "step": 26990
    },
    {
      "epoch": 1.0115769360458582,
      "grad_norm": 0.05146516114473343,
      "learning_rate": 5.317736272109981e-05,
      "loss": 0.088,
      "step": 27000
    },
    {
      "epoch": 1.0119515941703197,
      "grad_norm": 0.061786193400621414,
      "learning_rate": 5.3146450483614796e-05,
      "loss": 0.0892,
      "step": 27010
    },
    {
      "epoch": 1.012326252294781,
      "grad_norm": 0.2448648363351822,
      "learning_rate": 5.3115537038644006e-05,
      "loss": 0.0862,
      "step": 27020
    },
    {
      "epoch": 1.0127009104192424,
      "grad_norm": 0.09256260097026825,
      "learning_rate": 5.308462239805078e-05,
      "loss": 0.0885,
      "step": 27030
    },
    {
      "epoch": 1.0130755685437038,
      "grad_norm": 0.08474892377853394,
      "learning_rate": 5.305370657369901e-05,
      "loss": 0.0886,
      "step": 27040
    },
    {
      "epoch": 1.0134502266681653,
      "grad_norm": 0.08314520120620728,
      "learning_rate": 5.302278957745297e-05,
      "loss": 0.0914,
      "step": 27050
    },
    {
      "epoch": 1.0138248847926268,
      "grad_norm": 0.08236005902290344,
      "learning_rate": 5.29918714211774e-05,
      "loss": 0.0887,
      "step": 27060
    },
    {
      "epoch": 1.0141995429170882,
      "grad_norm": 0.06892554461956024,
      "learning_rate": 5.2960952116737503e-05,
      "loss": 0.0872,
      "step": 27070
    },
    {
      "epoch": 1.0145742010415495,
      "grad_norm": 0.08279775083065033,
      "learning_rate": 5.293003167599889e-05,
      "loss": 0.0905,
      "step": 27080
    },
    {
      "epoch": 1.014948859166011,
      "grad_norm": 0.06507188826799393,
      "learning_rate": 5.2899110110827665e-05,
      "loss": 0.0921,
      "step": 27090
    },
    {
      "epoch": 1.0153235172904724,
      "grad_norm": 0.08397114276885986,
      "learning_rate": 5.286818743309028e-05,
      "loss": 0.091,
      "step": 27100
    },
    {
      "epoch": 1.015698175414934,
      "grad_norm": 0.09051336348056793,
      "learning_rate": 5.2837263654653715e-05,
      "loss": 0.0909,
      "step": 27110
    },
    {
      "epoch": 1.0160728335393954,
      "grad_norm": 0.10293033719062805,
      "learning_rate": 5.280633878738526e-05,
      "loss": 0.0924,
      "step": 27120
    },
    {
      "epoch": 1.0164474916638568,
      "grad_norm": 0.08186677098274231,
      "learning_rate": 5.277541284315272e-05,
      "loss": 0.086,
      "step": 27130
    },
    {
      "epoch": 1.016822149788318,
      "grad_norm": 0.09796065837144852,
      "learning_rate": 5.274448583382426e-05,
      "loss": 0.091,
      "step": 27140
    },
    {
      "epoch": 1.0171968079127796,
      "grad_norm": 0.07860983908176422,
      "learning_rate": 5.271355777126847e-05,
      "loss": 0.0927,
      "step": 27150
    },
    {
      "epoch": 1.017571466037241,
      "grad_norm": 0.07447504252195358,
      "learning_rate": 5.268262866735435e-05,
      "loss": 0.0929,
      "step": 27160
    },
    {
      "epoch": 1.0179461241617025,
      "grad_norm": 0.07958918064832687,
      "learning_rate": 5.2651698533951285e-05,
      "loss": 0.0908,
      "step": 27170
    },
    {
      "epoch": 1.018320782286164,
      "grad_norm": 0.06771871447563171,
      "learning_rate": 5.2620767382929046e-05,
      "loss": 0.091,
      "step": 27180
    },
    {
      "epoch": 1.0186954404106252,
      "grad_norm": 0.09777320176362991,
      "learning_rate": 5.2589835226157856e-05,
      "loss": 0.0924,
      "step": 27190
    },
    {
      "epoch": 1.0190700985350867,
      "grad_norm": 0.09624200314283371,
      "learning_rate": 5.255890207550821e-05,
      "loss": 0.0925,
      "step": 27200
    },
    {
      "epoch": 1.0194447566595481,
      "grad_norm": 0.08270849287509918,
      "learning_rate": 5.252796794285112e-05,
      "loss": 0.0907,
      "step": 27210
    },
    {
      "epoch": 1.0198194147840096,
      "grad_norm": 0.07064302265644073,
      "learning_rate": 5.249703284005786e-05,
      "loss": 0.0908,
      "step": 27220
    },
    {
      "epoch": 1.020194072908471,
      "grad_norm": 0.07378499954938889,
      "learning_rate": 5.2466096779000153e-05,
      "loss": 0.0904,
      "step": 27230
    },
    {
      "epoch": 1.0205687310329326,
      "grad_norm": 0.07140862941741943,
      "learning_rate": 5.2435159771550043e-05,
      "loss": 0.0933,
      "step": 27240
    },
    {
      "epoch": 1.0209433891573938,
      "grad_norm": 0.08429422974586487,
      "learning_rate": 5.240422182957997e-05,
      "loss": 0.0894,
      "step": 27250
    },
    {
      "epoch": 1.0213180472818553,
      "grad_norm": 0.09675873070955276,
      "learning_rate": 5.23732829649627e-05,
      "loss": 0.0906,
      "step": 27260
    },
    {
      "epoch": 1.0216927054063167,
      "grad_norm": 0.08951110392808914,
      "learning_rate": 5.234234318957139e-05,
      "loss": 0.0895,
      "step": 27270
    },
    {
      "epoch": 1.0220673635307782,
      "grad_norm": 0.07214164733886719,
      "learning_rate": 5.231140251527952e-05,
      "loss": 0.0882,
      "step": 27280
    },
    {
      "epoch": 1.0224420216552397,
      "grad_norm": 0.07677841186523438,
      "learning_rate": 5.228046095396091e-05,
      "loss": 0.0878,
      "step": 27290
    },
    {
      "epoch": 1.022816679779701,
      "grad_norm": 0.07385651022195816,
      "learning_rate": 5.2249518517489735e-05,
      "loss": 0.0873,
      "step": 27300
    },
    {
      "epoch": 1.0231913379041624,
      "grad_norm": 0.10348641127347946,
      "learning_rate": 5.221857521774052e-05,
      "loss": 0.091,
      "step": 27310
    },
    {
      "epoch": 1.0235659960286239,
      "grad_norm": 0.07104673236608505,
      "learning_rate": 5.2187631066588085e-05,
      "loss": 0.0874,
      "step": 27320
    },
    {
      "epoch": 1.0239406541530853,
      "grad_norm": 0.09738775342702866,
      "learning_rate": 5.215668607590761e-05,
      "loss": 0.0924,
      "step": 27330
    },
    {
      "epoch": 1.0243153122775468,
      "grad_norm": 0.08788815885782242,
      "learning_rate": 5.212574025757457e-05,
      "loss": 0.0904,
      "step": 27340
    },
    {
      "epoch": 1.0246899704020083,
      "grad_norm": 0.06110265851020813,
      "learning_rate": 5.2094793623464797e-05,
      "loss": 0.0904,
      "step": 27350
    },
    {
      "epoch": 1.0250646285264695,
      "grad_norm": 0.08960425108671188,
      "learning_rate": 5.206384618545439e-05,
      "loss": 0.0917,
      "step": 27360
    },
    {
      "epoch": 1.025439286650931,
      "grad_norm": 0.08455147594213486,
      "learning_rate": 5.203289795541978e-05,
      "loss": 0.0929,
      "step": 27370
    },
    {
      "epoch": 1.0258139447753925,
      "grad_norm": 0.07643304020166397,
      "learning_rate": 5.200194894523769e-05,
      "loss": 0.0905,
      "step": 27380
    },
    {
      "epoch": 1.026188602899854,
      "grad_norm": 0.09670693427324295,
      "learning_rate": 5.197099916678516e-05,
      "loss": 0.0884,
      "step": 27390
    },
    {
      "epoch": 1.0265632610243154,
      "grad_norm": 0.06436394900083542,
      "learning_rate": 5.194004863193951e-05,
      "loss": 0.0921,
      "step": 27400
    },
    {
      "epoch": 1.0269379191487766,
      "grad_norm": 0.08362887054681778,
      "learning_rate": 5.1909097352578385e-05,
      "loss": 0.0887,
      "step": 27410
    },
    {
      "epoch": 1.027312577273238,
      "grad_norm": 0.07561927288770676,
      "learning_rate": 5.187814534057964e-05,
      "loss": 0.0927,
      "step": 27420
    },
    {
      "epoch": 1.0276872353976996,
      "grad_norm": 0.05953230708837509,
      "learning_rate": 5.184719260782149e-05,
      "loss": 0.0898,
      "step": 27430
    },
    {
      "epoch": 1.028061893522161,
      "grad_norm": 0.06859705597162247,
      "learning_rate": 5.1816239166182376e-05,
      "loss": 0.0944,
      "step": 27440
    },
    {
      "epoch": 1.0284365516466225,
      "grad_norm": 0.0735471099615097,
      "learning_rate": 5.178528502754105e-05,
      "loss": 0.092,
      "step": 27450
    },
    {
      "epoch": 1.028811209771084,
      "grad_norm": 0.07356777042150497,
      "learning_rate": 5.1754330203776505e-05,
      "loss": 0.0898,
      "step": 27460
    },
    {
      "epoch": 1.0291858678955452,
      "grad_norm": 0.06382562965154648,
      "learning_rate": 5.172337470676798e-05,
      "loss": 0.0892,
      "step": 27470
    },
    {
      "epoch": 1.0295605260200067,
      "grad_norm": 0.0551476888358593,
      "learning_rate": 5.169241854839504e-05,
      "loss": 0.0895,
      "step": 27480
    },
    {
      "epoch": 1.0299351841444682,
      "grad_norm": 0.08486498147249222,
      "learning_rate": 5.166146174053742e-05,
      "loss": 0.0924,
      "step": 27490
    },
    {
      "epoch": 1.0303098422689296,
      "grad_norm": 0.08578580617904663,
      "learning_rate": 5.163050429507515e-05,
      "loss": 0.0902,
      "step": 27500
    },
    {
      "epoch": 1.030684500393391,
      "grad_norm": 0.07203719019889832,
      "learning_rate": 5.1599546223888506e-05,
      "loss": 0.0888,
      "step": 27510
    },
    {
      "epoch": 1.0310591585178526,
      "grad_norm": 0.0721518024802208,
      "learning_rate": 5.1568587538858004e-05,
      "loss": 0.0885,
      "step": 27520
    },
    {
      "epoch": 1.0314338166423138,
      "grad_norm": 0.0722155049443245,
      "learning_rate": 5.1537628251864375e-05,
      "loss": 0.0947,
      "step": 27530
    },
    {
      "epoch": 1.0318084747667753,
      "grad_norm": 0.07345712184906006,
      "learning_rate": 5.150666837478859e-05,
      "loss": 0.0942,
      "step": 27540
    },
    {
      "epoch": 1.0321831328912368,
      "grad_norm": 0.08231373876333237,
      "learning_rate": 5.1475707919511874e-05,
      "loss": 0.0905,
      "step": 27550
    },
    {
      "epoch": 1.0325577910156982,
      "grad_norm": 0.07455619424581528,
      "learning_rate": 5.144474689791562e-05,
      "loss": 0.0898,
      "step": 27560
    },
    {
      "epoch": 1.0329324491401597,
      "grad_norm": 0.060632627457380295,
      "learning_rate": 5.141378532188148e-05,
      "loss": 0.0934,
      "step": 27570
    },
    {
      "epoch": 1.033307107264621,
      "grad_norm": 0.07342168688774109,
      "learning_rate": 5.138282320329132e-05,
      "loss": 0.0896,
      "step": 27580
    },
    {
      "epoch": 1.0336817653890824,
      "grad_norm": 0.08362224698066711,
      "learning_rate": 5.1351860554027184e-05,
      "loss": 0.0938,
      "step": 27590
    },
    {
      "epoch": 1.0340564235135439,
      "grad_norm": 0.08788353949785233,
      "learning_rate": 5.1320897385971356e-05,
      "loss": 0.0898,
      "step": 27600
    },
    {
      "epoch": 1.0344310816380053,
      "grad_norm": 0.08148112148046494,
      "learning_rate": 5.128993371100627e-05,
      "loss": 0.089,
      "step": 27610
    },
    {
      "epoch": 1.0348057397624668,
      "grad_norm": 0.08645670861005783,
      "learning_rate": 5.125896954101462e-05,
      "loss": 0.0881,
      "step": 27620
    },
    {
      "epoch": 1.0351803978869283,
      "grad_norm": 0.07073058933019638,
      "learning_rate": 5.122800488787923e-05,
      "loss": 0.0899,
      "step": 27630
    },
    {
      "epoch": 1.0355550560113895,
      "grad_norm": 0.08645249903202057,
      "learning_rate": 5.119703976348315e-05,
      "loss": 0.0916,
      "step": 27640
    },
    {
      "epoch": 1.035929714135851,
      "grad_norm": 0.1014043316245079,
      "learning_rate": 5.116607417970958e-05,
      "loss": 0.0889,
      "step": 27650
    },
    {
      "epoch": 1.0363043722603125,
      "grad_norm": 0.09330973774194717,
      "learning_rate": 5.113510814844192e-05,
      "loss": 0.0934,
      "step": 27660
    },
    {
      "epoch": 1.036679030384774,
      "grad_norm": 0.06984337419271469,
      "learning_rate": 5.110414168156374e-05,
      "loss": 0.0885,
      "step": 27670
    },
    {
      "epoch": 1.0370536885092354,
      "grad_norm": 0.07428891211748123,
      "learning_rate": 5.107317479095874e-05,
      "loss": 0.0927,
      "step": 27680
    },
    {
      "epoch": 1.0374283466336967,
      "grad_norm": 0.10947689414024353,
      "learning_rate": 5.104220748851086e-05,
      "loss": 0.0866,
      "step": 27690
    },
    {
      "epoch": 1.0378030047581581,
      "grad_norm": 0.073747918009758,
      "learning_rate": 5.101123978610412e-05,
      "loss": 0.0912,
      "step": 27700
    },
    {
      "epoch": 1.0381776628826196,
      "grad_norm": 0.0799279659986496,
      "learning_rate": 5.098027169562273e-05,
      "loss": 0.0857,
      "step": 27710
    },
    {
      "epoch": 1.038552321007081,
      "grad_norm": 0.0892782136797905,
      "learning_rate": 5.094930322895105e-05,
      "loss": 0.0918,
      "step": 27720
    },
    {
      "epoch": 1.0389269791315425,
      "grad_norm": 0.06725344806909561,
      "learning_rate": 5.091833439797354e-05,
      "loss": 0.0868,
      "step": 27730
    },
    {
      "epoch": 1.039301637256004,
      "grad_norm": 0.08236956596374512,
      "learning_rate": 5.08873652145749e-05,
      "loss": 0.0946,
      "step": 27740
    },
    {
      "epoch": 1.0396762953804652,
      "grad_norm": 0.07704368233680725,
      "learning_rate": 5.085639569063985e-05,
      "loss": 0.0941,
      "step": 27750
    },
    {
      "epoch": 1.0400509535049267,
      "grad_norm": 0.09187209606170654,
      "learning_rate": 5.082542583805333e-05,
      "loss": 0.0929,
      "step": 27760
    },
    {
      "epoch": 1.0404256116293882,
      "grad_norm": 0.07551894336938858,
      "learning_rate": 5.079445566870036e-05,
      "loss": 0.0916,
      "step": 27770
    },
    {
      "epoch": 1.0408002697538496,
      "grad_norm": 0.07381941378116608,
      "learning_rate": 5.076348519446607e-05,
      "loss": 0.089,
      "step": 27780
    },
    {
      "epoch": 1.0411749278783111,
      "grad_norm": 0.09012657403945923,
      "learning_rate": 5.0732514427235755e-05,
      "loss": 0.0916,
      "step": 27790
    },
    {
      "epoch": 1.0415495860027724,
      "grad_norm": 0.10341894626617432,
      "learning_rate": 5.0701543378894764e-05,
      "loss": 0.095,
      "step": 27800
    },
    {
      "epoch": 1.0419242441272338,
      "grad_norm": 0.16480788588523865,
      "learning_rate": 5.067057206132863e-05,
      "loss": 0.0904,
      "step": 27810
    },
    {
      "epoch": 1.0422989022516953,
      "grad_norm": 0.06924169510602951,
      "learning_rate": 5.06396004864229e-05,
      "loss": 0.0906,
      "step": 27820
    },
    {
      "epoch": 1.0426735603761568,
      "grad_norm": 0.06571707129478455,
      "learning_rate": 5.060862866606329e-05,
      "loss": 0.0898,
      "step": 27830
    },
    {
      "epoch": 1.0430482185006182,
      "grad_norm": 0.0588434636592865,
      "learning_rate": 5.0577656612135595e-05,
      "loss": 0.0888,
      "step": 27840
    },
    {
      "epoch": 1.0434228766250797,
      "grad_norm": 0.07836581021547318,
      "learning_rate": 5.054668433652564e-05,
      "loss": 0.0878,
      "step": 27850
    },
    {
      "epoch": 1.043797534749541,
      "grad_norm": 0.2244110405445099,
      "learning_rate": 5.051571185111945e-05,
      "loss": 0.0901,
      "step": 27860
    },
    {
      "epoch": 1.0441721928740024,
      "grad_norm": 0.07237128913402557,
      "learning_rate": 5.048473916780301e-05,
      "loss": 0.0881,
      "step": 27870
    },
    {
      "epoch": 1.044546850998464,
      "grad_norm": 0.07523278146982193,
      "learning_rate": 5.045376629846247e-05,
      "loss": 0.0891,
      "step": 27880
    },
    {
      "epoch": 1.0449215091229254,
      "grad_norm": 0.07999888062477112,
      "learning_rate": 5.0422793254984004e-05,
      "loss": 0.0868,
      "step": 27890
    },
    {
      "epoch": 1.0452961672473868,
      "grad_norm": 0.0797014832496643,
      "learning_rate": 5.039182004925387e-05,
      "loss": 0.0875,
      "step": 27900
    },
    {
      "epoch": 1.045670825371848,
      "grad_norm": 0.0902366116642952,
      "learning_rate": 5.036084669315838e-05,
      "loss": 0.0889,
      "step": 27910
    },
    {
      "epoch": 1.0460454834963095,
      "grad_norm": 0.06498962640762329,
      "learning_rate": 5.03298731985839e-05,
      "loss": 0.0837,
      "step": 27920
    },
    {
      "epoch": 1.046420141620771,
      "grad_norm": 0.08511849492788315,
      "learning_rate": 5.029889957741688e-05,
      "loss": 0.0909,
      "step": 27930
    },
    {
      "epoch": 1.0467947997452325,
      "grad_norm": 0.08032316714525223,
      "learning_rate": 5.026792584154378e-05,
      "loss": 0.0912,
      "step": 27940
    },
    {
      "epoch": 1.047169457869694,
      "grad_norm": 0.08549738675355911,
      "learning_rate": 5.023695200285112e-05,
      "loss": 0.088,
      "step": 27950
    },
    {
      "epoch": 1.0475441159941554,
      "grad_norm": 0.088245689868927,
      "learning_rate": 5.0205978073225456e-05,
      "loss": 0.0902,
      "step": 27960
    },
    {
      "epoch": 1.0479187741186167,
      "grad_norm": 0.09633256494998932,
      "learning_rate": 5.01750040645534e-05,
      "loss": 0.0878,
      "step": 27970
    },
    {
      "epoch": 1.0482934322430781,
      "grad_norm": 0.0700259879231453,
      "learning_rate": 5.0144029988721575e-05,
      "loss": 0.0883,
      "step": 27980
    },
    {
      "epoch": 1.0486680903675396,
      "grad_norm": 0.06627554446458817,
      "learning_rate": 5.0113055857616587e-05,
      "loss": 0.0897,
      "step": 27990
    },
    {
      "epoch": 1.049042748492001,
      "grad_norm": 0.09473446011543274,
      "learning_rate": 5.008208168312517e-05,
      "loss": 0.0933,
      "step": 28000
    },
    {
      "epoch": 1.0494174066164625,
      "grad_norm": 0.0706946924328804,
      "learning_rate": 5.0051107477133976e-05,
      "loss": 0.0924,
      "step": 28010
    },
    {
      "epoch": 1.049792064740924,
      "grad_norm": 0.07373775541782379,
      "learning_rate": 5.00201332515297e-05,
      "loss": 0.0933,
      "step": 28020
    },
    {
      "epoch": 1.0501667228653853,
      "grad_norm": 0.09730464965105057,
      "learning_rate": 4.998915901819907e-05,
      "loss": 0.0888,
      "step": 28030
    },
    {
      "epoch": 1.0505413809898467,
      "grad_norm": 0.10795286297798157,
      "learning_rate": 4.995818478902879e-05,
      "loss": 0.088,
      "step": 28040
    },
    {
      "epoch": 1.0509160391143082,
      "grad_norm": 0.08442842215299606,
      "learning_rate": 4.992721057590558e-05,
      "loss": 0.0906,
      "step": 28050
    },
    {
      "epoch": 1.0512906972387697,
      "grad_norm": 0.12018934637308121,
      "learning_rate": 4.9896236390716116e-05,
      "loss": 0.0924,
      "step": 28060
    },
    {
      "epoch": 1.0516653553632311,
      "grad_norm": 0.11327734589576721,
      "learning_rate": 4.986526224534709e-05,
      "loss": 0.0901,
      "step": 28070
    },
    {
      "epoch": 1.0520400134876924,
      "grad_norm": 0.11021091043949127,
      "learning_rate": 4.983428815168523e-05,
      "loss": 0.0911,
      "step": 28080
    },
    {
      "epoch": 1.0524146716121539,
      "grad_norm": 0.07357650995254517,
      "learning_rate": 4.9803314121617136e-05,
      "loss": 0.0883,
      "step": 28090
    },
    {
      "epoch": 1.0527893297366153,
      "grad_norm": 0.07109928131103516,
      "learning_rate": 4.977234016702947e-05,
      "loss": 0.0904,
      "step": 28100
    },
    {
      "epoch": 1.0531639878610768,
      "grad_norm": 0.051371484994888306,
      "learning_rate": 4.9741366299808814e-05,
      "loss": 0.0876,
      "step": 28110
    },
    {
      "epoch": 1.0535386459855383,
      "grad_norm": 0.06496957689523697,
      "learning_rate": 4.971039253184175e-05,
      "loss": 0.0933,
      "step": 28120
    },
    {
      "epoch": 1.0539133041099997,
      "grad_norm": 0.06862547993659973,
      "learning_rate": 4.967941887501483e-05,
      "loss": 0.0895,
      "step": 28130
    },
    {
      "epoch": 1.054287962234461,
      "grad_norm": 0.10988996177911758,
      "learning_rate": 4.964844534121452e-05,
      "loss": 0.0911,
      "step": 28140
    },
    {
      "epoch": 1.0546626203589224,
      "grad_norm": 0.11221086233854294,
      "learning_rate": 4.961747194232727e-05,
      "loss": 0.0902,
      "step": 28150
    },
    {
      "epoch": 1.055037278483384,
      "grad_norm": 0.08655653148889542,
      "learning_rate": 4.958649869023945e-05,
      "loss": 0.0873,
      "step": 28160
    },
    {
      "epoch": 1.0554119366078454,
      "grad_norm": 0.08272749185562134,
      "learning_rate": 4.955552559683742e-05,
      "loss": 0.0867,
      "step": 28170
    },
    {
      "epoch": 1.0557865947323068,
      "grad_norm": 0.0710965171456337,
      "learning_rate": 4.952455267400746e-05,
      "loss": 0.0923,
      "step": 28180
    },
    {
      "epoch": 1.056161252856768,
      "grad_norm": 0.0674249678850174,
      "learning_rate": 4.949357993363574e-05,
      "loss": 0.0908,
      "step": 28190
    },
    {
      "epoch": 1.0565359109812296,
      "grad_norm": 0.09233571588993073,
      "learning_rate": 4.9462607387608445e-05,
      "loss": 0.0894,
      "step": 28200
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 0.09371821582317352,
      "learning_rate": 4.943163504781157e-05,
      "loss": 0.0901,
      "step": 28210
    },
    {
      "epoch": 1.0572852272301525,
      "grad_norm": 0.08350200206041336,
      "learning_rate": 4.940066292613119e-05,
      "loss": 0.0908,
      "step": 28220
    },
    {
      "epoch": 1.057659885354614,
      "grad_norm": 0.10047006607055664,
      "learning_rate": 4.936969103445314e-05,
      "loss": 0.0895,
      "step": 28230
    },
    {
      "epoch": 1.0580345434790754,
      "grad_norm": 0.06804876774549484,
      "learning_rate": 4.933871938466325e-05,
      "loss": 0.0908,
      "step": 28240
    },
    {
      "epoch": 1.0584092016035367,
      "grad_norm": 0.08190098404884338,
      "learning_rate": 4.930774798864723e-05,
      "loss": 0.0891,
      "step": 28250
    },
    {
      "epoch": 1.0587838597279982,
      "grad_norm": 0.07113529741764069,
      "learning_rate": 4.9276776858290716e-05,
      "loss": 0.0896,
      "step": 28260
    },
    {
      "epoch": 1.0591585178524596,
      "grad_norm": 0.07653440535068512,
      "learning_rate": 4.924580600547924e-05,
      "loss": 0.0865,
      "step": 28270
    },
    {
      "epoch": 1.059533175976921,
      "grad_norm": 0.07227169722318649,
      "learning_rate": 4.9214835442098185e-05,
      "loss": 0.0857,
      "step": 28280
    },
    {
      "epoch": 1.0599078341013826,
      "grad_norm": 0.06534330546855927,
      "learning_rate": 4.9183865180032875e-05,
      "loss": 0.0879,
      "step": 28290
    },
    {
      "epoch": 1.060282492225844,
      "grad_norm": 0.1233617439866066,
      "learning_rate": 4.9152895231168466e-05,
      "loss": 0.0923,
      "step": 28300
    },
    {
      "epoch": 1.0606571503503053,
      "grad_norm": 0.08446081727743149,
      "learning_rate": 4.912192560739006e-05,
      "loss": 0.0937,
      "step": 28310
    },
    {
      "epoch": 1.0610318084747667,
      "grad_norm": 0.09537096321582794,
      "learning_rate": 4.909095632058259e-05,
      "loss": 0.09,
      "step": 28320
    },
    {
      "epoch": 1.0614064665992282,
      "grad_norm": 0.09751936048269272,
      "learning_rate": 4.905998738263086e-05,
      "loss": 0.0916,
      "step": 28330
    },
    {
      "epoch": 1.0617811247236897,
      "grad_norm": 0.12869012355804443,
      "learning_rate": 4.9029018805419564e-05,
      "loss": 0.0898,
      "step": 28340
    },
    {
      "epoch": 1.0621557828481512,
      "grad_norm": 0.06987304240465164,
      "learning_rate": 4.89980506008332e-05,
      "loss": 0.0931,
      "step": 28350
    },
    {
      "epoch": 1.0625304409726124,
      "grad_norm": 0.06843703240156174,
      "learning_rate": 4.8967082780756216e-05,
      "loss": 0.0864,
      "step": 28360
    },
    {
      "epoch": 1.0629050990970739,
      "grad_norm": 0.07195982336997986,
      "learning_rate": 4.893611535707284e-05,
      "loss": 0.0887,
      "step": 28370
    },
    {
      "epoch": 1.0632797572215353,
      "grad_norm": 0.07849302887916565,
      "learning_rate": 4.890514834166717e-05,
      "loss": 0.0946,
      "step": 28380
    },
    {
      "epoch": 1.0636544153459968,
      "grad_norm": 0.06902255862951279,
      "learning_rate": 4.887418174642313e-05,
      "loss": 0.0874,
      "step": 28390
    },
    {
      "epoch": 1.0640290734704583,
      "grad_norm": 0.10348692536354065,
      "learning_rate": 4.8843215583224544e-05,
      "loss": 0.0896,
      "step": 28400
    },
    {
      "epoch": 1.0644037315949197,
      "grad_norm": 0.10819441080093384,
      "learning_rate": 4.8812249863954986e-05,
      "loss": 0.0928,
      "step": 28410
    },
    {
      "epoch": 1.064778389719381,
      "grad_norm": 0.06304188072681427,
      "learning_rate": 4.878128460049792e-05,
      "loss": 0.0931,
      "step": 28420
    },
    {
      "epoch": 1.0651530478438425,
      "grad_norm": 0.08121945708990097,
      "learning_rate": 4.875031980473659e-05,
      "loss": 0.0887,
      "step": 28430
    },
    {
      "epoch": 1.065527705968304,
      "grad_norm": 0.07534218579530716,
      "learning_rate": 4.87193554885541e-05,
      "loss": 0.0953,
      "step": 28440
    },
    {
      "epoch": 1.0659023640927654,
      "grad_norm": 0.07773149013519287,
      "learning_rate": 4.868839166383336e-05,
      "loss": 0.0938,
      "step": 28450
    },
    {
      "epoch": 1.0662770222172269,
      "grad_norm": 0.0806262269616127,
      "learning_rate": 4.86574283424571e-05,
      "loss": 0.0901,
      "step": 28460
    },
    {
      "epoch": 1.0666516803416881,
      "grad_norm": 0.06240633502602577,
      "learning_rate": 4.8626465536307805e-05,
      "loss": 0.093,
      "step": 28470
    },
    {
      "epoch": 1.0670263384661496,
      "grad_norm": 0.0782952830195427,
      "learning_rate": 4.8595503257267813e-05,
      "loss": 0.0864,
      "step": 28480
    },
    {
      "epoch": 1.067400996590611,
      "grad_norm": 0.07862481474876404,
      "learning_rate": 4.8564541517219274e-05,
      "loss": 0.09,
      "step": 28490
    },
    {
      "epoch": 1.0677756547150725,
      "grad_norm": 0.06125953048467636,
      "learning_rate": 4.8533580328044074e-05,
      "loss": 0.091,
      "step": 28500
    },
    {
      "epoch": 1.068150312839534,
      "grad_norm": 0.05648088827729225,
      "learning_rate": 4.8502619701623944e-05,
      "loss": 0.0899,
      "step": 28510
    },
    {
      "epoch": 1.0685249709639955,
      "grad_norm": 0.09553848952054977,
      "learning_rate": 4.8471659649840336e-05,
      "loss": 0.0913,
      "step": 28520
    },
    {
      "epoch": 1.0688996290884567,
      "grad_norm": 0.07435113191604614,
      "learning_rate": 4.8440700184574536e-05,
      "loss": 0.0897,
      "step": 28530
    },
    {
      "epoch": 1.0692742872129182,
      "grad_norm": 0.06546736508607864,
      "learning_rate": 4.8409741317707615e-05,
      "loss": 0.0911,
      "step": 28540
    },
    {
      "epoch": 1.0696489453373796,
      "grad_norm": 0.07708914577960968,
      "learning_rate": 4.837878306112035e-05,
      "loss": 0.0907,
      "step": 28550
    },
    {
      "epoch": 1.070023603461841,
      "grad_norm": 0.10107969492673874,
      "learning_rate": 4.834782542669334e-05,
      "loss": 0.0883,
      "step": 28560
    },
    {
      "epoch": 1.0703982615863026,
      "grad_norm": 0.07268109917640686,
      "learning_rate": 4.8316868426306916e-05,
      "loss": 0.0887,
      "step": 28570
    },
    {
      "epoch": 1.0707729197107638,
      "grad_norm": 0.07880766689777374,
      "learning_rate": 4.828591207184116e-05,
      "loss": 0.0907,
      "step": 28580
    },
    {
      "epoch": 1.0711475778352253,
      "grad_norm": 0.25187918543815613,
      "learning_rate": 4.8254956375175955e-05,
      "loss": 0.0905,
      "step": 28590
    },
    {
      "epoch": 1.0715222359596868,
      "grad_norm": 0.07493346929550171,
      "learning_rate": 4.8224001348190884e-05,
      "loss": 0.088,
      "step": 28600
    },
    {
      "epoch": 1.0718968940841482,
      "grad_norm": 0.06563124060630798,
      "learning_rate": 4.819304700276529e-05,
      "loss": 0.0899,
      "step": 28610
    },
    {
      "epoch": 1.0722715522086097,
      "grad_norm": 0.06173646077513695,
      "learning_rate": 4.816209335077822e-05,
      "loss": 0.0877,
      "step": 28620
    },
    {
      "epoch": 1.0726462103330712,
      "grad_norm": 0.10088451951742172,
      "learning_rate": 4.8131140404108537e-05,
      "loss": 0.0884,
      "step": 28630
    },
    {
      "epoch": 1.0730208684575324,
      "grad_norm": 0.058161113411188126,
      "learning_rate": 4.8100188174634766e-05,
      "loss": 0.0898,
      "step": 28640
    },
    {
      "epoch": 1.0733955265819939,
      "grad_norm": 0.06411455571651459,
      "learning_rate": 4.806923667423515e-05,
      "loss": 0.0882,
      "step": 28650
    },
    {
      "epoch": 1.0737701847064554,
      "grad_norm": 0.06757531315088272,
      "learning_rate": 4.8038285914787705e-05,
      "loss": 0.088,
      "step": 28660
    },
    {
      "epoch": 1.0741448428309168,
      "grad_norm": 0.06525420397520065,
      "learning_rate": 4.8007335908170084e-05,
      "loss": 0.0952,
      "step": 28670
    },
    {
      "epoch": 1.0745195009553783,
      "grad_norm": 0.06394711136817932,
      "learning_rate": 4.797638666625978e-05,
      "loss": 0.0888,
      "step": 28680
    },
    {
      "epoch": 1.0748941590798395,
      "grad_norm": 0.06582160294055939,
      "learning_rate": 4.794543820093385e-05,
      "loss": 0.0912,
      "step": 28690
    },
    {
      "epoch": 1.075268817204301,
      "grad_norm": 0.06910526007413864,
      "learning_rate": 4.791449052406915e-05,
      "loss": 0.0915,
      "step": 28700
    },
    {
      "epoch": 1.0756434753287625,
      "grad_norm": 0.13475653529167175,
      "learning_rate": 4.788354364754217e-05,
      "loss": 0.0916,
      "step": 28710
    },
    {
      "epoch": 1.076018133453224,
      "grad_norm": 0.08153921365737915,
      "learning_rate": 4.7852597583229125e-05,
      "loss": 0.093,
      "step": 28720
    },
    {
      "epoch": 1.0763927915776854,
      "grad_norm": 0.07053368538618088,
      "learning_rate": 4.782165234300595e-05,
      "loss": 0.0852,
      "step": 28730
    },
    {
      "epoch": 1.0767674497021469,
      "grad_norm": 0.0705365538597107,
      "learning_rate": 4.77907079387482e-05,
      "loss": 0.0893,
      "step": 28740
    },
    {
      "epoch": 1.0771421078266081,
      "grad_norm": 0.12475883215665817,
      "learning_rate": 4.7759764382331155e-05,
      "loss": 0.0929,
      "step": 28750
    },
    {
      "epoch": 1.0775167659510696,
      "grad_norm": 0.08429598808288574,
      "learning_rate": 4.772882168562972e-05,
      "loss": 0.0872,
      "step": 28760
    },
    {
      "epoch": 1.077891424075531,
      "grad_norm": 0.11649151146411896,
      "learning_rate": 4.769787986051854e-05,
      "loss": 0.0897,
      "step": 28770
    },
    {
      "epoch": 1.0782660821999925,
      "grad_norm": 0.07764506340026855,
      "learning_rate": 4.766693891887188e-05,
      "loss": 0.0897,
      "step": 28780
    },
    {
      "epoch": 1.078640740324454,
      "grad_norm": 0.06647462397813797,
      "learning_rate": 4.763599887256366e-05,
      "loss": 0.0942,
      "step": 28790
    },
    {
      "epoch": 1.0790153984489153,
      "grad_norm": 0.06144328787922859,
      "learning_rate": 4.76050597334675e-05,
      "loss": 0.0913,
      "step": 28800
    },
    {
      "epoch": 1.0793900565733767,
      "grad_norm": 0.07428418844938278,
      "learning_rate": 4.7574121513456593e-05,
      "loss": 0.0898,
      "step": 28810
    },
    {
      "epoch": 1.0797647146978382,
      "grad_norm": 0.10255949944257736,
      "learning_rate": 4.754318422440387e-05,
      "loss": 0.0878,
      "step": 28820
    },
    {
      "epoch": 1.0801393728222997,
      "grad_norm": 0.0756489709019661,
      "learning_rate": 4.751224787818187e-05,
      "loss": 0.0887,
      "step": 28830
    },
    {
      "epoch": 1.0805140309467611,
      "grad_norm": 0.1133008748292923,
      "learning_rate": 4.7481312486662734e-05,
      "loss": 0.0901,
      "step": 28840
    },
    {
      "epoch": 1.0808886890712226,
      "grad_norm": 0.0764395147562027,
      "learning_rate": 4.745037806171829e-05,
      "loss": 0.0878,
      "step": 28850
    },
    {
      "epoch": 1.0812633471956838,
      "grad_norm": 0.08147329837083817,
      "learning_rate": 4.741944461521994e-05,
      "loss": 0.0917,
      "step": 28860
    },
    {
      "epoch": 1.0816380053201453,
      "grad_norm": 0.07313662767410278,
      "learning_rate": 4.738851215903878e-05,
      "loss": 0.0896,
      "step": 28870
    },
    {
      "epoch": 1.0820126634446068,
      "grad_norm": 0.09058082103729248,
      "learning_rate": 4.735758070504548e-05,
      "loss": 0.0898,
      "step": 28880
    },
    {
      "epoch": 1.0823873215690683,
      "grad_norm": 0.11146341264247894,
      "learning_rate": 4.732665026511032e-05,
      "loss": 0.0894,
      "step": 28890
    },
    {
      "epoch": 1.0827619796935297,
      "grad_norm": 0.07517855614423752,
      "learning_rate": 4.7295720851103195e-05,
      "loss": 0.0899,
      "step": 28900
    },
    {
      "epoch": 1.083136637817991,
      "grad_norm": 0.08472209423780441,
      "learning_rate": 4.7264792474893636e-05,
      "loss": 0.0922,
      "step": 28910
    },
    {
      "epoch": 1.0835112959424524,
      "grad_norm": 0.08413207530975342,
      "learning_rate": 4.723386514835077e-05,
      "loss": 0.0928,
      "step": 28920
    },
    {
      "epoch": 1.083885954066914,
      "grad_norm": 0.08286368101835251,
      "learning_rate": 4.720293888334327e-05,
      "loss": 0.0906,
      "step": 28930
    },
    {
      "epoch": 1.0842606121913754,
      "grad_norm": 0.0708349198102951,
      "learning_rate": 4.7172013691739476e-05,
      "loss": 0.0904,
      "step": 28940
    },
    {
      "epoch": 1.0846352703158368,
      "grad_norm": 0.08769412338733673,
      "learning_rate": 4.7141089585407236e-05,
      "loss": 0.093,
      "step": 28950
    },
    {
      "epoch": 1.0850099284402983,
      "grad_norm": 0.08268566429615021,
      "learning_rate": 4.7110166576214046e-05,
      "loss": 0.0879,
      "step": 28960
    },
    {
      "epoch": 1.0853845865647596,
      "grad_norm": 0.06642317026853561,
      "learning_rate": 4.707924467602698e-05,
      "loss": 0.0892,
      "step": 28970
    },
    {
      "epoch": 1.085759244689221,
      "grad_norm": 0.07660932093858719,
      "learning_rate": 4.7048323896712633e-05,
      "loss": 0.0902,
      "step": 28980
    },
    {
      "epoch": 1.0861339028136825,
      "grad_norm": 0.09807083755731583,
      "learning_rate": 4.701740425013722e-05,
      "loss": 0.0926,
      "step": 28990
    },
    {
      "epoch": 1.086508560938144,
      "grad_norm": 0.08393049985170364,
      "learning_rate": 4.698648574816647e-05,
      "loss": 0.0901,
      "step": 29000
    },
    {
      "epoch": 1.0868832190626054,
      "grad_norm": 0.10216183215379715,
      "learning_rate": 4.695556840266575e-05,
      "loss": 0.0901,
      "step": 29010
    },
    {
      "epoch": 1.087257877187067,
      "grad_norm": 0.0918823629617691,
      "learning_rate": 4.6924652225499934e-05,
      "loss": 0.094,
      "step": 29020
    },
    {
      "epoch": 1.0876325353115281,
      "grad_norm": 0.08962822705507278,
      "learning_rate": 4.689373722853343e-05,
      "loss": 0.088,
      "step": 29030
    },
    {
      "epoch": 1.0880071934359896,
      "grad_norm": 0.060401130467653275,
      "learning_rate": 4.686282342363021e-05,
      "loss": 0.0866,
      "step": 29040
    },
    {
      "epoch": 1.088381851560451,
      "grad_norm": 0.11796410381793976,
      "learning_rate": 4.6831910822653844e-05,
      "loss": 0.0922,
      "step": 29050
    },
    {
      "epoch": 1.0887565096849126,
      "grad_norm": 0.08052163571119308,
      "learning_rate": 4.6800999437467335e-05,
      "loss": 0.0905,
      "step": 29060
    },
    {
      "epoch": 1.089131167809374,
      "grad_norm": 0.08119764178991318,
      "learning_rate": 4.6770089279933315e-05,
      "loss": 0.0937,
      "step": 29070
    },
    {
      "epoch": 1.0895058259338355,
      "grad_norm": 0.07699233293533325,
      "learning_rate": 4.673918036191388e-05,
      "loss": 0.0893,
      "step": 29080
    },
    {
      "epoch": 1.0898804840582967,
      "grad_norm": 0.0817379355430603,
      "learning_rate": 4.670827269527067e-05,
      "loss": 0.0925,
      "step": 29090
    },
    {
      "epoch": 1.0902551421827582,
      "grad_norm": 0.06044415757060051,
      "learning_rate": 4.667736629186488e-05,
      "loss": 0.0865,
      "step": 29100
    },
    {
      "epoch": 1.0906298003072197,
      "grad_norm": 0.07410635054111481,
      "learning_rate": 4.664646116355717e-05,
      "loss": 0.0908,
      "step": 29110
    },
    {
      "epoch": 1.0910044584316811,
      "grad_norm": 0.07367989420890808,
      "learning_rate": 4.6615557322207746e-05,
      "loss": 0.088,
      "step": 29120
    },
    {
      "epoch": 1.0913791165561426,
      "grad_norm": 0.0789911225438118,
      "learning_rate": 4.658465477967625e-05,
      "loss": 0.0848,
      "step": 29130
    },
    {
      "epoch": 1.0917537746806039,
      "grad_norm": 0.05701379477977753,
      "learning_rate": 4.6553753547821967e-05,
      "loss": 0.0899,
      "step": 29140
    },
    {
      "epoch": 1.0921284328050653,
      "grad_norm": 0.06796416640281677,
      "learning_rate": 4.652285363850353e-05,
      "loss": 0.0876,
      "step": 29150
    },
    {
      "epoch": 1.0925030909295268,
      "grad_norm": 0.06783097982406616,
      "learning_rate": 4.6491955063579165e-05,
      "loss": 0.0906,
      "step": 29160
    },
    {
      "epoch": 1.0928777490539883,
      "grad_norm": 0.06738888472318649,
      "learning_rate": 4.6461057834906516e-05,
      "loss": 0.0898,
      "step": 29170
    },
    {
      "epoch": 1.0932524071784497,
      "grad_norm": 0.08132033795118332,
      "learning_rate": 4.6430161964342736e-05,
      "loss": 0.0886,
      "step": 29180
    },
    {
      "epoch": 1.0936270653029112,
      "grad_norm": 0.06678139418363571,
      "learning_rate": 4.639926746374451e-05,
      "loss": 0.0889,
      "step": 29190
    },
    {
      "epoch": 1.0940017234273725,
      "grad_norm": 0.09098875522613525,
      "learning_rate": 4.6368374344967914e-05,
      "loss": 0.0923,
      "step": 29200
    },
    {
      "epoch": 1.094376381551834,
      "grad_norm": 0.06898795813322067,
      "learning_rate": 4.633748261986855e-05,
      "loss": 0.0914,
      "step": 29210
    },
    {
      "epoch": 1.0947510396762954,
      "grad_norm": 0.06685683131217957,
      "learning_rate": 4.630659230030144e-05,
      "loss": 0.0895,
      "step": 29220
    },
    {
      "epoch": 1.0951256978007569,
      "grad_norm": 0.06559684127569199,
      "learning_rate": 4.6275703398121094e-05,
      "loss": 0.0901,
      "step": 29230
    },
    {
      "epoch": 1.0955003559252183,
      "grad_norm": 0.08359172195196152,
      "learning_rate": 4.624481592518152e-05,
      "loss": 0.0891,
      "step": 29240
    },
    {
      "epoch": 1.0958750140496796,
      "grad_norm": 0.05937795341014862,
      "learning_rate": 4.621392989333608e-05,
      "loss": 0.0897,
      "step": 29250
    },
    {
      "epoch": 1.096249672174141,
      "grad_norm": 0.06138000637292862,
      "learning_rate": 4.618304531443767e-05,
      "loss": 0.0875,
      "step": 29260
    },
    {
      "epoch": 1.0966243302986025,
      "grad_norm": 0.560221254825592,
      "learning_rate": 4.6152162200338563e-05,
      "loss": 0.0907,
      "step": 29270
    },
    {
      "epoch": 1.096998988423064,
      "grad_norm": 0.057865217328071594,
      "learning_rate": 4.612128056289053e-05,
      "loss": 0.0895,
      "step": 29280
    },
    {
      "epoch": 1.0973736465475254,
      "grad_norm": 0.07386836409568787,
      "learning_rate": 4.6090400413944746e-05,
      "loss": 0.0906,
      "step": 29290
    },
    {
      "epoch": 1.097748304671987,
      "grad_norm": 0.07958986610174179,
      "learning_rate": 4.60595217653518e-05,
      "loss": 0.0929,
      "step": 29300
    },
    {
      "epoch": 1.0981229627964482,
      "grad_norm": 0.09074649214744568,
      "learning_rate": 4.6028644628961735e-05,
      "loss": 0.092,
      "step": 29310
    },
    {
      "epoch": 1.0984976209209096,
      "grad_norm": 0.08513128012418747,
      "learning_rate": 4.5997769016623974e-05,
      "loss": 0.0898,
      "step": 29320
    },
    {
      "epoch": 1.098872279045371,
      "grad_norm": 0.08250140398740768,
      "learning_rate": 4.5966894940187414e-05,
      "loss": 0.0878,
      "step": 29330
    },
    {
      "epoch": 1.0992469371698326,
      "grad_norm": 0.08224786818027496,
      "learning_rate": 4.593602241150031e-05,
      "loss": 0.0918,
      "step": 29340
    },
    {
      "epoch": 1.099621595294294,
      "grad_norm": 0.10414078831672668,
      "learning_rate": 4.5905151442410365e-05,
      "loss": 0.0915,
      "step": 29350
    },
    {
      "epoch": 1.0999962534187553,
      "grad_norm": 0.10198990255594254,
      "learning_rate": 4.5874282044764634e-05,
      "loss": 0.0891,
      "step": 29360
    },
    {
      "epoch": 1.1003709115432168,
      "grad_norm": 0.08376540243625641,
      "learning_rate": 4.5843414230409584e-05,
      "loss": 0.0881,
      "step": 29370
    },
    {
      "epoch": 1.1007455696676782,
      "grad_norm": 0.07844734191894531,
      "learning_rate": 4.5812548011191135e-05,
      "loss": 0.0944,
      "step": 29380
    },
    {
      "epoch": 1.1011202277921397,
      "grad_norm": 0.08267541974782944,
      "learning_rate": 4.578168339895451e-05,
      "loss": 0.0893,
      "step": 29390
    },
    {
      "epoch": 1.1014948859166012,
      "grad_norm": 0.08036064356565475,
      "learning_rate": 4.575082040554437e-05,
      "loss": 0.0924,
      "step": 29400
    },
    {
      "epoch": 1.1018695440410626,
      "grad_norm": 0.07561156898736954,
      "learning_rate": 4.57199590428047e-05,
      "loss": 0.0922,
      "step": 29410
    },
    {
      "epoch": 1.1022442021655239,
      "grad_norm": 0.06700845807790756,
      "learning_rate": 4.568909932257894e-05,
      "loss": 0.0891,
      "step": 29420
    },
    {
      "epoch": 1.1026188602899853,
      "grad_norm": 0.07994475215673447,
      "learning_rate": 4.5658241256709836e-05,
      "loss": 0.0912,
      "step": 29430
    },
    {
      "epoch": 1.1029935184144468,
      "grad_norm": 0.07331034541130066,
      "learning_rate": 4.562738485703951e-05,
      "loss": 0.0886,
      "step": 29440
    },
    {
      "epoch": 1.1033681765389083,
      "grad_norm": 0.3667030930519104,
      "learning_rate": 4.559653013540948e-05,
      "loss": 0.0898,
      "step": 29450
    },
    {
      "epoch": 1.1037428346633698,
      "grad_norm": 0.059152476489543915,
      "learning_rate": 4.556567710366055e-05,
      "loss": 0.0916,
      "step": 29460
    },
    {
      "epoch": 1.104117492787831,
      "grad_norm": 0.06784223020076752,
      "learning_rate": 4.5534825773632946e-05,
      "loss": 0.0908,
      "step": 29470
    },
    {
      "epoch": 1.1044921509122925,
      "grad_norm": 0.08719319850206375,
      "learning_rate": 4.550397615716622e-05,
      "loss": 0.0867,
      "step": 29480
    },
    {
      "epoch": 1.104866809036754,
      "grad_norm": 0.0692901611328125,
      "learning_rate": 4.547312826609925e-05,
      "loss": 0.0893,
      "step": 29490
    },
    {
      "epoch": 1.1052414671612154,
      "grad_norm": 0.08028007298707962,
      "learning_rate": 4.5442282112270254e-05,
      "loss": 0.0918,
      "step": 29500
    },
    {
      "epoch": 1.1056161252856769,
      "grad_norm": 0.06905906647443771,
      "learning_rate": 4.541143770751679e-05,
      "loss": 0.0896,
      "step": 29510
    },
    {
      "epoch": 1.1059907834101383,
      "grad_norm": 0.08348479866981506,
      "learning_rate": 4.538059506367575e-05,
      "loss": 0.0863,
      "step": 29520
    },
    {
      "epoch": 1.1063654415345996,
      "grad_norm": 0.0917712152004242,
      "learning_rate": 4.534975419258337e-05,
      "loss": 0.091,
      "step": 29530
    },
    {
      "epoch": 1.106740099659061,
      "grad_norm": 0.10457224398851395,
      "learning_rate": 4.531891510607514e-05,
      "loss": 0.0899,
      "step": 29540
    },
    {
      "epoch": 1.1071147577835225,
      "grad_norm": 0.06617158651351929,
      "learning_rate": 4.528807781598592e-05,
      "loss": 0.0894,
      "step": 29550
    },
    {
      "epoch": 1.107489415907984,
      "grad_norm": 0.08323685079813004,
      "learning_rate": 4.525724233414988e-05,
      "loss": 0.0899,
      "step": 29560
    },
    {
      "epoch": 1.1078640740324455,
      "grad_norm": 0.08142958581447601,
      "learning_rate": 4.522640867240049e-05,
      "loss": 0.0918,
      "step": 29570
    },
    {
      "epoch": 1.1082387321569067,
      "grad_norm": 0.06524083763360977,
      "learning_rate": 4.519557684257049e-05,
      "loss": 0.0949,
      "step": 29580
    },
    {
      "epoch": 1.1086133902813682,
      "grad_norm": 0.09123051166534424,
      "learning_rate": 4.5164746856491965e-05,
      "loss": 0.0912,
      "step": 29590
    },
    {
      "epoch": 1.1089880484058297,
      "grad_norm": 0.07636062055826187,
      "learning_rate": 4.513391872599623e-05,
      "loss": 0.091,
      "step": 29600
    },
    {
      "epoch": 1.1093627065302911,
      "grad_norm": 0.08387628197669983,
      "learning_rate": 4.510309246291397e-05,
      "loss": 0.0919,
      "step": 29610
    },
    {
      "epoch": 1.1097373646547526,
      "grad_norm": 0.08383370190858841,
      "learning_rate": 4.507226807907511e-05,
      "loss": 0.09,
      "step": 29620
    },
    {
      "epoch": 1.110112022779214,
      "grad_norm": 0.16408397257328033,
      "learning_rate": 4.5041445586308815e-05,
      "loss": 0.0931,
      "step": 29630
    },
    {
      "epoch": 1.1104866809036753,
      "grad_norm": 0.09237176179885864,
      "learning_rate": 4.501062499644361e-05,
      "loss": 0.0845,
      "step": 29640
    },
    {
      "epoch": 1.1108613390281368,
      "grad_norm": 0.07591067254543304,
      "learning_rate": 4.4979806321307185e-05,
      "loss": 0.094,
      "step": 29650
    },
    {
      "epoch": 1.1112359971525982,
      "grad_norm": 0.0805245041847229,
      "learning_rate": 4.4948989572726615e-05,
      "loss": 0.0937,
      "step": 29660
    },
    {
      "epoch": 1.1116106552770597,
      "grad_norm": 0.08557603508234024,
      "learning_rate": 4.491817476252815e-05,
      "loss": 0.0946,
      "step": 29670
    },
    {
      "epoch": 1.1119853134015212,
      "grad_norm": 0.06399716436862946,
      "learning_rate": 4.488736190253731e-05,
      "loss": 0.0912,
      "step": 29680
    },
    {
      "epoch": 1.1123599715259824,
      "grad_norm": 0.10599625110626221,
      "learning_rate": 4.485655100457887e-05,
      "loss": 0.0954,
      "step": 29690
    },
    {
      "epoch": 1.112734629650444,
      "grad_norm": 0.06968415528535843,
      "learning_rate": 4.48257420804769e-05,
      "loss": 0.0887,
      "step": 29700
    },
    {
      "epoch": 1.1131092877749054,
      "grad_norm": 0.08613955229520798,
      "learning_rate": 4.479493514205464e-05,
      "loss": 0.0871,
      "step": 29710
    },
    {
      "epoch": 1.1134839458993668,
      "grad_norm": 0.10011821240186691,
      "learning_rate": 4.476413020113461e-05,
      "loss": 0.0935,
      "step": 29720
    },
    {
      "epoch": 1.1138586040238283,
      "grad_norm": 0.060368649661540985,
      "learning_rate": 4.473332726953853e-05,
      "loss": 0.0896,
      "step": 29730
    },
    {
      "epoch": 1.1142332621482898,
      "grad_norm": 0.06658785045146942,
      "learning_rate": 4.4702526359087386e-05,
      "loss": 0.0888,
      "step": 29740
    },
    {
      "epoch": 1.114607920272751,
      "grad_norm": 0.08932901918888092,
      "learning_rate": 4.46717274816014e-05,
      "loss": 0.0914,
      "step": 29750
    },
    {
      "epoch": 1.1149825783972125,
      "grad_norm": 0.08822955936193466,
      "learning_rate": 4.464093064889995e-05,
      "loss": 0.0912,
      "step": 29760
    },
    {
      "epoch": 1.115357236521674,
      "grad_norm": 0.05813110992312431,
      "learning_rate": 4.461013587280169e-05,
      "loss": 0.0893,
      "step": 29770
    },
    {
      "epoch": 1.1157318946461354,
      "grad_norm": 0.06401752680540085,
      "learning_rate": 4.457934316512445e-05,
      "loss": 0.0887,
      "step": 29780
    },
    {
      "epoch": 1.116106552770597,
      "grad_norm": 0.06435132771730423,
      "learning_rate": 4.454855253768527e-05,
      "loss": 0.0886,
      "step": 29790
    },
    {
      "epoch": 1.1164812108950584,
      "grad_norm": 0.09090544283390045,
      "learning_rate": 4.451776400230043e-05,
      "loss": 0.0914,
      "step": 29800
    },
    {
      "epoch": 1.1168558690195196,
      "grad_norm": 0.08200690150260925,
      "learning_rate": 4.448697757078536e-05,
      "loss": 0.0933,
      "step": 29810
    },
    {
      "epoch": 1.117230527143981,
      "grad_norm": 0.12669964134693146,
      "learning_rate": 4.445619325495469e-05,
      "loss": 0.0869,
      "step": 29820
    },
    {
      "epoch": 1.1176051852684425,
      "grad_norm": 0.07909931987524033,
      "learning_rate": 4.442541106662225e-05,
      "loss": 0.0916,
      "step": 29830
    },
    {
      "epoch": 1.117979843392904,
      "grad_norm": 0.08771068602800369,
      "learning_rate": 4.439463101760108e-05,
      "loss": 0.0917,
      "step": 29840
    },
    {
      "epoch": 1.1183545015173655,
      "grad_norm": 0.06281121075153351,
      "learning_rate": 4.4363853119703335e-05,
      "loss": 0.0926,
      "step": 29850
    },
    {
      "epoch": 1.1187291596418267,
      "grad_norm": 0.060802094638347626,
      "learning_rate": 4.4333077384740404e-05,
      "loss": 0.0895,
      "step": 29860
    },
    {
      "epoch": 1.1191038177662882,
      "grad_norm": 0.09031254053115845,
      "learning_rate": 4.4302303824522795e-05,
      "loss": 0.0872,
      "step": 29870
    },
    {
      "epoch": 1.1194784758907497,
      "grad_norm": 0.0738367885351181,
      "learning_rate": 4.427153245086022e-05,
      "loss": 0.087,
      "step": 29880
    },
    {
      "epoch": 1.1198531340152111,
      "grad_norm": 0.08641179651021957,
      "learning_rate": 4.424076327556156e-05,
      "loss": 0.0875,
      "step": 29890
    },
    {
      "epoch": 1.1202277921396726,
      "grad_norm": 0.10145968943834305,
      "learning_rate": 4.420999631043481e-05,
      "loss": 0.0908,
      "step": 29900
    },
    {
      "epoch": 1.120602450264134,
      "grad_norm": 0.0834323987364769,
      "learning_rate": 4.417923156728715e-05,
      "loss": 0.0892,
      "step": 29910
    },
    {
      "epoch": 1.1209771083885953,
      "grad_norm": 0.08318663388490677,
      "learning_rate": 4.414846905792487e-05,
      "loss": 0.0887,
      "step": 29920
    },
    {
      "epoch": 1.1213517665130568,
      "grad_norm": 0.09152276813983917,
      "learning_rate": 4.411770879415347e-05,
      "loss": 0.0893,
      "step": 29930
    },
    {
      "epoch": 1.1217264246375183,
      "grad_norm": 0.08347094058990479,
      "learning_rate": 4.4086950787777534e-05,
      "loss": 0.0853,
      "step": 29940
    },
    {
      "epoch": 1.1221010827619797,
      "grad_norm": 0.06355607509613037,
      "learning_rate": 4.405619505060077e-05,
      "loss": 0.0899,
      "step": 29950
    },
    {
      "epoch": 1.1224757408864412,
      "grad_norm": 0.13369758427143097,
      "learning_rate": 4.402544159442608e-05,
      "loss": 0.0883,
      "step": 29960
    },
    {
      "epoch": 1.1228503990109027,
      "grad_norm": 0.10724532604217529,
      "learning_rate": 4.399469043105539e-05,
      "loss": 0.0892,
      "step": 29970
    },
    {
      "epoch": 1.123225057135364,
      "grad_norm": 0.08832233399152756,
      "learning_rate": 4.3963941572289855e-05,
      "loss": 0.0889,
      "step": 29980
    },
    {
      "epoch": 1.1235997152598254,
      "grad_norm": 0.07993217557668686,
      "learning_rate": 4.3933195029929693e-05,
      "loss": 0.0893,
      "step": 29990
    },
    {
      "epoch": 1.1239743733842869,
      "grad_norm": 0.10035933554172516,
      "learning_rate": 4.390245081577422e-05,
      "loss": 0.0891,
      "step": 30000
    },
    {
      "epoch": 1.1243490315087483,
      "grad_norm": 0.0734000876545906,
      "learning_rate": 4.387170894162187e-05,
      "loss": 0.0898,
      "step": 30010
    },
    {
      "epoch": 1.1247236896332098,
      "grad_norm": 0.09250262379646301,
      "learning_rate": 4.384096941927017e-05,
      "loss": 0.0926,
      "step": 30020
    },
    {
      "epoch": 1.125098347757671,
      "grad_norm": 0.07676023989915848,
      "learning_rate": 4.3810232260515815e-05,
      "loss": 0.086,
      "step": 30030
    },
    {
      "epoch": 1.1254730058821325,
      "grad_norm": 0.09773627668619156,
      "learning_rate": 4.377949747715448e-05,
      "loss": 0.0918,
      "step": 30040
    },
    {
      "epoch": 1.125847664006594,
      "grad_norm": 0.0817319005727768,
      "learning_rate": 4.374876508098101e-05,
      "loss": 0.0929,
      "step": 30050
    },
    {
      "epoch": 1.1262223221310554,
      "grad_norm": 0.12437663227319717,
      "learning_rate": 4.371803508378929e-05,
      "loss": 0.0877,
      "step": 30060
    },
    {
      "epoch": 1.126596980255517,
      "grad_norm": 0.08465576171875,
      "learning_rate": 4.368730749737232e-05,
      "loss": 0.0915,
      "step": 30070
    },
    {
      "epoch": 1.1269716383799784,
      "grad_norm": 0.0647704228758812,
      "learning_rate": 4.365658233352215e-05,
      "loss": 0.0902,
      "step": 30080
    },
    {
      "epoch": 1.1273462965044396,
      "grad_norm": 0.09636594355106354,
      "learning_rate": 4.3625859604029914e-05,
      "loss": 0.0909,
      "step": 30090
    },
    {
      "epoch": 1.127720954628901,
      "grad_norm": 0.08356351405382156,
      "learning_rate": 4.35951393206858e-05,
      "loss": 0.089,
      "step": 30100
    },
    {
      "epoch": 1.1280956127533626,
      "grad_norm": 0.05317261442542076,
      "learning_rate": 4.3564421495279046e-05,
      "loss": 0.0879,
      "step": 30110
    },
    {
      "epoch": 1.128470270877824,
      "grad_norm": 0.0839260071516037,
      "learning_rate": 4.3533706139597985e-05,
      "loss": 0.0926,
      "step": 30120
    },
    {
      "epoch": 1.1288449290022855,
      "grad_norm": 0.05975979194045067,
      "learning_rate": 4.3502993265429986e-05,
      "loss": 0.088,
      "step": 30130
    },
    {
      "epoch": 1.1292195871267467,
      "grad_norm": 0.10671963542699814,
      "learning_rate": 4.347228288456144e-05,
      "loss": 0.0906,
      "step": 30140
    },
    {
      "epoch": 1.1295942452512082,
      "grad_norm": 0.12176819890737534,
      "learning_rate": 4.3441575008777815e-05,
      "loss": 0.0886,
      "step": 30150
    },
    {
      "epoch": 1.1299689033756697,
      "grad_norm": 0.08223125338554382,
      "learning_rate": 4.3410869649863576e-05,
      "loss": 0.0915,
      "step": 30160
    },
    {
      "epoch": 1.1303435615001312,
      "grad_norm": 0.062077756971120834,
      "learning_rate": 4.338016681960229e-05,
      "loss": 0.0899,
      "step": 30170
    },
    {
      "epoch": 1.1307182196245926,
      "grad_norm": 0.08351199328899384,
      "learning_rate": 4.334946652977651e-05,
      "loss": 0.0935,
      "step": 30180
    },
    {
      "epoch": 1.131092877749054,
      "grad_norm": 0.07519112527370453,
      "learning_rate": 4.3318768792167786e-05,
      "loss": 0.0925,
      "step": 30190
    },
    {
      "epoch": 1.1314675358735153,
      "grad_norm": 0.08199988305568695,
      "learning_rate": 4.328807361855673e-05,
      "loss": 0.0904,
      "step": 30200
    },
    {
      "epoch": 1.1318421939979768,
      "grad_norm": 0.08400119841098785,
      "learning_rate": 4.3257381020722985e-05,
      "loss": 0.0916,
      "step": 30210
    },
    {
      "epoch": 1.1322168521224383,
      "grad_norm": 0.08201619982719421,
      "learning_rate": 4.322669101044516e-05,
      "loss": 0.0893,
      "step": 30220
    },
    {
      "epoch": 1.1325915102468997,
      "grad_norm": 0.07606121152639389,
      "learning_rate": 4.3196003599500903e-05,
      "loss": 0.094,
      "step": 30230
    },
    {
      "epoch": 1.1329661683713612,
      "grad_norm": 0.07846385985612869,
      "learning_rate": 4.3165318799666845e-05,
      "loss": 0.0912,
      "step": 30240
    },
    {
      "epoch": 1.1333408264958225,
      "grad_norm": 0.06205914542078972,
      "learning_rate": 4.31346366227186e-05,
      "loss": 0.0894,
      "step": 30250
    },
    {
      "epoch": 1.133715484620284,
      "grad_norm": 0.06788694113492966,
      "learning_rate": 4.310395708043085e-05,
      "loss": 0.0919,
      "step": 30260
    },
    {
      "epoch": 1.1340901427447454,
      "grad_norm": 0.08745205402374268,
      "learning_rate": 4.307328018457719e-05,
      "loss": 0.0888,
      "step": 30270
    },
    {
      "epoch": 1.1344648008692069,
      "grad_norm": 0.09976530075073242,
      "learning_rate": 4.304260594693022e-05,
      "loss": 0.0922,
      "step": 30280
    },
    {
      "epoch": 1.1348394589936683,
      "grad_norm": 0.12201503664255142,
      "learning_rate": 4.301193437926153e-05,
      "loss": 0.0918,
      "step": 30290
    },
    {
      "epoch": 1.1352141171181298,
      "grad_norm": 0.07057338207960129,
      "learning_rate": 4.298126549334166e-05,
      "loss": 0.0889,
      "step": 30300
    },
    {
      "epoch": 1.135588775242591,
      "grad_norm": 0.08307044953107834,
      "learning_rate": 4.2950599300940166e-05,
      "loss": 0.09,
      "step": 30310
    },
    {
      "epoch": 1.1359634333670525,
      "grad_norm": 0.061452221125364304,
      "learning_rate": 4.291993581382554e-05,
      "loss": 0.0919,
      "step": 30320
    },
    {
      "epoch": 1.136338091491514,
      "grad_norm": 0.0643853023648262,
      "learning_rate": 4.288927504376523e-05,
      "loss": 0.0875,
      "step": 30330
    },
    {
      "epoch": 1.1367127496159755,
      "grad_norm": 0.06872262805700302,
      "learning_rate": 4.285861700252564e-05,
      "loss": 0.0887,
      "step": 30340
    },
    {
      "epoch": 1.137087407740437,
      "grad_norm": 0.09789979457855225,
      "learning_rate": 4.2827961701872185e-05,
      "loss": 0.0936,
      "step": 30350
    },
    {
      "epoch": 1.1374620658648982,
      "grad_norm": 0.10339532792568207,
      "learning_rate": 4.279730915356913e-05,
      "loss": 0.0908,
      "step": 30360
    },
    {
      "epoch": 1.1378367239893596,
      "grad_norm": 0.06995727866888046,
      "learning_rate": 4.276665936937978e-05,
      "loss": 0.0934,
      "step": 30370
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 0.12386950105428696,
      "learning_rate": 4.273601236106629e-05,
      "loss": 0.0908,
      "step": 30380
    },
    {
      "epoch": 1.1385860402382826,
      "grad_norm": 0.07258515059947968,
      "learning_rate": 4.2705368140389814e-05,
      "loss": 0.0942,
      "step": 30390
    },
    {
      "epoch": 1.138960698362744,
      "grad_norm": 0.053152650594711304,
      "learning_rate": 4.267472671911044e-05,
      "loss": 0.0871,
      "step": 30400
    },
    {
      "epoch": 1.1393353564872055,
      "grad_norm": 0.06790454685688019,
      "learning_rate": 4.264408810898712e-05,
      "loss": 0.0917,
      "step": 30410
    },
    {
      "epoch": 1.1397100146116668,
      "grad_norm": 0.08599065244197845,
      "learning_rate": 4.261345232177779e-05,
      "loss": 0.0921,
      "step": 30420
    },
    {
      "epoch": 1.1400846727361282,
      "grad_norm": 0.10488873720169067,
      "learning_rate": 4.258281936923927e-05,
      "loss": 0.0909,
      "step": 30430
    },
    {
      "epoch": 1.1404593308605897,
      "grad_norm": 0.06114167720079422,
      "learning_rate": 4.255218926312729e-05,
      "loss": 0.091,
      "step": 30440
    },
    {
      "epoch": 1.1408339889850512,
      "grad_norm": 0.08935350924730301,
      "learning_rate": 4.252156201519652e-05,
      "loss": 0.0912,
      "step": 30450
    },
    {
      "epoch": 1.1412086471095126,
      "grad_norm": 0.0802164077758789,
      "learning_rate": 4.2490937637200495e-05,
      "loss": 0.0921,
      "step": 30460
    },
    {
      "epoch": 1.1415833052339739,
      "grad_norm": 0.0985756516456604,
      "learning_rate": 4.246031614089168e-05,
      "loss": 0.0919,
      "step": 30470
    },
    {
      "epoch": 1.1419579633584354,
      "grad_norm": 0.08751899749040604,
      "learning_rate": 4.242969753802138e-05,
      "loss": 0.0907,
      "step": 30480
    },
    {
      "epoch": 1.1423326214828968,
      "grad_norm": 0.08173342794179916,
      "learning_rate": 4.239908184033989e-05,
      "loss": 0.0894,
      "step": 30490
    },
    {
      "epoch": 1.1427072796073583,
      "grad_norm": 0.06380335241556168,
      "learning_rate": 4.2368469059596284e-05,
      "loss": 0.0881,
      "step": 30500
    },
    {
      "epoch": 1.1430819377318198,
      "grad_norm": 0.12236128002405167,
      "learning_rate": 4.233785920753858e-05,
      "loss": 0.0899,
      "step": 30510
    },
    {
      "epoch": 1.1434565958562812,
      "grad_norm": 0.06506451219320297,
      "learning_rate": 4.230725229591363e-05,
      "loss": 0.0924,
      "step": 30520
    },
    {
      "epoch": 1.1438312539807425,
      "grad_norm": 0.08843503892421722,
      "learning_rate": 4.22766483364672e-05,
      "loss": 0.0873,
      "step": 30530
    },
    {
      "epoch": 1.144205912105204,
      "grad_norm": 0.10995373129844666,
      "learning_rate": 4.224604734094392e-05,
      "loss": 0.0933,
      "step": 30540
    },
    {
      "epoch": 1.1445805702296654,
      "grad_norm": 0.07240357249975204,
      "learning_rate": 4.221544932108723e-05,
      "loss": 0.091,
      "step": 30550
    },
    {
      "epoch": 1.1449552283541269,
      "grad_norm": 0.08968895673751831,
      "learning_rate": 4.218485428863949e-05,
      "loss": 0.0878,
      "step": 30560
    },
    {
      "epoch": 1.1453298864785884,
      "grad_norm": 0.07340895384550095,
      "learning_rate": 4.2154262255341883e-05,
      "loss": 0.0879,
      "step": 30570
    },
    {
      "epoch": 1.1457045446030496,
      "grad_norm": 0.06961897015571594,
      "learning_rate": 4.212367323293442e-05,
      "loss": 0.0916,
      "step": 30580
    },
    {
      "epoch": 1.146079202727511,
      "grad_norm": 0.06759720295667648,
      "learning_rate": 4.2093087233156034e-05,
      "loss": 0.0931,
      "step": 30590
    },
    {
      "epoch": 1.1464538608519725,
      "grad_norm": 0.08835522085428238,
      "learning_rate": 4.20625042677444e-05,
      "loss": 0.0922,
      "step": 30600
    },
    {
      "epoch": 1.146828518976434,
      "grad_norm": 0.0869441106915474,
      "learning_rate": 4.203192434843611e-05,
      "loss": 0.093,
      "step": 30610
    },
    {
      "epoch": 1.1472031771008955,
      "grad_norm": 0.07414200901985168,
      "learning_rate": 4.2001347486966515e-05,
      "loss": 0.0891,
      "step": 30620
    },
    {
      "epoch": 1.147577835225357,
      "grad_norm": 0.08283144235610962,
      "learning_rate": 4.197077369506986e-05,
      "loss": 0.0851,
      "step": 30630
    },
    {
      "epoch": 1.1479524933498184,
      "grad_norm": 0.09209515154361725,
      "learning_rate": 4.194020298447918e-05,
      "loss": 0.0918,
      "step": 30640
    },
    {
      "epoch": 1.1483271514742797,
      "grad_norm": 0.09738791733980179,
      "learning_rate": 4.1909635366926316e-05,
      "loss": 0.0896,
      "step": 30650
    },
    {
      "epoch": 1.1487018095987411,
      "grad_norm": 0.23722633719444275,
      "learning_rate": 4.187907085414195e-05,
      "loss": 0.0927,
      "step": 30660
    },
    {
      "epoch": 1.1490764677232026,
      "grad_norm": 0.06788937002420425,
      "learning_rate": 4.1848509457855524e-05,
      "loss": 0.087,
      "step": 30670
    },
    {
      "epoch": 1.149451125847664,
      "grad_norm": 0.0794292464852333,
      "learning_rate": 4.1817951189795355e-05,
      "loss": 0.0898,
      "step": 30680
    },
    {
      "epoch": 1.1498257839721253,
      "grad_norm": 0.09777239710092545,
      "learning_rate": 4.178739606168852e-05,
      "loss": 0.0905,
      "step": 30690
    },
    {
      "epoch": 1.1502004420965868,
      "grad_norm": 0.0683351457118988,
      "learning_rate": 4.175684408526088e-05,
      "loss": 0.092,
      "step": 30700
    },
    {
      "epoch": 1.1505751002210483,
      "grad_norm": 0.0909007117152214,
      "learning_rate": 4.172629527223708e-05,
      "loss": 0.0894,
      "step": 30710
    },
    {
      "epoch": 1.1509497583455097,
      "grad_norm": 0.0648709088563919,
      "learning_rate": 4.169574963434061e-05,
      "loss": 0.0906,
      "step": 30720
    },
    {
      "epoch": 1.1513244164699712,
      "grad_norm": 0.11546704918146133,
      "learning_rate": 4.1665207183293696e-05,
      "loss": 0.0902,
      "step": 30730
    },
    {
      "epoch": 1.1516990745944327,
      "grad_norm": 0.10625167191028595,
      "learning_rate": 4.163466793081733e-05,
      "loss": 0.0904,
      "step": 30740
    },
    {
      "epoch": 1.1520737327188941,
      "grad_norm": 0.06604065001010895,
      "learning_rate": 4.160413188863131e-05,
      "loss": 0.0887,
      "step": 30750
    },
    {
      "epoch": 1.1524483908433554,
      "grad_norm": 0.07740695029497147,
      "learning_rate": 4.157359906845416e-05,
      "loss": 0.0868,
      "step": 30760
    },
    {
      "epoch": 1.1528230489678168,
      "grad_norm": 0.07660869508981705,
      "learning_rate": 4.154306948200322e-05,
      "loss": 0.09,
      "step": 30770
    },
    {
      "epoch": 1.1531977070922783,
      "grad_norm": 0.06473883241415024,
      "learning_rate": 4.151254314099456e-05,
      "loss": 0.0885,
      "step": 30780
    },
    {
      "epoch": 1.1535723652167398,
      "grad_norm": 0.12194022536277771,
      "learning_rate": 4.148202005714299e-05,
      "loss": 0.0939,
      "step": 30790
    },
    {
      "epoch": 1.1539470233412013,
      "grad_norm": 0.10156024247407913,
      "learning_rate": 4.1451500242162106e-05,
      "loss": 0.0893,
      "step": 30800
    },
    {
      "epoch": 1.1543216814656625,
      "grad_norm": 0.06843723356723785,
      "learning_rate": 4.1420983707764206e-05,
      "loss": 0.0937,
      "step": 30810
    },
    {
      "epoch": 1.154696339590124,
      "grad_norm": 0.07349327951669693,
      "learning_rate": 4.1390470465660375e-05,
      "loss": 0.0894,
      "step": 30820
    },
    {
      "epoch": 1.1550709977145854,
      "grad_norm": 0.08517760783433914,
      "learning_rate": 4.135996052756042e-05,
      "loss": 0.0929,
      "step": 30830
    },
    {
      "epoch": 1.155445655839047,
      "grad_norm": 0.09453409165143967,
      "learning_rate": 4.132945390517285e-05,
      "loss": 0.0887,
      "step": 30840
    },
    {
      "epoch": 1.1558203139635084,
      "grad_norm": 0.08992743492126465,
      "learning_rate": 4.129895061020492e-05,
      "loss": 0.0882,
      "step": 30850
    },
    {
      "epoch": 1.1561949720879698,
      "grad_norm": 0.07080162316560745,
      "learning_rate": 4.1268450654362655e-05,
      "loss": 0.0929,
      "step": 30860
    },
    {
      "epoch": 1.156569630212431,
      "grad_norm": 0.08100099861621857,
      "learning_rate": 4.1237954049350716e-05,
      "loss": 0.0904,
      "step": 30870
    },
    {
      "epoch": 1.1569442883368926,
      "grad_norm": 0.11506617069244385,
      "learning_rate": 4.120746080687254e-05,
      "loss": 0.0906,
      "step": 30880
    },
    {
      "epoch": 1.157318946461354,
      "grad_norm": 0.07862822711467743,
      "learning_rate": 4.117697093863023e-05,
      "loss": 0.0922,
      "step": 30890
    },
    {
      "epoch": 1.1576936045858155,
      "grad_norm": 0.06465943157672882,
      "learning_rate": 4.114648445632463e-05,
      "loss": 0.0915,
      "step": 30900
    },
    {
      "epoch": 1.158068262710277,
      "grad_norm": 0.056836627423763275,
      "learning_rate": 4.111600137165529e-05,
      "loss": 0.0907,
      "step": 30910
    },
    {
      "epoch": 1.1584429208347382,
      "grad_norm": 0.07287506014108658,
      "learning_rate": 4.10855216963204e-05,
      "loss": 0.0863,
      "step": 30920
    },
    {
      "epoch": 1.1588175789591997,
      "grad_norm": 0.0882270485162735,
      "learning_rate": 4.1055045442016895e-05,
      "loss": 0.0925,
      "step": 30930
    },
    {
      "epoch": 1.1591922370836611,
      "grad_norm": 0.0807252749800682,
      "learning_rate": 4.1024572620440396e-05,
      "loss": 0.0925,
      "step": 30940
    },
    {
      "epoch": 1.1595668952081226,
      "grad_norm": 0.06040159985423088,
      "learning_rate": 4.099410324328516e-05,
      "loss": 0.0904,
      "step": 30950
    },
    {
      "epoch": 1.159941553332584,
      "grad_norm": 0.06974165141582489,
      "learning_rate": 4.0963637322244174e-05,
      "loss": 0.0901,
      "step": 30960
    },
    {
      "epoch": 1.1603162114570456,
      "grad_norm": 0.09786325693130493,
      "learning_rate": 4.093317486900908e-05,
      "loss": 0.0894,
      "step": 30970
    },
    {
      "epoch": 1.1606908695815068,
      "grad_norm": 0.0995870977640152,
      "learning_rate": 4.0902715895270184e-05,
      "loss": 0.0857,
      "step": 30980
    },
    {
      "epoch": 1.1610655277059683,
      "grad_norm": 0.0899466797709465,
      "learning_rate": 4.087226041271644e-05,
      "loss": 0.0915,
      "step": 30990
    },
    {
      "epoch": 1.1614401858304297,
      "grad_norm": 0.06863836944103241,
      "learning_rate": 4.084180843303552e-05,
      "loss": 0.0901,
      "step": 31000
    },
    {
      "epoch": 1.1618148439548912,
      "grad_norm": 0.0624801442027092,
      "learning_rate": 4.0811359967913685e-05,
      "loss": 0.0887,
      "step": 31010
    },
    {
      "epoch": 1.1621895020793527,
      "grad_norm": 0.07861748337745667,
      "learning_rate": 4.078091502903589e-05,
      "loss": 0.0898,
      "step": 31020
    },
    {
      "epoch": 1.162564160203814,
      "grad_norm": 0.08906478434801102,
      "learning_rate": 4.075047362808572e-05,
      "loss": 0.0898,
      "step": 31030
    },
    {
      "epoch": 1.1629388183282754,
      "grad_norm": 0.07546151429414749,
      "learning_rate": 4.072003577674538e-05,
      "loss": 0.0929,
      "step": 31040
    },
    {
      "epoch": 1.1633134764527369,
      "grad_norm": 0.07545678317546844,
      "learning_rate": 4.068960148669579e-05,
      "loss": 0.0896,
      "step": 31050
    },
    {
      "epoch": 1.1636881345771983,
      "grad_norm": 0.08319729566574097,
      "learning_rate": 4.065917076961641e-05,
      "loss": 0.0884,
      "step": 31060
    },
    {
      "epoch": 1.1640627927016598,
      "grad_norm": 0.07647848129272461,
      "learning_rate": 4.062874363718539e-05,
      "loss": 0.0926,
      "step": 31070
    },
    {
      "epoch": 1.1644374508261213,
      "grad_norm": 0.06255657225847244,
      "learning_rate": 4.0598320101079466e-05,
      "loss": 0.0909,
      "step": 31080
    },
    {
      "epoch": 1.1648121089505825,
      "grad_norm": 0.06921566277742386,
      "learning_rate": 4.0567900172974016e-05,
      "loss": 0.0884,
      "step": 31090
    },
    {
      "epoch": 1.165186767075044,
      "grad_norm": 0.09145181626081467,
      "learning_rate": 4.053748386454306e-05,
      "loss": 0.0867,
      "step": 31100
    },
    {
      "epoch": 1.1655614251995055,
      "grad_norm": 0.1168520450592041,
      "learning_rate": 4.050707118745917e-05,
      "loss": 0.0928,
      "step": 31110
    },
    {
      "epoch": 1.165936083323967,
      "grad_norm": 0.06423233449459076,
      "learning_rate": 4.0476662153393555e-05,
      "loss": 0.0891,
      "step": 31120
    },
    {
      "epoch": 1.1663107414484284,
      "grad_norm": 0.07612577080726624,
      "learning_rate": 4.044625677401602e-05,
      "loss": 0.0881,
      "step": 31130
    },
    {
      "epoch": 1.1666853995728896,
      "grad_norm": 0.08061844110488892,
      "learning_rate": 4.041585506099499e-05,
      "loss": 0.0902,
      "step": 31140
    },
    {
      "epoch": 1.167060057697351,
      "grad_norm": 0.07683058083057404,
      "learning_rate": 4.0385457025997444e-05,
      "loss": 0.0874,
      "step": 31150
    },
    {
      "epoch": 1.1674347158218126,
      "grad_norm": 0.08525799959897995,
      "learning_rate": 4.0355062680689004e-05,
      "loss": 0.0862,
      "step": 31160
    },
    {
      "epoch": 1.167809373946274,
      "grad_norm": 0.07514770328998566,
      "learning_rate": 4.03246720367338e-05,
      "loss": 0.089,
      "step": 31170
    },
    {
      "epoch": 1.1681840320707355,
      "grad_norm": 0.0696590393781662,
      "learning_rate": 4.0294285105794595e-05,
      "loss": 0.0885,
      "step": 31180
    },
    {
      "epoch": 1.168558690195197,
      "grad_norm": 0.09642040729522705,
      "learning_rate": 4.026390189953274e-05,
      "loss": 0.092,
      "step": 31190
    },
    {
      "epoch": 1.1689333483196582,
      "grad_norm": 0.26677069067955017,
      "learning_rate": 4.023352242960811e-05,
      "loss": 0.0925,
      "step": 31200
    },
    {
      "epoch": 1.1693080064441197,
      "grad_norm": 0.13349229097366333,
      "learning_rate": 4.020314670767918e-05,
      "loss": 0.09,
      "step": 31210
    },
    {
      "epoch": 1.1696826645685812,
      "grad_norm": 0.07620523869991302,
      "learning_rate": 4.017277474540297e-05,
      "loss": 0.0881,
      "step": 31220
    },
    {
      "epoch": 1.1700573226930426,
      "grad_norm": 0.08791108429431915,
      "learning_rate": 4.014240655443504e-05,
      "loss": 0.0904,
      "step": 31230
    },
    {
      "epoch": 1.170431980817504,
      "grad_norm": 0.08815600723028183,
      "learning_rate": 4.0112042146429574e-05,
      "loss": 0.0907,
      "step": 31240
    },
    {
      "epoch": 1.1708066389419653,
      "grad_norm": 0.12741880118846893,
      "learning_rate": 4.008168153303921e-05,
      "loss": 0.0896,
      "step": 31250
    },
    {
      "epoch": 1.1711812970664268,
      "grad_norm": 0.06710313260555267,
      "learning_rate": 4.0051324725915204e-05,
      "loss": 0.0881,
      "step": 31260
    },
    {
      "epoch": 1.1715559551908883,
      "grad_norm": 0.06052746996283531,
      "learning_rate": 4.00209717367073e-05,
      "loss": 0.0891,
      "step": 31270
    },
    {
      "epoch": 1.1719306133153498,
      "grad_norm": 0.10813681036233902,
      "learning_rate": 3.999062257706381e-05,
      "loss": 0.091,
      "step": 31280
    },
    {
      "epoch": 1.1723052714398112,
      "grad_norm": 0.10963317006826401,
      "learning_rate": 3.9960277258631585e-05,
      "loss": 0.0888,
      "step": 31290
    },
    {
      "epoch": 1.1726799295642727,
      "grad_norm": 0.07907301932573318,
      "learning_rate": 3.992993579305595e-05,
      "loss": 0.0909,
      "step": 31300
    },
    {
      "epoch": 1.173054587688734,
      "grad_norm": 0.07123666256666183,
      "learning_rate": 3.989959819198081e-05,
      "loss": 0.0944,
      "step": 31310
    },
    {
      "epoch": 1.1734292458131954,
      "grad_norm": 0.05405721813440323,
      "learning_rate": 3.986926446704853e-05,
      "loss": 0.0918,
      "step": 31320
    },
    {
      "epoch": 1.1738039039376569,
      "grad_norm": 0.06546928733587265,
      "learning_rate": 3.9838934629900045e-05,
      "loss": 0.0892,
      "step": 31330
    },
    {
      "epoch": 1.1741785620621183,
      "grad_norm": 0.07850027829408646,
      "learning_rate": 3.980860869217478e-05,
      "loss": 0.0892,
      "step": 31340
    },
    {
      "epoch": 1.1745532201865798,
      "grad_norm": 0.08608584105968475,
      "learning_rate": 3.9778286665510615e-05,
      "loss": 0.0904,
      "step": 31350
    },
    {
      "epoch": 1.174927878311041,
      "grad_norm": 0.07281818985939026,
      "learning_rate": 3.974796856154401e-05,
      "loss": 0.0879,
      "step": 31360
    },
    {
      "epoch": 1.1753025364355025,
      "grad_norm": 0.14841815829277039,
      "learning_rate": 3.9717654391909834e-05,
      "loss": 0.0908,
      "step": 31370
    },
    {
      "epoch": 1.175677194559964,
      "grad_norm": 0.1312238723039627,
      "learning_rate": 3.9687344168241535e-05,
      "loss": 0.0881,
      "step": 31380
    },
    {
      "epoch": 1.1760518526844255,
      "grad_norm": 0.16905754804611206,
      "learning_rate": 3.965703790217099e-05,
      "loss": 0.0899,
      "step": 31390
    },
    {
      "epoch": 1.176426510808887,
      "grad_norm": 0.08147914707660675,
      "learning_rate": 3.9626735605328566e-05,
      "loss": 0.0915,
      "step": 31400
    },
    {
      "epoch": 1.1768011689333484,
      "grad_norm": 0.06967146694660187,
      "learning_rate": 3.9596437289343094e-05,
      "loss": 0.0891,
      "step": 31410
    },
    {
      "epoch": 1.1771758270578097,
      "grad_norm": 0.07325786352157593,
      "learning_rate": 3.956614296584192e-05,
      "loss": 0.0907,
      "step": 31420
    },
    {
      "epoch": 1.1775504851822711,
      "grad_norm": 0.08206368237733841,
      "learning_rate": 3.9535852646450826e-05,
      "loss": 0.0887,
      "step": 31430
    },
    {
      "epoch": 1.1779251433067326,
      "grad_norm": 0.08403214067220688,
      "learning_rate": 3.9505566342794055e-05,
      "loss": 0.0898,
      "step": 31440
    },
    {
      "epoch": 1.178299801431194,
      "grad_norm": 0.06465160101652145,
      "learning_rate": 3.9475284066494323e-05,
      "loss": 0.0912,
      "step": 31450
    },
    {
      "epoch": 1.1786744595556555,
      "grad_norm": 0.06237250566482544,
      "learning_rate": 3.9445005829172776e-05,
      "loss": 0.0909,
      "step": 31460
    },
    {
      "epoch": 1.1790491176801168,
      "grad_norm": 0.09177380800247192,
      "learning_rate": 3.941473164244905e-05,
      "loss": 0.089,
      "step": 31470
    },
    {
      "epoch": 1.1794237758045782,
      "grad_norm": 0.07474765926599503,
      "learning_rate": 3.9384461517941215e-05,
      "loss": 0.087,
      "step": 31480
    },
    {
      "epoch": 1.1797984339290397,
      "grad_norm": 0.07107377052307129,
      "learning_rate": 3.9354195467265735e-05,
      "loss": 0.0908,
      "step": 31490
    },
    {
      "epoch": 1.1801730920535012,
      "grad_norm": 0.11854767799377441,
      "learning_rate": 3.932393350203756e-05,
      "loss": 0.0889,
      "step": 31500
    },
    {
      "epoch": 1.1805477501779627,
      "grad_norm": 0.08093275129795074,
      "learning_rate": 3.92936756338701e-05,
      "loss": 0.0927,
      "step": 31510
    },
    {
      "epoch": 1.1809224083024241,
      "grad_norm": 0.07425902038812637,
      "learning_rate": 3.92634218743751e-05,
      "loss": 0.0928,
      "step": 31520
    },
    {
      "epoch": 1.1812970664268856,
      "grad_norm": 0.09563053399324417,
      "learning_rate": 3.923317223516281e-05,
      "loss": 0.0902,
      "step": 31530
    },
    {
      "epoch": 1.1816717245513468,
      "grad_norm": 0.08360153436660767,
      "learning_rate": 3.920292672784186e-05,
      "loss": 0.0912,
      "step": 31540
    },
    {
      "epoch": 1.1820463826758083,
      "grad_norm": 0.07363007962703705,
      "learning_rate": 3.9172685364019284e-05,
      "loss": 0.092,
      "step": 31550
    },
    {
      "epoch": 1.1824210408002698,
      "grad_norm": 0.06307113915681839,
      "learning_rate": 3.9142448155300595e-05,
      "loss": 0.0885,
      "step": 31560
    },
    {
      "epoch": 1.1827956989247312,
      "grad_norm": 0.09954673796892166,
      "learning_rate": 3.9112215113289616e-05,
      "loss": 0.0907,
      "step": 31570
    },
    {
      "epoch": 1.1831703570491925,
      "grad_norm": 0.0868898406624794,
      "learning_rate": 3.908198624958865e-05,
      "loss": 0.0857,
      "step": 31580
    },
    {
      "epoch": 1.183545015173654,
      "grad_norm": 0.0821545198559761,
      "learning_rate": 3.905176157579833e-05,
      "loss": 0.0905,
      "step": 31590
    },
    {
      "epoch": 1.1839196732981154,
      "grad_norm": 0.08817776292562485,
      "learning_rate": 3.902154110351773e-05,
      "loss": 0.0888,
      "step": 31600
    },
    {
      "epoch": 1.184294331422577,
      "grad_norm": 0.07962010055780411,
      "learning_rate": 3.899132484434431e-05,
      "loss": 0.0897,
      "step": 31610
    },
    {
      "epoch": 1.1846689895470384,
      "grad_norm": 0.07093688100576401,
      "learning_rate": 3.89611128098739e-05,
      "loss": 0.0901,
      "step": 31620
    },
    {
      "epoch": 1.1850436476714998,
      "grad_norm": 0.06791817396879196,
      "learning_rate": 3.893090501170068e-05,
      "loss": 0.0883,
      "step": 31630
    },
    {
      "epoch": 1.1854183057959613,
      "grad_norm": 0.07739667594432831,
      "learning_rate": 3.890070146141724e-05,
      "loss": 0.0894,
      "step": 31640
    },
    {
      "epoch": 1.1857929639204225,
      "grad_norm": 0.10121949762105942,
      "learning_rate": 3.887050217061457e-05,
      "loss": 0.0917,
      "step": 31650
    },
    {
      "epoch": 1.186167622044884,
      "grad_norm": 0.07069656252861023,
      "learning_rate": 3.8840307150881946e-05,
      "loss": 0.0897,
      "step": 31660
    },
    {
      "epoch": 1.1865422801693455,
      "grad_norm": 0.06311337649822235,
      "learning_rate": 3.881011641380707e-05,
      "loss": 0.0895,
      "step": 31670
    },
    {
      "epoch": 1.186916938293807,
      "grad_norm": 0.11123790591955185,
      "learning_rate": 3.877992997097596e-05,
      "loss": 0.0934,
      "step": 31680
    },
    {
      "epoch": 1.1872915964182684,
      "grad_norm": 0.08818762749433517,
      "learning_rate": 3.8749747833972995e-05,
      "loss": 0.0911,
      "step": 31690
    },
    {
      "epoch": 1.1876662545427297,
      "grad_norm": 0.10068196803331375,
      "learning_rate": 3.871957001438095e-05,
      "loss": 0.0919,
      "step": 31700
    },
    {
      "epoch": 1.1880409126671911,
      "grad_norm": 0.06795347481966019,
      "learning_rate": 3.8689396523780866e-05,
      "loss": 0.0871,
      "step": 31710
    },
    {
      "epoch": 1.1884155707916526,
      "grad_norm": 0.07881604880094528,
      "learning_rate": 3.865922737375219e-05,
      "loss": 0.088,
      "step": 31720
    },
    {
      "epoch": 1.188790228916114,
      "grad_norm": 0.11347115784883499,
      "learning_rate": 3.862906257587264e-05,
      "loss": 0.0947,
      "step": 31730
    },
    {
      "epoch": 1.1891648870405755,
      "grad_norm": 0.07061087340116501,
      "learning_rate": 3.8598902141718295e-05,
      "loss": 0.0853,
      "step": 31740
    },
    {
      "epoch": 1.189539545165037,
      "grad_norm": 0.10605960339307785,
      "learning_rate": 3.856874608286361e-05,
      "loss": 0.0919,
      "step": 31750
    },
    {
      "epoch": 1.1899142032894983,
      "grad_norm": 0.09550728648900986,
      "learning_rate": 3.8538594410881264e-05,
      "loss": 0.0902,
      "step": 31760
    },
    {
      "epoch": 1.1902888614139597,
      "grad_norm": 0.06423965841531754,
      "learning_rate": 3.850844713734233e-05,
      "loss": 0.0921,
      "step": 31770
    },
    {
      "epoch": 1.1906635195384212,
      "grad_norm": 0.09409255534410477,
      "learning_rate": 3.8478304273816124e-05,
      "loss": 0.0932,
      "step": 31780
    },
    {
      "epoch": 1.1910381776628827,
      "grad_norm": 0.10761313885450363,
      "learning_rate": 3.8448165831870346e-05,
      "loss": 0.0868,
      "step": 31790
    },
    {
      "epoch": 1.1914128357873441,
      "grad_norm": 0.08749236911535263,
      "learning_rate": 3.841803182307096e-05,
      "loss": 0.093,
      "step": 31800
    },
    {
      "epoch": 1.1917874939118054,
      "grad_norm": 0.07523707300424576,
      "learning_rate": 3.8387902258982225e-05,
      "loss": 0.0829,
      "step": 31810
    },
    {
      "epoch": 1.1921621520362669,
      "grad_norm": 0.08881516754627228,
      "learning_rate": 3.8357777151166695e-05,
      "loss": 0.091,
      "step": 31820
    },
    {
      "epoch": 1.1925368101607283,
      "grad_norm": 0.0947294756770134,
      "learning_rate": 3.832765651118519e-05,
      "loss": 0.0976,
      "step": 31830
    },
    {
      "epoch": 1.1929114682851898,
      "grad_norm": 0.07028288394212723,
      "learning_rate": 3.8297540350596925e-05,
      "loss": 0.0925,
      "step": 31840
    },
    {
      "epoch": 1.1932861264096513,
      "grad_norm": 0.15098895132541656,
      "learning_rate": 3.826742868095924e-05,
      "loss": 0.0909,
      "step": 31850
    },
    {
      "epoch": 1.1936607845341127,
      "grad_norm": 0.07615870237350464,
      "learning_rate": 3.823732151382787e-05,
      "loss": 0.0922,
      "step": 31860
    },
    {
      "epoch": 1.194035442658574,
      "grad_norm": 0.07256665825843811,
      "learning_rate": 3.8207218860756756e-05,
      "loss": 0.0921,
      "step": 31870
    },
    {
      "epoch": 1.1944101007830354,
      "grad_norm": 0.07780462503433228,
      "learning_rate": 3.817712073329812e-05,
      "loss": 0.0944,
      "step": 31880
    },
    {
      "epoch": 1.194784758907497,
      "grad_norm": 0.11081872880458832,
      "learning_rate": 3.81470271430025e-05,
      "loss": 0.0899,
      "step": 31890
    },
    {
      "epoch": 1.1951594170319584,
      "grad_norm": 0.07570339739322662,
      "learning_rate": 3.811693810141862e-05,
      "loss": 0.0905,
      "step": 31900
    },
    {
      "epoch": 1.1955340751564199,
      "grad_norm": 0.13920754194259644,
      "learning_rate": 3.8086853620093496e-05,
      "loss": 0.0873,
      "step": 31910
    },
    {
      "epoch": 1.195908733280881,
      "grad_norm": 0.08127898722887039,
      "learning_rate": 3.8056773710572366e-05,
      "loss": 0.0916,
      "step": 31920
    },
    {
      "epoch": 1.1962833914053426,
      "grad_norm": 0.09122756868600845,
      "learning_rate": 3.802669838439876e-05,
      "loss": 0.0882,
      "step": 31930
    },
    {
      "epoch": 1.196658049529804,
      "grad_norm": 0.09610359370708466,
      "learning_rate": 3.7996627653114426e-05,
      "loss": 0.0895,
      "step": 31940
    },
    {
      "epoch": 1.1970327076542655,
      "grad_norm": 0.06061447784304619,
      "learning_rate": 3.796656152825932e-05,
      "loss": 0.0849,
      "step": 31950
    },
    {
      "epoch": 1.197407365778727,
      "grad_norm": 0.0740044042468071,
      "learning_rate": 3.793650002137168e-05,
      "loss": 0.0953,
      "step": 31960
    },
    {
      "epoch": 1.1977820239031884,
      "grad_norm": 0.10357096791267395,
      "learning_rate": 3.790644314398792e-05,
      "loss": 0.0913,
      "step": 31970
    },
    {
      "epoch": 1.1981566820276497,
      "grad_norm": 0.07629784196615219,
      "learning_rate": 3.787639090764274e-05,
      "loss": 0.0899,
      "step": 31980
    },
    {
      "epoch": 1.1985313401521112,
      "grad_norm": 0.1306813657283783,
      "learning_rate": 3.7846343323869005e-05,
      "loss": 0.0886,
      "step": 31990
    },
    {
      "epoch": 1.1989059982765726,
      "grad_norm": 0.07342863827943802,
      "learning_rate": 3.78163004041978e-05,
      "loss": 0.0868,
      "step": 32000
    },
    {
      "epoch": 1.199280656401034,
      "grad_norm": 0.08052735775709152,
      "learning_rate": 3.778626216015847e-05,
      "loss": 0.0933,
      "step": 32010
    },
    {
      "epoch": 1.1996553145254956,
      "grad_norm": 0.06581825017929077,
      "learning_rate": 3.775622860327849e-05,
      "loss": 0.092,
      "step": 32020
    },
    {
      "epoch": 1.2000299726499568,
      "grad_norm": 0.08942234516143799,
      "learning_rate": 3.772619974508359e-05,
      "loss": 0.0938,
      "step": 32030
    },
    {
      "epoch": 1.2004046307744183,
      "grad_norm": 0.0711929202079773,
      "learning_rate": 3.769617559709771e-05,
      "loss": 0.0892,
      "step": 32040
    },
    {
      "epoch": 1.2007792888988797,
      "grad_norm": 0.08489544689655304,
      "learning_rate": 3.76661561708429e-05,
      "loss": 0.0929,
      "step": 32050
    },
    {
      "epoch": 1.2011539470233412,
      "grad_norm": 0.0985364094376564,
      "learning_rate": 3.763614147783949e-05,
      "loss": 0.0884,
      "step": 32060
    },
    {
      "epoch": 1.2015286051478027,
      "grad_norm": 0.09291864931583405,
      "learning_rate": 3.760613152960595e-05,
      "loss": 0.0906,
      "step": 32070
    },
    {
      "epoch": 1.2019032632722642,
      "grad_norm": 0.09983434528112411,
      "learning_rate": 3.7576126337658956e-05,
      "loss": 0.0894,
      "step": 32080
    },
    {
      "epoch": 1.2022779213967254,
      "grad_norm": 0.0746440589427948,
      "learning_rate": 3.7546125913513295e-05,
      "loss": 0.0918,
      "step": 32090
    },
    {
      "epoch": 1.2026525795211869,
      "grad_norm": 0.06848149746656418,
      "learning_rate": 3.7516130268682e-05,
      "loss": 0.091,
      "step": 32100
    },
    {
      "epoch": 1.2030272376456483,
      "grad_norm": 0.10130771994590759,
      "learning_rate": 3.7486139414676215e-05,
      "loss": 0.0883,
      "step": 32110
    },
    {
      "epoch": 1.2034018957701098,
      "grad_norm": 0.10738897323608398,
      "learning_rate": 3.745615336300529e-05,
      "loss": 0.0874,
      "step": 32120
    },
    {
      "epoch": 1.2037765538945713,
      "grad_norm": 0.09478239715099335,
      "learning_rate": 3.74261721251767e-05,
      "loss": 0.0907,
      "step": 32130
    },
    {
      "epoch": 1.2041512120190325,
      "grad_norm": 0.07439675182104111,
      "learning_rate": 3.739619571269607e-05,
      "loss": 0.086,
      "step": 32140
    },
    {
      "epoch": 1.204525870143494,
      "grad_norm": 0.0916016548871994,
      "learning_rate": 3.736622413706719e-05,
      "loss": 0.0933,
      "step": 32150
    },
    {
      "epoch": 1.2049005282679555,
      "grad_norm": 0.0797230526804924,
      "learning_rate": 3.733625740979202e-05,
      "loss": 0.0901,
      "step": 32160
    },
    {
      "epoch": 1.205275186392417,
      "grad_norm": 0.07888032495975494,
      "learning_rate": 3.73062955423706e-05,
      "loss": 0.0955,
      "step": 32170
    },
    {
      "epoch": 1.2056498445168784,
      "grad_norm": 0.09454888850450516,
      "learning_rate": 3.7276338546301134e-05,
      "loss": 0.0918,
      "step": 32180
    },
    {
      "epoch": 1.2060245026413399,
      "grad_norm": 0.06762398779392242,
      "learning_rate": 3.724638643307996e-05,
      "loss": 0.0908,
      "step": 32190
    },
    {
      "epoch": 1.2063991607658011,
      "grad_norm": 0.08917983621358871,
      "learning_rate": 3.721643921420153e-05,
      "loss": 0.0902,
      "step": 32200
    },
    {
      "epoch": 1.2067738188902626,
      "grad_norm": 0.06546951085329056,
      "learning_rate": 3.7186496901158464e-05,
      "loss": 0.0906,
      "step": 32210
    },
    {
      "epoch": 1.207148477014724,
      "grad_norm": 0.092012420296669,
      "learning_rate": 3.7156559505441416e-05,
      "loss": 0.0896,
      "step": 32220
    },
    {
      "epoch": 1.2075231351391855,
      "grad_norm": 0.09567669034004211,
      "learning_rate": 3.712662703853923e-05,
      "loss": 0.0903,
      "step": 32230
    },
    {
      "epoch": 1.207897793263647,
      "grad_norm": 0.07179423421621323,
      "learning_rate": 3.70966995119388e-05,
      "loss": 0.09,
      "step": 32240
    },
    {
      "epoch": 1.2082724513881082,
      "grad_norm": 0.07811272889375687,
      "learning_rate": 3.706677693712516e-05,
      "loss": 0.0918,
      "step": 32250
    },
    {
      "epoch": 1.2086471095125697,
      "grad_norm": 0.08602157235145569,
      "learning_rate": 3.703685932558146e-05,
      "loss": 0.0889,
      "step": 32260
    },
    {
      "epoch": 1.2090217676370312,
      "grad_norm": 0.07630555331707001,
      "learning_rate": 3.700694668878888e-05,
      "loss": 0.0883,
      "step": 32270
    },
    {
      "epoch": 1.2093964257614926,
      "grad_norm": 0.07210264354944229,
      "learning_rate": 3.6977039038226775e-05,
      "loss": 0.0905,
      "step": 32280
    },
    {
      "epoch": 1.2097710838859541,
      "grad_norm": 0.10710694640874863,
      "learning_rate": 3.694713638537248e-05,
      "loss": 0.0889,
      "step": 32290
    },
    {
      "epoch": 1.2101457420104156,
      "grad_norm": 0.1526346206665039,
      "learning_rate": 3.691723874170155e-05,
      "loss": 0.0902,
      "step": 32300
    },
    {
      "epoch": 1.2105204001348768,
      "grad_norm": 0.07172218710184097,
      "learning_rate": 3.688734611868749e-05,
      "loss": 0.0911,
      "step": 32310
    },
    {
      "epoch": 1.2108950582593383,
      "grad_norm": 0.06189025193452835,
      "learning_rate": 3.685745852780196e-05,
      "loss": 0.0879,
      "step": 32320
    },
    {
      "epoch": 1.2112697163837998,
      "grad_norm": 0.09179942309856415,
      "learning_rate": 3.682757598051463e-05,
      "loss": 0.0903,
      "step": 32330
    },
    {
      "epoch": 1.2116443745082612,
      "grad_norm": 0.0864240974187851,
      "learning_rate": 3.6797698488293274e-05,
      "loss": 0.0891,
      "step": 32340
    },
    {
      "epoch": 1.2120190326327227,
      "grad_norm": 0.09567706286907196,
      "learning_rate": 3.6767826062603746e-05,
      "loss": 0.0912,
      "step": 32350
    },
    {
      "epoch": 1.212393690757184,
      "grad_norm": 0.059740107506513596,
      "learning_rate": 3.673795871490988e-05,
      "loss": 0.0896,
      "step": 32360
    },
    {
      "epoch": 1.2127683488816454,
      "grad_norm": 0.06662248075008392,
      "learning_rate": 3.6708096456673644e-05,
      "loss": 0.0873,
      "step": 32370
    },
    {
      "epoch": 1.2131430070061069,
      "grad_norm": 0.10523808747529984,
      "learning_rate": 3.667823929935499e-05,
      "loss": 0.0867,
      "step": 32380
    },
    {
      "epoch": 1.2135176651305684,
      "grad_norm": 0.09823793917894363,
      "learning_rate": 3.664838725441194e-05,
      "loss": 0.09,
      "step": 32390
    },
    {
      "epoch": 1.2138923232550298,
      "grad_norm": 0.0802806094288826,
      "learning_rate": 3.661854033330058e-05,
      "loss": 0.091,
      "step": 32400
    },
    {
      "epoch": 1.2142669813794913,
      "grad_norm": 0.07901094108819962,
      "learning_rate": 3.658869854747497e-05,
      "loss": 0.0897,
      "step": 32410
    },
    {
      "epoch": 1.2146416395039528,
      "grad_norm": 0.0779164656996727,
      "learning_rate": 3.655886190838725e-05,
      "loss": 0.0877,
      "step": 32420
    },
    {
      "epoch": 1.215016297628414,
      "grad_norm": 0.07140128314495087,
      "learning_rate": 3.6529030427487544e-05,
      "loss": 0.0911,
      "step": 32430
    },
    {
      "epoch": 1.2153909557528755,
      "grad_norm": 0.07243568450212479,
      "learning_rate": 3.649920411622404e-05,
      "loss": 0.0919,
      "step": 32440
    },
    {
      "epoch": 1.215765613877337,
      "grad_norm": 0.08507747203111649,
      "learning_rate": 3.6469382986042923e-05,
      "loss": 0.0934,
      "step": 32450
    },
    {
      "epoch": 1.2161402720017984,
      "grad_norm": 0.07240138947963715,
      "learning_rate": 3.643956704838837e-05,
      "loss": 0.094,
      "step": 32460
    },
    {
      "epoch": 1.2165149301262597,
      "grad_norm": 0.08610785007476807,
      "learning_rate": 3.64097563147026e-05,
      "loss": 0.0874,
      "step": 32470
    },
    {
      "epoch": 1.2168895882507211,
      "grad_norm": 0.07409346103668213,
      "learning_rate": 3.637995079642579e-05,
      "loss": 0.0892,
      "step": 32480
    },
    {
      "epoch": 1.2172642463751826,
      "grad_norm": 0.07244069874286652,
      "learning_rate": 3.635015050499617e-05,
      "loss": 0.0864,
      "step": 32490
    },
    {
      "epoch": 1.217638904499644,
      "grad_norm": 0.10085097700357437,
      "learning_rate": 3.632035545184995e-05,
      "loss": 0.0922,
      "step": 32500
    },
    {
      "epoch": 1.2180135626241055,
      "grad_norm": 0.06751268357038498,
      "learning_rate": 3.629056564842128e-05,
      "loss": 0.0919,
      "step": 32510
    },
    {
      "epoch": 1.218388220748567,
      "grad_norm": 0.06988557428121567,
      "learning_rate": 3.626078110614235e-05,
      "loss": 0.0897,
      "step": 32520
    },
    {
      "epoch": 1.2187628788730285,
      "grad_norm": 0.06857023388147354,
      "learning_rate": 3.623100183644329e-05,
      "loss": 0.0911,
      "step": 32530
    },
    {
      "epoch": 1.2191375369974897,
      "grad_norm": 0.08147576451301575,
      "learning_rate": 3.6201227850752285e-05,
      "loss": 0.0901,
      "step": 32540
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 0.09558428823947906,
      "learning_rate": 3.617145916049539e-05,
      "loss": 0.093,
      "step": 32550
    },
    {
      "epoch": 1.2198868532464127,
      "grad_norm": 0.07884316146373749,
      "learning_rate": 3.61416957770967e-05,
      "loss": 0.0914,
      "step": 32560
    },
    {
      "epoch": 1.2202615113708741,
      "grad_norm": 0.06084185466170311,
      "learning_rate": 3.6111937711978216e-05,
      "loss": 0.0935,
      "step": 32570
    },
    {
      "epoch": 1.2206361694953356,
      "grad_norm": 0.11401309818029404,
      "learning_rate": 3.608218497655996e-05,
      "loss": 0.0896,
      "step": 32580
    },
    {
      "epoch": 1.2210108276197968,
      "grad_norm": 0.06512154638767242,
      "learning_rate": 3.605243758225989e-05,
      "loss": 0.0886,
      "step": 32590
    },
    {
      "epoch": 1.2213854857442583,
      "grad_norm": 0.06844067573547363,
      "learning_rate": 3.602269554049387e-05,
      "loss": 0.0927,
      "step": 32600
    },
    {
      "epoch": 1.2217601438687198,
      "grad_norm": 0.10334677994251251,
      "learning_rate": 3.5992958862675766e-05,
      "loss": 0.0903,
      "step": 32610
    },
    {
      "epoch": 1.2221348019931813,
      "grad_norm": 0.07530279457569122,
      "learning_rate": 3.5963227560217334e-05,
      "loss": 0.0903,
      "step": 32620
    },
    {
      "epoch": 1.2225094601176427,
      "grad_norm": 0.05160689353942871,
      "learning_rate": 3.593350164452832e-05,
      "loss": 0.089,
      "step": 32630
    },
    {
      "epoch": 1.2228841182421042,
      "grad_norm": 0.13669836521148682,
      "learning_rate": 3.59037811270164e-05,
      "loss": 0.0915,
      "step": 32640
    },
    {
      "epoch": 1.2232587763665654,
      "grad_norm": 0.07129450142383575,
      "learning_rate": 3.587406601908711e-05,
      "loss": 0.0856,
      "step": 32650
    },
    {
      "epoch": 1.223633434491027,
      "grad_norm": 0.07462829351425171,
      "learning_rate": 3.5844356332144e-05,
      "loss": 0.0878,
      "step": 32660
    },
    {
      "epoch": 1.2240080926154884,
      "grad_norm": 0.0777648389339447,
      "learning_rate": 3.5814652077588445e-05,
      "loss": 0.0927,
      "step": 32670
    },
    {
      "epoch": 1.2243827507399498,
      "grad_norm": 0.06401731818914413,
      "learning_rate": 3.578495326681983e-05,
      "loss": 0.0896,
      "step": 32680
    },
    {
      "epoch": 1.2247574088644113,
      "grad_norm": 0.08985690027475357,
      "learning_rate": 3.575525991123541e-05,
      "loss": 0.0925,
      "step": 32690
    },
    {
      "epoch": 1.2251320669888726,
      "grad_norm": 0.0835835188627243,
      "learning_rate": 3.5725572022230324e-05,
      "loss": 0.0907,
      "step": 32700
    },
    {
      "epoch": 1.225506725113334,
      "grad_norm": 0.058637212961912155,
      "learning_rate": 3.569588961119762e-05,
      "loss": 0.0908,
      "step": 32710
    },
    {
      "epoch": 1.2258813832377955,
      "grad_norm": 0.07080871611833572,
      "learning_rate": 3.5666212689528296e-05,
      "loss": 0.0896,
      "step": 32720
    },
    {
      "epoch": 1.226256041362257,
      "grad_norm": 0.05858456343412399,
      "learning_rate": 3.563654126861119e-05,
      "loss": 0.0925,
      "step": 32730
    },
    {
      "epoch": 1.2266306994867184,
      "grad_norm": 0.0723603218793869,
      "learning_rate": 3.560687535983304e-05,
      "loss": 0.0891,
      "step": 32740
    },
    {
      "epoch": 1.22700535761118,
      "grad_norm": 0.07004596292972565,
      "learning_rate": 3.5577214974578464e-05,
      "loss": 0.0897,
      "step": 32750
    },
    {
      "epoch": 1.2273800157356411,
      "grad_norm": 0.09413179010152817,
      "learning_rate": 3.5547560124229974e-05,
      "loss": 0.0864,
      "step": 32760
    },
    {
      "epoch": 1.2277546738601026,
      "grad_norm": 0.09330329298973083,
      "learning_rate": 3.551791082016796e-05,
      "loss": 0.0886,
      "step": 32770
    },
    {
      "epoch": 1.228129331984564,
      "grad_norm": 0.08114396780729294,
      "learning_rate": 3.5488267073770674e-05,
      "loss": 0.0875,
      "step": 32780
    },
    {
      "epoch": 1.2285039901090256,
      "grad_norm": 0.08316031843423843,
      "learning_rate": 3.545862889641424e-05,
      "loss": 0.0889,
      "step": 32790
    },
    {
      "epoch": 1.228878648233487,
      "grad_norm": 0.08512647449970245,
      "learning_rate": 3.542899629947264e-05,
      "loss": 0.0938,
      "step": 32800
    },
    {
      "epoch": 1.2292533063579483,
      "grad_norm": 0.07727859169244766,
      "learning_rate": 3.5399369294317685e-05,
      "loss": 0.0875,
      "step": 32810
    },
    {
      "epoch": 1.2296279644824097,
      "grad_norm": 0.06923391669988632,
      "learning_rate": 3.536974789231912e-05,
      "loss": 0.0854,
      "step": 32820
    },
    {
      "epoch": 1.2300026226068712,
      "grad_norm": 0.0750831589102745,
      "learning_rate": 3.534013210484448e-05,
      "loss": 0.0901,
      "step": 32830
    },
    {
      "epoch": 1.2303772807313327,
      "grad_norm": 0.0927136242389679,
      "learning_rate": 3.531052194325915e-05,
      "loss": 0.091,
      "step": 32840
    },
    {
      "epoch": 1.2307519388557941,
      "grad_norm": 0.0749223455786705,
      "learning_rate": 3.528091741892633e-05,
      "loss": 0.0908,
      "step": 32850
    },
    {
      "epoch": 1.2311265969802556,
      "grad_norm": 0.08777094632387161,
      "learning_rate": 3.525131854320715e-05,
      "loss": 0.0876,
      "step": 32860
    },
    {
      "epoch": 1.2315012551047169,
      "grad_norm": 0.08457452803850174,
      "learning_rate": 3.5221725327460475e-05,
      "loss": 0.0926,
      "step": 32870
    },
    {
      "epoch": 1.2318759132291783,
      "grad_norm": 0.16893039643764496,
      "learning_rate": 3.5192137783043056e-05,
      "loss": 0.0883,
      "step": 32880
    },
    {
      "epoch": 1.2322505713536398,
      "grad_norm": 0.07675225287675858,
      "learning_rate": 3.5162555921309413e-05,
      "loss": 0.0917,
      "step": 32890
    },
    {
      "epoch": 1.2326252294781013,
      "grad_norm": 0.09179723262786865,
      "learning_rate": 3.513297975361193e-05,
      "loss": 0.0904,
      "step": 32900
    },
    {
      "epoch": 1.2329998876025627,
      "grad_norm": 0.07513550668954849,
      "learning_rate": 3.510340929130083e-05,
      "loss": 0.0888,
      "step": 32910
    },
    {
      "epoch": 1.233374545727024,
      "grad_norm": 0.07729470729827881,
      "learning_rate": 3.5073844545724066e-05,
      "loss": 0.0908,
      "step": 32920
    },
    {
      "epoch": 1.2337492038514855,
      "grad_norm": 0.06162659451365471,
      "learning_rate": 3.5044285528227464e-05,
      "loss": 0.0883,
      "step": 32930
    },
    {
      "epoch": 1.234123861975947,
      "grad_norm": 0.06699353456497192,
      "learning_rate": 3.50147322501546e-05,
      "loss": 0.0886,
      "step": 32940
    },
    {
      "epoch": 1.2344985201004084,
      "grad_norm": 0.08954574912786484,
      "learning_rate": 3.4985184722846934e-05,
      "loss": 0.0942,
      "step": 32950
    },
    {
      "epoch": 1.2348731782248699,
      "grad_norm": 0.09390230476856232,
      "learning_rate": 3.4955642957643615e-05,
      "loss": 0.0902,
      "step": 32960
    },
    {
      "epoch": 1.2352478363493313,
      "grad_norm": 0.07788334786891937,
      "learning_rate": 3.492610696588166e-05,
      "loss": 0.0907,
      "step": 32970
    },
    {
      "epoch": 1.2356224944737926,
      "grad_norm": 0.10387955605983734,
      "learning_rate": 3.4896576758895796e-05,
      "loss": 0.0857,
      "step": 32980
    },
    {
      "epoch": 1.235997152598254,
      "grad_norm": 0.07862727344036102,
      "learning_rate": 3.4867052348018596e-05,
      "loss": 0.0892,
      "step": 32990
    },
    {
      "epoch": 1.2363718107227155,
      "grad_norm": 0.08151721954345703,
      "learning_rate": 3.4837533744580406e-05,
      "loss": 0.0923,
      "step": 33000
    },
    {
      "epoch": 1.236746468847177,
      "grad_norm": 0.09538502246141434,
      "learning_rate": 3.4808020959909294e-05,
      "loss": 0.0929,
      "step": 33010
    },
    {
      "epoch": 1.2371211269716385,
      "grad_norm": 0.0727861300110817,
      "learning_rate": 3.477851400533114e-05,
      "loss": 0.0909,
      "step": 33020
    },
    {
      "epoch": 1.2374957850960997,
      "grad_norm": 0.0797029361128807,
      "learning_rate": 3.474901289216955e-05,
      "loss": 0.0947,
      "step": 33030
    },
    {
      "epoch": 1.2378704432205612,
      "grad_norm": 0.07303114980459213,
      "learning_rate": 3.4719517631745915e-05,
      "loss": 0.0858,
      "step": 33040
    },
    {
      "epoch": 1.2382451013450226,
      "grad_norm": 0.06478211283683777,
      "learning_rate": 3.469002823537939e-05,
      "loss": 0.0888,
      "step": 33050
    },
    {
      "epoch": 1.238619759469484,
      "grad_norm": 0.08356049656867981,
      "learning_rate": 3.466054471438685e-05,
      "loss": 0.0903,
      "step": 33060
    },
    {
      "epoch": 1.2389944175939456,
      "grad_norm": 0.09070723503828049,
      "learning_rate": 3.463106708008294e-05,
      "loss": 0.0911,
      "step": 33070
    },
    {
      "epoch": 1.239369075718407,
      "grad_norm": 0.06357154995203018,
      "learning_rate": 3.4601595343779994e-05,
      "loss": 0.0908,
      "step": 33080
    },
    {
      "epoch": 1.2397437338428683,
      "grad_norm": 0.08946830034255981,
      "learning_rate": 3.457212951678816e-05,
      "loss": 0.0893,
      "step": 33090
    },
    {
      "epoch": 1.2401183919673298,
      "grad_norm": 0.07158755511045456,
      "learning_rate": 3.454266961041529e-05,
      "loss": 0.0904,
      "step": 33100
    },
    {
      "epoch": 1.2404930500917912,
      "grad_norm": 0.08708677440881729,
      "learning_rate": 3.451321563596692e-05,
      "loss": 0.0878,
      "step": 33110
    },
    {
      "epoch": 1.2408677082162527,
      "grad_norm": 0.06901032477617264,
      "learning_rate": 3.4483767604746376e-05,
      "loss": 0.0909,
      "step": 33120
    },
    {
      "epoch": 1.2412423663407142,
      "grad_norm": 0.07693276554346085,
      "learning_rate": 3.445432552805463e-05,
      "loss": 0.0899,
      "step": 33130
    },
    {
      "epoch": 1.2416170244651754,
      "grad_norm": 0.1013609990477562,
      "learning_rate": 3.442488941719045e-05,
      "loss": 0.0904,
      "step": 33140
    },
    {
      "epoch": 1.2419916825896369,
      "grad_norm": 0.0769379734992981,
      "learning_rate": 3.439545928345028e-05,
      "loss": 0.0927,
      "step": 33150
    },
    {
      "epoch": 1.2423663407140983,
      "grad_norm": 0.08030357211828232,
      "learning_rate": 3.436603513812823e-05,
      "loss": 0.0919,
      "step": 33160
    },
    {
      "epoch": 1.2427409988385598,
      "grad_norm": 0.10894057899713516,
      "learning_rate": 3.433661699251617e-05,
      "loss": 0.0898,
      "step": 33170
    },
    {
      "epoch": 1.2431156569630213,
      "grad_norm": 0.06645671278238297,
      "learning_rate": 3.4307204857903606e-05,
      "loss": 0.0916,
      "step": 33180
    },
    {
      "epoch": 1.2434903150874828,
      "grad_norm": 0.07237027585506439,
      "learning_rate": 3.427779874557784e-05,
      "loss": 0.0918,
      "step": 33190
    },
    {
      "epoch": 1.2438649732119442,
      "grad_norm": 0.10480530560016632,
      "learning_rate": 3.4248398666823764e-05,
      "loss": 0.0922,
      "step": 33200
    },
    {
      "epoch": 1.2442396313364055,
      "grad_norm": 0.08670361340045929,
      "learning_rate": 3.4219004632923994e-05,
      "loss": 0.09,
      "step": 33210
    },
    {
      "epoch": 1.244614289460867,
      "grad_norm": 0.0770636647939682,
      "learning_rate": 3.41896166551588e-05,
      "loss": 0.0922,
      "step": 33220
    },
    {
      "epoch": 1.2449889475853284,
      "grad_norm": 0.0967545211315155,
      "learning_rate": 3.416023474480618e-05,
      "loss": 0.0887,
      "step": 33230
    },
    {
      "epoch": 1.2453636057097899,
      "grad_norm": 0.07233212143182755,
      "learning_rate": 3.4130858913141775e-05,
      "loss": 0.088,
      "step": 33240
    },
    {
      "epoch": 1.2457382638342511,
      "grad_norm": 0.08142668753862381,
      "learning_rate": 3.410148917143886e-05,
      "loss": 0.0886,
      "step": 33250
    },
    {
      "epoch": 1.2461129219587126,
      "grad_norm": 0.09221422672271729,
      "learning_rate": 3.407212553096843e-05,
      "loss": 0.0892,
      "step": 33260
    },
    {
      "epoch": 1.246487580083174,
      "grad_norm": 0.09096401184797287,
      "learning_rate": 3.4042768002999084e-05,
      "loss": 0.0929,
      "step": 33270
    },
    {
      "epoch": 1.2468622382076355,
      "grad_norm": 0.07728823274374008,
      "learning_rate": 3.401341659879714e-05,
      "loss": 0.0897,
      "step": 33280
    },
    {
      "epoch": 1.247236896332097,
      "grad_norm": 0.1218564435839653,
      "learning_rate": 3.3984071329626516e-05,
      "loss": 0.0906,
      "step": 33290
    },
    {
      "epoch": 1.2476115544565585,
      "grad_norm": 0.07349452376365662,
      "learning_rate": 3.395473220674878e-05,
      "loss": 0.09,
      "step": 33300
    },
    {
      "epoch": 1.24798621258102,
      "grad_norm": 0.07997091859579086,
      "learning_rate": 3.392539924142316e-05,
      "loss": 0.0913,
      "step": 33310
    },
    {
      "epoch": 1.2483608707054812,
      "grad_norm": 0.078663170337677,
      "learning_rate": 3.3896072444906494e-05,
      "loss": 0.0939,
      "step": 33320
    },
    {
      "epoch": 1.2487355288299427,
      "grad_norm": 0.09690384566783905,
      "learning_rate": 3.38667518284533e-05,
      "loss": 0.0935,
      "step": 33330
    },
    {
      "epoch": 1.2491101869544041,
      "grad_norm": 0.11791382730007172,
      "learning_rate": 3.383743740331568e-05,
      "loss": 0.0912,
      "step": 33340
    },
    {
      "epoch": 1.2494848450788656,
      "grad_norm": 0.09205150604248047,
      "learning_rate": 3.380812918074337e-05,
      "loss": 0.0939,
      "step": 33350
    },
    {
      "epoch": 1.249859503203327,
      "grad_norm": 0.079743392765522,
      "learning_rate": 3.377882717198372e-05,
      "loss": 0.0875,
      "step": 33360
    },
    {
      "epoch": 1.2502341613277883,
      "grad_norm": 0.06660545617341995,
      "learning_rate": 3.3749531388281744e-05,
      "loss": 0.0894,
      "step": 33370
    },
    {
      "epoch": 1.2506088194522498,
      "grad_norm": 0.0715646892786026,
      "learning_rate": 3.372024184087999e-05,
      "loss": 0.0889,
      "step": 33380
    },
    {
      "epoch": 1.2509834775767112,
      "grad_norm": 0.05571402981877327,
      "learning_rate": 3.369095854101868e-05,
      "loss": 0.0903,
      "step": 33390
    },
    {
      "epoch": 1.2513581357011727,
      "grad_norm": 0.07871781289577484,
      "learning_rate": 3.366168149993558e-05,
      "loss": 0.089,
      "step": 33400
    },
    {
      "epoch": 1.2517327938256342,
      "grad_norm": 0.08792339265346527,
      "learning_rate": 3.36324107288661e-05,
      "loss": 0.0918,
      "step": 33410
    },
    {
      "epoch": 1.2521074519500957,
      "grad_norm": 0.07337728887796402,
      "learning_rate": 3.360314623904323e-05,
      "loss": 0.0878,
      "step": 33420
    },
    {
      "epoch": 1.252482110074557,
      "grad_norm": 0.07423771172761917,
      "learning_rate": 3.357388804169756e-05,
      "loss": 0.0907,
      "step": 33430
    },
    {
      "epoch": 1.2528567681990184,
      "grad_norm": 0.06097671762108803,
      "learning_rate": 3.354463614805722e-05,
      "loss": 0.0884,
      "step": 33440
    },
    {
      "epoch": 1.2532314263234798,
      "grad_norm": 0.0576874278485775,
      "learning_rate": 3.3515390569347995e-05,
      "loss": 0.0862,
      "step": 33450
    },
    {
      "epoch": 1.2536060844479413,
      "grad_norm": 0.10309423506259918,
      "learning_rate": 3.3486151316793145e-05,
      "loss": 0.0898,
      "step": 33460
    },
    {
      "epoch": 1.2539807425724026,
      "grad_norm": 0.09214192628860474,
      "learning_rate": 3.345691840161362e-05,
      "loss": 0.0899,
      "step": 33470
    },
    {
      "epoch": 1.254355400696864,
      "grad_norm": 0.07342002540826797,
      "learning_rate": 3.342769183502786e-05,
      "loss": 0.0863,
      "step": 33480
    },
    {
      "epoch": 1.2547300588213255,
      "grad_norm": 0.07643432170152664,
      "learning_rate": 3.339847162825189e-05,
      "loss": 0.0875,
      "step": 33490
    },
    {
      "epoch": 1.255104716945787,
      "grad_norm": 0.07994361221790314,
      "learning_rate": 3.3369257792499274e-05,
      "loss": 0.0879,
      "step": 33500
    },
    {
      "epoch": 1.2554793750702484,
      "grad_norm": 0.08673728257417679,
      "learning_rate": 3.334005033898119e-05,
      "loss": 0.0861,
      "step": 33510
    },
    {
      "epoch": 1.25585403319471,
      "grad_norm": 0.07653681188821793,
      "learning_rate": 3.3310849278906286e-05,
      "loss": 0.0906,
      "step": 33520
    },
    {
      "epoch": 1.2562286913191714,
      "grad_norm": 0.07017562538385391,
      "learning_rate": 3.328165462348083e-05,
      "loss": 0.0925,
      "step": 33530
    },
    {
      "epoch": 1.2566033494436326,
      "grad_norm": 0.06141365319490433,
      "learning_rate": 3.325246638390858e-05,
      "loss": 0.0958,
      "step": 33540
    },
    {
      "epoch": 1.256978007568094,
      "grad_norm": 0.4366358518600464,
      "learning_rate": 3.322328457139084e-05,
      "loss": 0.0941,
      "step": 33550
    },
    {
      "epoch": 1.2573526656925555,
      "grad_norm": 0.07562234252691269,
      "learning_rate": 3.31941091971265e-05,
      "loss": 0.0904,
      "step": 33560
    },
    {
      "epoch": 1.257727323817017,
      "grad_norm": 0.15694735944271088,
      "learning_rate": 3.31649402723119e-05,
      "loss": 0.0916,
      "step": 33570
    },
    {
      "epoch": 1.2581019819414783,
      "grad_norm": 0.06192260608077049,
      "learning_rate": 3.313577780814097e-05,
      "loss": 0.0912,
      "step": 33580
    },
    {
      "epoch": 1.2584766400659397,
      "grad_norm": 0.0602373369038105,
      "learning_rate": 3.310662181580511e-05,
      "loss": 0.093,
      "step": 33590
    },
    {
      "epoch": 1.2588512981904012,
      "grad_norm": 0.06894414126873016,
      "learning_rate": 3.307747230649326e-05,
      "loss": 0.0903,
      "step": 33600
    },
    {
      "epoch": 1.2592259563148627,
      "grad_norm": 0.08874012529850006,
      "learning_rate": 3.304832929139191e-05,
      "loss": 0.0921,
      "step": 33610
    },
    {
      "epoch": 1.2596006144393241,
      "grad_norm": 0.07525858283042908,
      "learning_rate": 3.3019192781684974e-05,
      "loss": 0.0903,
      "step": 33620
    },
    {
      "epoch": 1.2599752725637856,
      "grad_norm": 0.09637521207332611,
      "learning_rate": 3.299006278855395e-05,
      "loss": 0.0935,
      "step": 33630
    },
    {
      "epoch": 1.260349930688247,
      "grad_norm": 0.09768933802843094,
      "learning_rate": 3.2960939323177755e-05,
      "loss": 0.0907,
      "step": 33640
    },
    {
      "epoch": 1.2607245888127083,
      "grad_norm": 0.07206223160028458,
      "learning_rate": 3.2931822396732915e-05,
      "loss": 0.0858,
      "step": 33650
    },
    {
      "epoch": 1.2610992469371698,
      "grad_norm": 0.07284321635961533,
      "learning_rate": 3.290271202039332e-05,
      "loss": 0.0877,
      "step": 33660
    },
    {
      "epoch": 1.2614739050616313,
      "grad_norm": 0.08767715841531754,
      "learning_rate": 3.287360820533044e-05,
      "loss": 0.0896,
      "step": 33670
    },
    {
      "epoch": 1.2618485631860927,
      "grad_norm": 0.07986859232187271,
      "learning_rate": 3.284451096271317e-05,
      "loss": 0.0902,
      "step": 33680
    },
    {
      "epoch": 1.2622232213105542,
      "grad_norm": 0.09975516051054001,
      "learning_rate": 3.28154203037079e-05,
      "loss": 0.0913,
      "step": 33690
    },
    {
      "epoch": 1.2625978794350154,
      "grad_norm": 0.07558906078338623,
      "learning_rate": 3.278633623947853e-05,
      "loss": 0.0903,
      "step": 33700
    },
    {
      "epoch": 1.262972537559477,
      "grad_norm": 0.09396566450595856,
      "learning_rate": 3.2757258781186365e-05,
      "loss": 0.0865,
      "step": 33710
    },
    {
      "epoch": 1.2633471956839384,
      "grad_norm": 0.09907656162977219,
      "learning_rate": 3.272818793999022e-05,
      "loss": 0.089,
      "step": 33720
    },
    {
      "epoch": 1.2637218538083999,
      "grad_norm": 0.08897767961025238,
      "learning_rate": 3.269912372704634e-05,
      "loss": 0.0898,
      "step": 33730
    },
    {
      "epoch": 1.2640965119328613,
      "grad_norm": 0.08098285645246506,
      "learning_rate": 3.267006615350847e-05,
      "loss": 0.0858,
      "step": 33740
    },
    {
      "epoch": 1.2644711700573228,
      "grad_norm": 0.09639438986778259,
      "learning_rate": 3.264101523052779e-05,
      "loss": 0.0915,
      "step": 33750
    },
    {
      "epoch": 1.2648458281817843,
      "grad_norm": 0.11518307775259018,
      "learning_rate": 3.261197096925288e-05,
      "loss": 0.0919,
      "step": 33760
    },
    {
      "epoch": 1.2652204863062455,
      "grad_norm": 0.06535930931568146,
      "learning_rate": 3.258293338082983e-05,
      "loss": 0.0907,
      "step": 33770
    },
    {
      "epoch": 1.265595144430707,
      "grad_norm": 0.11609995365142822,
      "learning_rate": 3.255390247640211e-05,
      "loss": 0.0877,
      "step": 33780
    },
    {
      "epoch": 1.2659698025551684,
      "grad_norm": 0.06837927550077438,
      "learning_rate": 3.2524878267110706e-05,
      "loss": 0.0877,
      "step": 33790
    },
    {
      "epoch": 1.26634446067963,
      "grad_norm": 0.07253707200288773,
      "learning_rate": 3.2495860764093947e-05,
      "loss": 0.0891,
      "step": 33800
    },
    {
      "epoch": 1.2667191188040912,
      "grad_norm": 0.06476347893476486,
      "learning_rate": 3.246684997848764e-05,
      "loss": 0.091,
      "step": 33810
    },
    {
      "epoch": 1.2670937769285526,
      "grad_norm": 0.06706766039133072,
      "learning_rate": 3.2437845921425004e-05,
      "loss": 0.0892,
      "step": 33820
    },
    {
      "epoch": 1.267468435053014,
      "grad_norm": 0.07956556975841522,
      "learning_rate": 3.240884860403665e-05,
      "loss": 0.0937,
      "step": 33830
    },
    {
      "epoch": 1.2678430931774756,
      "grad_norm": 0.07010229676961899,
      "learning_rate": 3.237985803745064e-05,
      "loss": 0.0895,
      "step": 33840
    },
    {
      "epoch": 1.268217751301937,
      "grad_norm": 0.08354101330041885,
      "learning_rate": 3.235087423279245e-05,
      "loss": 0.0919,
      "step": 33850
    },
    {
      "epoch": 1.2685924094263985,
      "grad_norm": 0.07438471913337708,
      "learning_rate": 3.232189720118491e-05,
      "loss": 0.0899,
      "step": 33860
    },
    {
      "epoch": 1.26896706755086,
      "grad_norm": 0.07664415240287781,
      "learning_rate": 3.2292926953748285e-05,
      "loss": 0.0917,
      "step": 33870
    },
    {
      "epoch": 1.2693417256753212,
      "grad_norm": 0.09259792417287827,
      "learning_rate": 3.226396350160026e-05,
      "loss": 0.0887,
      "step": 33880
    },
    {
      "epoch": 1.2697163837997827,
      "grad_norm": 0.09717255085706711,
      "learning_rate": 3.223500685585587e-05,
      "loss": 0.0878,
      "step": 33890
    },
    {
      "epoch": 1.2700910419242442,
      "grad_norm": 0.0922652930021286,
      "learning_rate": 3.2206057027627544e-05,
      "loss": 0.0932,
      "step": 33900
    },
    {
      "epoch": 1.2704657000487056,
      "grad_norm": 0.08003444969654083,
      "learning_rate": 3.217711402802512e-05,
      "loss": 0.0924,
      "step": 33910
    },
    {
      "epoch": 1.2708403581731669,
      "grad_norm": 0.07240799069404602,
      "learning_rate": 3.214817786815578e-05,
      "loss": 0.0907,
      "step": 33920
    },
    {
      "epoch": 1.2712150162976283,
      "grad_norm": 0.08831744641065598,
      "learning_rate": 3.2119248559124125e-05,
      "loss": 0.0884,
      "step": 33930
    },
    {
      "epoch": 1.2715896744220898,
      "grad_norm": 0.0984240174293518,
      "learning_rate": 3.20903261120321e-05,
      "loss": 0.091,
      "step": 33940
    },
    {
      "epoch": 1.2719643325465513,
      "grad_norm": 0.09162791073322296,
      "learning_rate": 3.2061410537978995e-05,
      "loss": 0.0854,
      "step": 33950
    },
    {
      "epoch": 1.2723389906710127,
      "grad_norm": 0.0680784359574318,
      "learning_rate": 3.203250184806152e-05,
      "loss": 0.0868,
      "step": 33960
    },
    {
      "epoch": 1.2727136487954742,
      "grad_norm": 0.10556219518184662,
      "learning_rate": 3.2003600053373673e-05,
      "loss": 0.0885,
      "step": 33970
    },
    {
      "epoch": 1.2730883069199357,
      "grad_norm": 0.0772055834531784,
      "learning_rate": 3.197470516500687e-05,
      "loss": 0.0892,
      "step": 33980
    },
    {
      "epoch": 1.273462965044397,
      "grad_norm": 0.11696338653564453,
      "learning_rate": 3.1945817194049856e-05,
      "loss": 0.0862,
      "step": 33990
    },
    {
      "epoch": 1.2738376231688584,
      "grad_norm": 0.07705027610063553,
      "learning_rate": 3.1916936151588694e-05,
      "loss": 0.0895,
      "step": 34000
    },
    {
      "epoch": 1.2742122812933199,
      "grad_norm": 0.06731072813272476,
      "learning_rate": 3.18880620487068e-05,
      "loss": 0.0909,
      "step": 34010
    },
    {
      "epoch": 1.2745869394177813,
      "grad_norm": 0.09236022084951401,
      "learning_rate": 3.185919489648498e-05,
      "loss": 0.0889,
      "step": 34020
    },
    {
      "epoch": 1.2749615975422426,
      "grad_norm": 0.08367309719324112,
      "learning_rate": 3.1830334706001296e-05,
      "loss": 0.0891,
      "step": 34030
    },
    {
      "epoch": 1.275336255666704,
      "grad_norm": 0.08225461840629578,
      "learning_rate": 3.180148148833118e-05,
      "loss": 0.093,
      "step": 34040
    },
    {
      "epoch": 1.2757109137911655,
      "grad_norm": 0.09057947993278503,
      "learning_rate": 3.177263525454737e-05,
      "loss": 0.0938,
      "step": 34050
    },
    {
      "epoch": 1.276085571915627,
      "grad_norm": 0.09184697270393372,
      "learning_rate": 3.1743796015719923e-05,
      "loss": 0.0911,
      "step": 34060
    },
    {
      "epoch": 1.2764602300400885,
      "grad_norm": 0.08309801667928696,
      "learning_rate": 3.171496378291626e-05,
      "loss": 0.0926,
      "step": 34070
    },
    {
      "epoch": 1.27683488816455,
      "grad_norm": 0.0824490413069725,
      "learning_rate": 3.168613856720104e-05,
      "loss": 0.0925,
      "step": 34080
    },
    {
      "epoch": 1.2772095462890114,
      "grad_norm": 0.0775638148188591,
      "learning_rate": 3.165732037963629e-05,
      "loss": 0.0892,
      "step": 34090
    },
    {
      "epoch": 1.2775842044134726,
      "grad_norm": 0.07219981402158737,
      "learning_rate": 3.16285092312813e-05,
      "loss": 0.091,
      "step": 34100
    },
    {
      "epoch": 1.2779588625379341,
      "grad_norm": 0.4117996394634247,
      "learning_rate": 3.1599705133192647e-05,
      "loss": 0.0931,
      "step": 34110
    },
    {
      "epoch": 1.2783335206623956,
      "grad_norm": 0.06746166199445724,
      "learning_rate": 3.157090809642427e-05,
      "loss": 0.0909,
      "step": 34120
    },
    {
      "epoch": 1.278708178786857,
      "grad_norm": 0.08645755797624588,
      "learning_rate": 3.1542118132027344e-05,
      "loss": 0.0912,
      "step": 34130
    },
    {
      "epoch": 1.2790828369113183,
      "grad_norm": 0.05787109211087227,
      "learning_rate": 3.151333525105033e-05,
      "loss": 0.0901,
      "step": 34140
    },
    {
      "epoch": 1.2794574950357798,
      "grad_norm": 0.05974235385656357,
      "learning_rate": 3.148455946453898e-05,
      "loss": 0.0906,
      "step": 34150
    },
    {
      "epoch": 1.2798321531602412,
      "grad_norm": 0.07255665212869644,
      "learning_rate": 3.145579078353635e-05,
      "loss": 0.0912,
      "step": 34160
    },
    {
      "epoch": 1.2802068112847027,
      "grad_norm": 0.09356480091810226,
      "learning_rate": 3.142702921908273e-05,
      "loss": 0.0896,
      "step": 34170
    },
    {
      "epoch": 1.2805814694091642,
      "grad_norm": 0.05805462971329689,
      "learning_rate": 3.1398274782215705e-05,
      "loss": 0.0901,
      "step": 34180
    },
    {
      "epoch": 1.2809561275336256,
      "grad_norm": 0.06762532889842987,
      "learning_rate": 3.136952748397009e-05,
      "loss": 0.0882,
      "step": 34190
    },
    {
      "epoch": 1.2813307856580871,
      "grad_norm": 0.08851222693920135,
      "learning_rate": 3.134078733537799e-05,
      "loss": 0.0941,
      "step": 34200
    },
    {
      "epoch": 1.2817054437825484,
      "grad_norm": 0.09910847991704941,
      "learning_rate": 3.131205434746879e-05,
      "loss": 0.0878,
      "step": 34210
    },
    {
      "epoch": 1.2820801019070098,
      "grad_norm": 0.0718669444322586,
      "learning_rate": 3.1283328531269055e-05,
      "loss": 0.091,
      "step": 34220
    },
    {
      "epoch": 1.2824547600314713,
      "grad_norm": 0.07033368945121765,
      "learning_rate": 3.125460989780268e-05,
      "loss": 0.0908,
      "step": 34230
    },
    {
      "epoch": 1.2828294181559328,
      "grad_norm": 0.0698971375823021,
      "learning_rate": 3.122589845809074e-05,
      "loss": 0.0887,
      "step": 34240
    },
    {
      "epoch": 1.283204076280394,
      "grad_norm": 0.09434672445058823,
      "learning_rate": 3.119719422315156e-05,
      "loss": 0.0919,
      "step": 34250
    },
    {
      "epoch": 1.2835787344048555,
      "grad_norm": 0.0789637491106987,
      "learning_rate": 3.116849720400076e-05,
      "loss": 0.0925,
      "step": 34260
    },
    {
      "epoch": 1.283953392529317,
      "grad_norm": 0.06341706961393356,
      "learning_rate": 3.113980741165109e-05,
      "loss": 0.0931,
      "step": 34270
    },
    {
      "epoch": 1.2843280506537784,
      "grad_norm": 0.10402405261993408,
      "learning_rate": 3.1111124857112626e-05,
      "loss": 0.0914,
      "step": 34280
    },
    {
      "epoch": 1.2847027087782399,
      "grad_norm": 0.10858646035194397,
      "learning_rate": 3.1082449551392583e-05,
      "loss": 0.0924,
      "step": 34290
    },
    {
      "epoch": 1.2850773669027014,
      "grad_norm": 0.0992894396185875,
      "learning_rate": 3.105378150549546e-05,
      "loss": 0.0917,
      "step": 34300
    },
    {
      "epoch": 1.2854520250271628,
      "grad_norm": 0.06817617267370224,
      "learning_rate": 3.102512073042293e-05,
      "loss": 0.0912,
      "step": 34310
    },
    {
      "epoch": 1.285826683151624,
      "grad_norm": 0.09297271072864532,
      "learning_rate": 3.099646723717391e-05,
      "loss": 0.0944,
      "step": 34320
    },
    {
      "epoch": 1.2862013412760855,
      "grad_norm": 0.08222100883722305,
      "learning_rate": 3.096782103674446e-05,
      "loss": 0.089,
      "step": 34330
    },
    {
      "epoch": 1.286575999400547,
      "grad_norm": 0.06861615180969238,
      "learning_rate": 3.09391821401279e-05,
      "loss": 0.0877,
      "step": 34340
    },
    {
      "epoch": 1.2869506575250085,
      "grad_norm": 0.0911020040512085,
      "learning_rate": 3.091055055831475e-05,
      "loss": 0.0917,
      "step": 34350
    },
    {
      "epoch": 1.2873253156494697,
      "grad_norm": 0.08127015084028244,
      "learning_rate": 3.0881926302292673e-05,
      "loss": 0.0875,
      "step": 34360
    },
    {
      "epoch": 1.2876999737739312,
      "grad_norm": 0.08373960107564926,
      "learning_rate": 3.085330938304658e-05,
      "loss": 0.0896,
      "step": 34370
    },
    {
      "epoch": 1.2880746318983927,
      "grad_norm": 0.10614372044801712,
      "learning_rate": 3.0824699811558496e-05,
      "loss": 0.0891,
      "step": 34380
    },
    {
      "epoch": 1.2884492900228541,
      "grad_norm": 0.09401275962591171,
      "learning_rate": 3.0796097598807685e-05,
      "loss": 0.09,
      "step": 34390
    },
    {
      "epoch": 1.2888239481473156,
      "grad_norm": 0.2249779999256134,
      "learning_rate": 3.076750275577058e-05,
      "loss": 0.0892,
      "step": 34400
    },
    {
      "epoch": 1.289198606271777,
      "grad_norm": 0.07047981768846512,
      "learning_rate": 3.073891529342076e-05,
      "loss": 0.0861,
      "step": 34410
    },
    {
      "epoch": 1.2895732643962385,
      "grad_norm": 0.07670242339372635,
      "learning_rate": 3.071033522272899e-05,
      "loss": 0.0876,
      "step": 34420
    },
    {
      "epoch": 1.2899479225206998,
      "grad_norm": 0.07147696614265442,
      "learning_rate": 3.0681762554663186e-05,
      "loss": 0.0859,
      "step": 34430
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 0.07348605245351791,
      "learning_rate": 3.065319730018844e-05,
      "loss": 0.0918,
      "step": 34440
    },
    {
      "epoch": 1.2906972387696227,
      "grad_norm": 0.08033090829849243,
      "learning_rate": 3.0624639470267e-05,
      "loss": 0.0887,
      "step": 34450
    },
    {
      "epoch": 1.2910718968940842,
      "grad_norm": 0.07368107885122299,
      "learning_rate": 3.0596089075858224e-05,
      "loss": 0.0896,
      "step": 34460
    },
    {
      "epoch": 1.2914465550185454,
      "grad_norm": 0.07014486193656921,
      "learning_rate": 3.0567546127918686e-05,
      "loss": 0.0889,
      "step": 34470
    },
    {
      "epoch": 1.291821213143007,
      "grad_norm": 0.09471485018730164,
      "learning_rate": 3.0539010637402024e-05,
      "loss": 0.0944,
      "step": 34480
    },
    {
      "epoch": 1.2921958712674684,
      "grad_norm": 0.12551234662532806,
      "learning_rate": 3.0510482615259074e-05,
      "loss": 0.0928,
      "step": 34490
    },
    {
      "epoch": 1.2925705293919298,
      "grad_norm": 0.0761469379067421,
      "learning_rate": 3.0481962072437797e-05,
      "loss": 0.0874,
      "step": 34500
    },
    {
      "epoch": 1.2929451875163913,
      "grad_norm": 0.08736098557710648,
      "learning_rate": 3.045344901988325e-05,
      "loss": 0.0888,
      "step": 34510
    },
    {
      "epoch": 1.2933198456408528,
      "grad_norm": 0.06309841573238373,
      "learning_rate": 3.042494346853765e-05,
      "loss": 0.0906,
      "step": 34520
    },
    {
      "epoch": 1.2936945037653143,
      "grad_norm": 0.06685974448919296,
      "learning_rate": 3.0396445429340315e-05,
      "loss": 0.0898,
      "step": 34530
    },
    {
      "epoch": 1.2940691618897755,
      "grad_norm": 0.07022873312234879,
      "learning_rate": 3.036795491322771e-05,
      "loss": 0.092,
      "step": 34540
    },
    {
      "epoch": 1.294443820014237,
      "grad_norm": 0.08742664754390717,
      "learning_rate": 3.033947193113336e-05,
      "loss": 0.0913,
      "step": 34550
    },
    {
      "epoch": 1.2948184781386984,
      "grad_norm": 0.07270926237106323,
      "learning_rate": 3.031099649398796e-05,
      "loss": 0.0892,
      "step": 34560
    },
    {
      "epoch": 1.29519313626316,
      "grad_norm": 0.08024492859840393,
      "learning_rate": 3.028252861271924e-05,
      "loss": 0.0942,
      "step": 34570
    },
    {
      "epoch": 1.2955677943876214,
      "grad_norm": 0.07621774822473526,
      "learning_rate": 3.0254068298252113e-05,
      "loss": 0.0956,
      "step": 34580
    },
    {
      "epoch": 1.2959424525120826,
      "grad_norm": 0.10444454103708267,
      "learning_rate": 3.022561556150853e-05,
      "loss": 0.0894,
      "step": 34590
    },
    {
      "epoch": 1.296317110636544,
      "grad_norm": 0.09239321947097778,
      "learning_rate": 3.019717041340754e-05,
      "loss": 0.0933,
      "step": 34600
    },
    {
      "epoch": 1.2966917687610056,
      "grad_norm": 0.08048465847969055,
      "learning_rate": 3.01687328648653e-05,
      "loss": 0.0888,
      "step": 34610
    },
    {
      "epoch": 1.297066426885467,
      "grad_norm": 0.07487219572067261,
      "learning_rate": 3.0140302926795017e-05,
      "loss": 0.0907,
      "step": 34620
    },
    {
      "epoch": 1.2974410850099285,
      "grad_norm": 0.07225200533866882,
      "learning_rate": 3.0111880610107025e-05,
      "loss": 0.0909,
      "step": 34630
    },
    {
      "epoch": 1.29781574313439,
      "grad_norm": 0.06626460701227188,
      "learning_rate": 3.0083465925708703e-05,
      "loss": 0.0905,
      "step": 34640
    },
    {
      "epoch": 1.2981904012588514,
      "grad_norm": 0.0867135152220726,
      "learning_rate": 3.0055058884504494e-05,
      "loss": 0.0905,
      "step": 34650
    },
    {
      "epoch": 1.2985650593833127,
      "grad_norm": 0.08657677471637726,
      "learning_rate": 3.0026659497395927e-05,
      "loss": 0.0905,
      "step": 34660
    },
    {
      "epoch": 1.2989397175077741,
      "grad_norm": 0.0645681619644165,
      "learning_rate": 2.9998267775281596e-05,
      "loss": 0.0889,
      "step": 34670
    },
    {
      "epoch": 1.2993143756322356,
      "grad_norm": 0.09028197079896927,
      "learning_rate": 2.9969883729057137e-05,
      "loss": 0.0882,
      "step": 34680
    },
    {
      "epoch": 1.299689033756697,
      "grad_norm": 0.08771233260631561,
      "learning_rate": 2.994150736961525e-05,
      "loss": 0.0898,
      "step": 34690
    },
    {
      "epoch": 1.3000636918811583,
      "grad_norm": 0.07435225695371628,
      "learning_rate": 2.9913138707845668e-05,
      "loss": 0.0917,
      "step": 34700
    },
    {
      "epoch": 1.3004383500056198,
      "grad_norm": 0.07487601041793823,
      "learning_rate": 2.9884777754635184e-05,
      "loss": 0.0913,
      "step": 34710
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 0.08126804232597351,
      "learning_rate": 2.9856424520867664e-05,
      "loss": 0.0888,
      "step": 34720
    },
    {
      "epoch": 1.3011876662545427,
      "grad_norm": 0.12184478342533112,
      "learning_rate": 2.9828079017423948e-05,
      "loss": 0.0876,
      "step": 34730
    },
    {
      "epoch": 1.3015623243790042,
      "grad_norm": 0.07094412297010422,
      "learning_rate": 2.979974125518196e-05,
      "loss": 0.09,
      "step": 34740
    },
    {
      "epoch": 1.3019369825034657,
      "grad_norm": 0.10295282304286957,
      "learning_rate": 2.9771411245016627e-05,
      "loss": 0.0896,
      "step": 34750
    },
    {
      "epoch": 1.3023116406279271,
      "grad_norm": 0.1361737996339798,
      "learning_rate": 2.9743088997799896e-05,
      "loss": 0.0902,
      "step": 34760
    },
    {
      "epoch": 1.3026862987523884,
      "grad_norm": 0.1651124805212021,
      "learning_rate": 2.971477452440078e-05,
      "loss": 0.0904,
      "step": 34770
    },
    {
      "epoch": 1.3030609568768499,
      "grad_norm": 0.06715520471334457,
      "learning_rate": 2.968646783568526e-05,
      "loss": 0.0912,
      "step": 34780
    },
    {
      "epoch": 1.3034356150013113,
      "grad_norm": 0.06626580655574799,
      "learning_rate": 2.9658168942516345e-05,
      "loss": 0.0876,
      "step": 34790
    },
    {
      "epoch": 1.3038102731257728,
      "grad_norm": 0.07512220740318298,
      "learning_rate": 2.962987785575404e-05,
      "loss": 0.0894,
      "step": 34800
    },
    {
      "epoch": 1.304184931250234,
      "grad_norm": 0.09277938306331635,
      "learning_rate": 2.960159458625541e-05,
      "loss": 0.0907,
      "step": 34810
    },
    {
      "epoch": 1.3045595893746955,
      "grad_norm": 0.06246742978692055,
      "learning_rate": 2.9573319144874434e-05,
      "loss": 0.0904,
      "step": 34820
    },
    {
      "epoch": 1.304934247499157,
      "grad_norm": 0.10192879289388657,
      "learning_rate": 2.9545051542462165e-05,
      "loss": 0.0887,
      "step": 34830
    },
    {
      "epoch": 1.3053089056236185,
      "grad_norm": 0.10814613103866577,
      "learning_rate": 2.9516791789866582e-05,
      "loss": 0.091,
      "step": 34840
    },
    {
      "epoch": 1.30568356374808,
      "grad_norm": 0.07763534784317017,
      "learning_rate": 2.948853989793269e-05,
      "loss": 0.0908,
      "step": 34850
    },
    {
      "epoch": 1.3060582218725414,
      "grad_norm": 0.07142428308725357,
      "learning_rate": 2.9460295877502497e-05,
      "loss": 0.0937,
      "step": 34860
    },
    {
      "epoch": 1.3064328799970029,
      "grad_norm": 0.06946543604135513,
      "learning_rate": 2.943205973941494e-05,
      "loss": 0.0934,
      "step": 34870
    },
    {
      "epoch": 1.306807538121464,
      "grad_norm": 0.07624056935310364,
      "learning_rate": 2.9403831494505962e-05,
      "loss": 0.0915,
      "step": 34880
    },
    {
      "epoch": 1.3071821962459256,
      "grad_norm": 0.0879148617386818,
      "learning_rate": 2.9375611153608463e-05,
      "loss": 0.0888,
      "step": 34890
    },
    {
      "epoch": 1.307556854370387,
      "grad_norm": 0.08644578605890274,
      "learning_rate": 2.934739872755231e-05,
      "loss": 0.0902,
      "step": 34900
    },
    {
      "epoch": 1.3079315124948485,
      "grad_norm": 0.09237300604581833,
      "learning_rate": 2.931919422716437e-05,
      "loss": 0.0876,
      "step": 34910
    },
    {
      "epoch": 1.3083061706193098,
      "grad_norm": 0.06747141480445862,
      "learning_rate": 2.929099766326841e-05,
      "loss": 0.0947,
      "step": 34920
    },
    {
      "epoch": 1.3086808287437712,
      "grad_norm": 0.14590851962566376,
      "learning_rate": 2.92628090466852e-05,
      "loss": 0.0864,
      "step": 34930
    },
    {
      "epoch": 1.3090554868682327,
      "grad_norm": 0.1306125968694687,
      "learning_rate": 2.9234628388232398e-05,
      "loss": 0.0869,
      "step": 34940
    },
    {
      "epoch": 1.3094301449926942,
      "grad_norm": 0.116459921002388,
      "learning_rate": 2.9206455698724687e-05,
      "loss": 0.091,
      "step": 34950
    },
    {
      "epoch": 1.3098048031171556,
      "grad_norm": 0.09381938725709915,
      "learning_rate": 2.9178290988973644e-05,
      "loss": 0.09,
      "step": 34960
    },
    {
      "epoch": 1.310179461241617,
      "grad_norm": 0.08678161352872849,
      "learning_rate": 2.915013426978781e-05,
      "loss": 0.0903,
      "step": 34970
    },
    {
      "epoch": 1.3105541193660786,
      "grad_norm": 0.07172014564275742,
      "learning_rate": 2.9121985551972607e-05,
      "loss": 0.0882,
      "step": 34980
    },
    {
      "epoch": 1.3109287774905398,
      "grad_norm": 0.07523878663778305,
      "learning_rate": 2.9093844846330424e-05,
      "loss": 0.0913,
      "step": 34990
    },
    {
      "epoch": 1.3113034356150013,
      "grad_norm": 0.0963444709777832,
      "learning_rate": 2.9065712163660607e-05,
      "loss": 0.0939,
      "step": 35000
    },
    {
      "epoch": 1.3116780937394628,
      "grad_norm": 0.06830953806638718,
      "learning_rate": 2.9037587514759397e-05,
      "loss": 0.0924,
      "step": 35010
    },
    {
      "epoch": 1.3120527518639242,
      "grad_norm": 0.0828775092959404,
      "learning_rate": 2.90094709104199e-05,
      "loss": 0.0884,
      "step": 35020
    },
    {
      "epoch": 1.3124274099883855,
      "grad_norm": 0.08127608895301819,
      "learning_rate": 2.8981362361432197e-05,
      "loss": 0.0933,
      "step": 35030
    },
    {
      "epoch": 1.312802068112847,
      "grad_norm": 0.08891436457633972,
      "learning_rate": 2.895326187858326e-05,
      "loss": 0.0905,
      "step": 35040
    },
    {
      "epoch": 1.3131767262373084,
      "grad_norm": 0.10314931720495224,
      "learning_rate": 2.8925169472656965e-05,
      "loss": 0.0914,
      "step": 35050
    },
    {
      "epoch": 1.3135513843617699,
      "grad_norm": 0.09858932346105576,
      "learning_rate": 2.88970851544341e-05,
      "loss": 0.0898,
      "step": 35060
    },
    {
      "epoch": 1.3139260424862313,
      "grad_norm": 0.07538682967424393,
      "learning_rate": 2.8869008934692332e-05,
      "loss": 0.0863,
      "step": 35070
    },
    {
      "epoch": 1.3143007006106928,
      "grad_norm": 0.08178354799747467,
      "learning_rate": 2.884094082420622e-05,
      "loss": 0.0918,
      "step": 35080
    },
    {
      "epoch": 1.3146753587351543,
      "grad_norm": 0.09014948457479477,
      "learning_rate": 2.8812880833747224e-05,
      "loss": 0.0884,
      "step": 35090
    },
    {
      "epoch": 1.3150500168596155,
      "grad_norm": 0.07167817652225494,
      "learning_rate": 2.8784828974083677e-05,
      "loss": 0.0852,
      "step": 35100
    },
    {
      "epoch": 1.315424674984077,
      "grad_norm": 0.09528186172246933,
      "learning_rate": 2.875678525598082e-05,
      "loss": 0.0879,
      "step": 35110
    },
    {
      "epoch": 1.3157993331085385,
      "grad_norm": 0.10076344013214111,
      "learning_rate": 2.87287496902007e-05,
      "loss": 0.0957,
      "step": 35120
    },
    {
      "epoch": 1.316173991233,
      "grad_norm": 0.09646959602832794,
      "learning_rate": 2.870072228750229e-05,
      "loss": 0.0948,
      "step": 35130
    },
    {
      "epoch": 1.3165486493574612,
      "grad_norm": 0.10918699204921722,
      "learning_rate": 2.867270305864147e-05,
      "loss": 0.09,
      "step": 35140
    },
    {
      "epoch": 1.3169233074819227,
      "grad_norm": 0.08056825399398804,
      "learning_rate": 2.8644692014370888e-05,
      "loss": 0.0878,
      "step": 35150
    },
    {
      "epoch": 1.3172979656063841,
      "grad_norm": 0.06764073669910431,
      "learning_rate": 2.8616689165440103e-05,
      "loss": 0.0891,
      "step": 35160
    },
    {
      "epoch": 1.3176726237308456,
      "grad_norm": 0.06336752325296402,
      "learning_rate": 2.8588694522595533e-05,
      "loss": 0.0899,
      "step": 35170
    },
    {
      "epoch": 1.318047281855307,
      "grad_norm": 0.06904131919145584,
      "learning_rate": 2.8560708096580436e-05,
      "loss": 0.0913,
      "step": 35180
    },
    {
      "epoch": 1.3184219399797685,
      "grad_norm": 0.08211278915405273,
      "learning_rate": 2.8532729898134914e-05,
      "loss": 0.0922,
      "step": 35190
    },
    {
      "epoch": 1.31879659810423,
      "grad_norm": 0.07096027582883835,
      "learning_rate": 2.8504759937995917e-05,
      "loss": 0.0887,
      "step": 35200
    },
    {
      "epoch": 1.3191712562286912,
      "grad_norm": 0.0704183280467987,
      "learning_rate": 2.8476798226897256e-05,
      "loss": 0.0891,
      "step": 35210
    },
    {
      "epoch": 1.3195459143531527,
      "grad_norm": 0.08228863030672073,
      "learning_rate": 2.8448844775569487e-05,
      "loss": 0.0856,
      "step": 35220
    },
    {
      "epoch": 1.3199205724776142,
      "grad_norm": 0.06822856515645981,
      "learning_rate": 2.8420899594740123e-05,
      "loss": 0.089,
      "step": 35230
    },
    {
      "epoch": 1.3202952306020757,
      "grad_norm": 0.08126527070999146,
      "learning_rate": 2.8392962695133428e-05,
      "loss": 0.087,
      "step": 35240
    },
    {
      "epoch": 1.320669888726537,
      "grad_norm": 0.07458827644586563,
      "learning_rate": 2.836503408747051e-05,
      "loss": 0.088,
      "step": 35250
    },
    {
      "epoch": 1.3210445468509984,
      "grad_norm": 0.08281265199184418,
      "learning_rate": 2.833711378246926e-05,
      "loss": 0.0928,
      "step": 35260
    },
    {
      "epoch": 1.3214192049754598,
      "grad_norm": 0.07199444621801376,
      "learning_rate": 2.8309201790844396e-05,
      "loss": 0.0865,
      "step": 35270
    },
    {
      "epoch": 1.3217938630999213,
      "grad_norm": 0.06391319632530212,
      "learning_rate": 2.828129812330753e-05,
      "loss": 0.0894,
      "step": 35280
    },
    {
      "epoch": 1.3221685212243828,
      "grad_norm": 0.07489229738712311,
      "learning_rate": 2.825340279056694e-05,
      "loss": 0.0859,
      "step": 35290
    },
    {
      "epoch": 1.3225431793488442,
      "grad_norm": 0.07611015439033508,
      "learning_rate": 2.8225515803327803e-05,
      "loss": 0.0894,
      "step": 35300
    },
    {
      "epoch": 1.3229178374733057,
      "grad_norm": 0.08509083092212677,
      "learning_rate": 2.819763717229205e-05,
      "loss": 0.09,
      "step": 35310
    },
    {
      "epoch": 1.323292495597767,
      "grad_norm": 0.07352661341428757,
      "learning_rate": 2.8169766908158436e-05,
      "loss": 0.0928,
      "step": 35320
    },
    {
      "epoch": 1.3236671537222284,
      "grad_norm": 0.0775284618139267,
      "learning_rate": 2.8141905021622474e-05,
      "loss": 0.0906,
      "step": 35330
    },
    {
      "epoch": 1.32404181184669,
      "grad_norm": 0.07114605605602264,
      "learning_rate": 2.811405152337649e-05,
      "loss": 0.0903,
      "step": 35340
    },
    {
      "epoch": 1.3244164699711514,
      "grad_norm": 0.07747311145067215,
      "learning_rate": 2.8086206424109585e-05,
      "loss": 0.0905,
      "step": 35350
    },
    {
      "epoch": 1.3247911280956128,
      "grad_norm": 0.06623981148004532,
      "learning_rate": 2.805836973450758e-05,
      "loss": 0.0941,
      "step": 35360
    },
    {
      "epoch": 1.325165786220074,
      "grad_norm": 0.14254359900951385,
      "learning_rate": 2.803054146525317e-05,
      "loss": 0.0915,
      "step": 35370
    },
    {
      "epoch": 1.3255404443445356,
      "grad_norm": 0.06070222333073616,
      "learning_rate": 2.8002721627025775e-05,
      "loss": 0.0866,
      "step": 35380
    },
    {
      "epoch": 1.325915102468997,
      "grad_norm": 0.08098037540912628,
      "learning_rate": 2.7974910230501517e-05,
      "loss": 0.0908,
      "step": 35390
    },
    {
      "epoch": 1.3262897605934585,
      "grad_norm": 0.07280692458152771,
      "learning_rate": 2.7947107286353364e-05,
      "loss": 0.0905,
      "step": 35400
    },
    {
      "epoch": 1.32666441871792,
      "grad_norm": 0.07489979267120361,
      "learning_rate": 2.791931280525101e-05,
      "loss": 0.0907,
      "step": 35410
    },
    {
      "epoch": 1.3270390768423814,
      "grad_norm": 0.08670319616794586,
      "learning_rate": 2.789152679786089e-05,
      "loss": 0.0901,
      "step": 35420
    },
    {
      "epoch": 1.3274137349668427,
      "grad_norm": 0.0860055461525917,
      "learning_rate": 2.7863749274846206e-05,
      "loss": 0.093,
      "step": 35430
    },
    {
      "epoch": 1.3277883930913041,
      "grad_norm": 0.0616966113448143,
      "learning_rate": 2.783598024686689e-05,
      "loss": 0.0892,
      "step": 35440
    },
    {
      "epoch": 1.3281630512157656,
      "grad_norm": 0.08413011580705643,
      "learning_rate": 2.780821972457962e-05,
      "loss": 0.0943,
      "step": 35450
    },
    {
      "epoch": 1.328537709340227,
      "grad_norm": 0.09998448938131332,
      "learning_rate": 2.7780467718637815e-05,
      "loss": 0.0868,
      "step": 35460
    },
    {
      "epoch": 1.3289123674646885,
      "grad_norm": 0.07955043762922287,
      "learning_rate": 2.7752724239691606e-05,
      "loss": 0.0915,
      "step": 35470
    },
    {
      "epoch": 1.3292870255891498,
      "grad_norm": 0.08506764471530914,
      "learning_rate": 2.7724989298387884e-05,
      "loss": 0.0923,
      "step": 35480
    },
    {
      "epoch": 1.3296616837136113,
      "grad_norm": 0.0895291194319725,
      "learning_rate": 2.7697262905370247e-05,
      "loss": 0.0903,
      "step": 35490
    },
    {
      "epoch": 1.3300363418380727,
      "grad_norm": 0.0918392762541771,
      "learning_rate": 2.7669545071278967e-05,
      "loss": 0.0891,
      "step": 35500
    },
    {
      "epoch": 1.3304109999625342,
      "grad_norm": 0.06324032694101334,
      "learning_rate": 2.7641835806751116e-05,
      "loss": 0.0895,
      "step": 35510
    },
    {
      "epoch": 1.3307856580869957,
      "grad_norm": 0.07635178416967392,
      "learning_rate": 2.7614135122420448e-05,
      "loss": 0.0867,
      "step": 35520
    },
    {
      "epoch": 1.3311603162114571,
      "grad_norm": 0.06170394644141197,
      "learning_rate": 2.7586443028917374e-05,
      "loss": 0.0922,
      "step": 35530
    },
    {
      "epoch": 1.3315349743359186,
      "grad_norm": 0.08115599304437637,
      "learning_rate": 2.755875953686906e-05,
      "loss": 0.0903,
      "step": 35540
    },
    {
      "epoch": 1.3319096324603799,
      "grad_norm": 0.07592172175645828,
      "learning_rate": 2.7531084656899364e-05,
      "loss": 0.0896,
      "step": 35550
    },
    {
      "epoch": 1.3322842905848413,
      "grad_norm": 0.09279855340719223,
      "learning_rate": 2.7503418399628834e-05,
      "loss": 0.0861,
      "step": 35560
    },
    {
      "epoch": 1.3326589487093028,
      "grad_norm": 0.08594515919685364,
      "learning_rate": 2.7475760775674697e-05,
      "loss": 0.0904,
      "step": 35570
    },
    {
      "epoch": 1.3330336068337643,
      "grad_norm": 0.1003258153796196,
      "learning_rate": 2.7448111795650877e-05,
      "loss": 0.0904,
      "step": 35580
    },
    {
      "epoch": 1.3334082649582255,
      "grad_norm": 0.08581393212080002,
      "learning_rate": 2.7420471470167996e-05,
      "loss": 0.0912,
      "step": 35590
    },
    {
      "epoch": 1.333782923082687,
      "grad_norm": 0.07975777238607407,
      "learning_rate": 2.739283980983333e-05,
      "loss": 0.0916,
      "step": 35600
    },
    {
      "epoch": 1.3341575812071484,
      "grad_norm": 0.07021878659725189,
      "learning_rate": 2.736521682525084e-05,
      "loss": 0.0867,
      "step": 35610
    },
    {
      "epoch": 1.33453223933161,
      "grad_norm": 0.06705960631370544,
      "learning_rate": 2.7337602527021182e-05,
      "loss": 0.087,
      "step": 35620
    },
    {
      "epoch": 1.3349068974560714,
      "grad_norm": 0.07201304286718369,
      "learning_rate": 2.730999692574161e-05,
      "loss": 0.0888,
      "step": 35630
    },
    {
      "epoch": 1.3352815555805329,
      "grad_norm": 0.07321912050247192,
      "learning_rate": 2.7282400032006083e-05,
      "loss": 0.0885,
      "step": 35640
    },
    {
      "epoch": 1.3356562137049943,
      "grad_norm": 0.07441333681344986,
      "learning_rate": 2.725481185640528e-05,
      "loss": 0.0922,
      "step": 35650
    },
    {
      "epoch": 1.3360308718294556,
      "grad_norm": 0.06605056673288345,
      "learning_rate": 2.7227232409526417e-05,
      "loss": 0.0826,
      "step": 35660
    },
    {
      "epoch": 1.336405529953917,
      "grad_norm": 0.09203539043664932,
      "learning_rate": 2.719966170195344e-05,
      "loss": 0.0884,
      "step": 35670
    },
    {
      "epoch": 1.3367801880783785,
      "grad_norm": 0.08363137394189835,
      "learning_rate": 2.7172099744266906e-05,
      "loss": 0.0902,
      "step": 35680
    },
    {
      "epoch": 1.33715484620284,
      "grad_norm": 0.08058294653892517,
      "learning_rate": 2.7144546547044035e-05,
      "loss": 0.0915,
      "step": 35690
    },
    {
      "epoch": 1.3375295043273012,
      "grad_norm": 0.09279650449752808,
      "learning_rate": 2.711700212085868e-05,
      "loss": 0.0926,
      "step": 35700
    },
    {
      "epoch": 1.3379041624517627,
      "grad_norm": 0.07670620083808899,
      "learning_rate": 2.7089466476281313e-05,
      "loss": 0.088,
      "step": 35710
    },
    {
      "epoch": 1.3382788205762242,
      "grad_norm": 0.0873868465423584,
      "learning_rate": 2.7061939623879064e-05,
      "loss": 0.0879,
      "step": 35720
    },
    {
      "epoch": 1.3386534787006856,
      "grad_norm": 0.07338643074035645,
      "learning_rate": 2.7034421574215657e-05,
      "loss": 0.0895,
      "step": 35730
    },
    {
      "epoch": 1.339028136825147,
      "grad_norm": 0.0773855596780777,
      "learning_rate": 2.7006912337851465e-05,
      "loss": 0.0879,
      "step": 35740
    },
    {
      "epoch": 1.3394027949496086,
      "grad_norm": 0.07547849416732788,
      "learning_rate": 2.6979411925343455e-05,
      "loss": 0.0917,
      "step": 35750
    },
    {
      "epoch": 1.33977745307407,
      "grad_norm": 0.08288168907165527,
      "learning_rate": 2.6951920347245246e-05,
      "loss": 0.088,
      "step": 35760
    },
    {
      "epoch": 1.3401521111985313,
      "grad_norm": 0.06566786766052246,
      "learning_rate": 2.6924437614107005e-05,
      "loss": 0.087,
      "step": 35770
    },
    {
      "epoch": 1.3405267693229928,
      "grad_norm": 0.06670533120632172,
      "learning_rate": 2.6896963736475533e-05,
      "loss": 0.0907,
      "step": 35780
    },
    {
      "epoch": 1.3409014274474542,
      "grad_norm": 0.07553309947252274,
      "learning_rate": 2.686949872489431e-05,
      "loss": 0.0911,
      "step": 35790
    },
    {
      "epoch": 1.3412760855719157,
      "grad_norm": 0.10368628799915314,
      "learning_rate": 2.6842042589903272e-05,
      "loss": 0.0902,
      "step": 35800
    },
    {
      "epoch": 1.341650743696377,
      "grad_norm": 0.08543244004249573,
      "learning_rate": 2.6814595342039038e-05,
      "loss": 0.0916,
      "step": 35810
    },
    {
      "epoch": 1.3420254018208384,
      "grad_norm": 0.08100252598524094,
      "learning_rate": 2.6787156991834815e-05,
      "loss": 0.0888,
      "step": 35820
    },
    {
      "epoch": 1.3424000599452999,
      "grad_norm": 0.08239016681909561,
      "learning_rate": 2.6759727549820363e-05,
      "loss": 0.0891,
      "step": 35830
    },
    {
      "epoch": 1.3427747180697613,
      "grad_norm": 0.08619464188814163,
      "learning_rate": 2.6732307026522046e-05,
      "loss": 0.0869,
      "step": 35840
    },
    {
      "epoch": 1.3431493761942228,
      "grad_norm": 0.08928875625133514,
      "learning_rate": 2.6704895432462794e-05,
      "loss": 0.0896,
      "step": 35850
    },
    {
      "epoch": 1.3435240343186843,
      "grad_norm": 0.07563678175210953,
      "learning_rate": 2.667749277816214e-05,
      "loss": 0.0905,
      "step": 35860
    },
    {
      "epoch": 1.3438986924431457,
      "grad_norm": 0.08745001256465912,
      "learning_rate": 2.6650099074136093e-05,
      "loss": 0.0896,
      "step": 35870
    },
    {
      "epoch": 1.344273350567607,
      "grad_norm": 0.09450713545084,
      "learning_rate": 2.6622714330897368e-05,
      "loss": 0.0915,
      "step": 35880
    },
    {
      "epoch": 1.3446480086920685,
      "grad_norm": 0.09827053546905518,
      "learning_rate": 2.6595338558955158e-05,
      "loss": 0.0917,
      "step": 35890
    },
    {
      "epoch": 1.34502266681653,
      "grad_norm": 0.07361742109060287,
      "learning_rate": 2.6567971768815185e-05,
      "loss": 0.0918,
      "step": 35900
    },
    {
      "epoch": 1.3453973249409914,
      "grad_norm": 0.08522015064954758,
      "learning_rate": 2.6540613970979787e-05,
      "loss": 0.0869,
      "step": 35910
    },
    {
      "epoch": 1.3457719830654526,
      "grad_norm": 0.08889983594417572,
      "learning_rate": 2.6513265175947805e-05,
      "loss": 0.0912,
      "step": 35920
    },
    {
      "epoch": 1.3461466411899141,
      "grad_norm": 0.08007405698299408,
      "learning_rate": 2.6485925394214705e-05,
      "loss": 0.0885,
      "step": 35930
    },
    {
      "epoch": 1.3465212993143756,
      "grad_norm": 0.06315074115991592,
      "learning_rate": 2.6458594636272378e-05,
      "loss": 0.0876,
      "step": 35940
    },
    {
      "epoch": 1.346895957438837,
      "grad_norm": 0.08998166769742966,
      "learning_rate": 2.643127291260933e-05,
      "loss": 0.0858,
      "step": 35950
    },
    {
      "epoch": 1.3472706155632985,
      "grad_norm": 0.06672608852386475,
      "learning_rate": 2.6403960233710573e-05,
      "loss": 0.0925,
      "step": 35960
    },
    {
      "epoch": 1.34764527368776,
      "grad_norm": 0.09831155091524124,
      "learning_rate": 2.6376656610057666e-05,
      "loss": 0.0851,
      "step": 35970
    },
    {
      "epoch": 1.3480199318122215,
      "grad_norm": 0.07200761139392853,
      "learning_rate": 2.634936205212868e-05,
      "loss": 0.0858,
      "step": 35980
    },
    {
      "epoch": 1.3483945899366827,
      "grad_norm": 0.09042435884475708,
      "learning_rate": 2.63220765703982e-05,
      "loss": 0.0885,
      "step": 35990
    },
    {
      "epoch": 1.3487692480611442,
      "grad_norm": 0.08930980414152145,
      "learning_rate": 2.6294800175337363e-05,
      "loss": 0.0942,
      "step": 36000
    },
    {
      "epoch": 1.3491439061856056,
      "grad_norm": 0.08382092416286469,
      "learning_rate": 2.6267532877413735e-05,
      "loss": 0.0884,
      "step": 36010
    },
    {
      "epoch": 1.3495185643100671,
      "grad_norm": 0.06660265475511551,
      "learning_rate": 2.6240274687091503e-05,
      "loss": 0.0914,
      "step": 36020
    },
    {
      "epoch": 1.3498932224345284,
      "grad_norm": 0.06680889427661896,
      "learning_rate": 2.621302561483131e-05,
      "loss": 0.0911,
      "step": 36030
    },
    {
      "epoch": 1.3502678805589898,
      "grad_norm": 0.07906287163496017,
      "learning_rate": 2.6185785671090247e-05,
      "loss": 0.0945,
      "step": 36040
    },
    {
      "epoch": 1.3506425386834513,
      "grad_norm": 0.0714653953909874,
      "learning_rate": 2.6158554866321976e-05,
      "loss": 0.089,
      "step": 36050
    },
    {
      "epoch": 1.3510171968079128,
      "grad_norm": 0.07288026064634323,
      "learning_rate": 2.613133321097662e-05,
      "loss": 0.0927,
      "step": 36060
    },
    {
      "epoch": 1.3513918549323742,
      "grad_norm": 0.07967294007539749,
      "learning_rate": 2.61041207155008e-05,
      "loss": 0.0904,
      "step": 36070
    },
    {
      "epoch": 1.3517665130568357,
      "grad_norm": 0.07755313813686371,
      "learning_rate": 2.6076917390337618e-05,
      "loss": 0.0924,
      "step": 36080
    },
    {
      "epoch": 1.3521411711812972,
      "grad_norm": 0.08218948543071747,
      "learning_rate": 2.604972324592665e-05,
      "loss": 0.093,
      "step": 36090
    },
    {
      "epoch": 1.3525158293057584,
      "grad_norm": 0.07544372975826263,
      "learning_rate": 2.602253829270396e-05,
      "loss": 0.088,
      "step": 36100
    },
    {
      "epoch": 1.35289048743022,
      "grad_norm": 0.08778663724660873,
      "learning_rate": 2.5995362541102076e-05,
      "loss": 0.0909,
      "step": 36110
    },
    {
      "epoch": 1.3532651455546814,
      "grad_norm": 0.06310299038887024,
      "learning_rate": 2.5968196001549995e-05,
      "loss": 0.0888,
      "step": 36120
    },
    {
      "epoch": 1.3536398036791428,
      "grad_norm": 0.09469210356473923,
      "learning_rate": 2.5941038684473195e-05,
      "loss": 0.0896,
      "step": 36130
    },
    {
      "epoch": 1.354014461803604,
      "grad_norm": 0.07370617985725403,
      "learning_rate": 2.591389060029361e-05,
      "loss": 0.0891,
      "step": 36140
    },
    {
      "epoch": 1.3543891199280655,
      "grad_norm": 0.08098280429840088,
      "learning_rate": 2.588675175942957e-05,
      "loss": 0.0889,
      "step": 36150
    },
    {
      "epoch": 1.354763778052527,
      "grad_norm": 0.10311245173215866,
      "learning_rate": 2.5859622172295977e-05,
      "loss": 0.0894,
      "step": 36160
    },
    {
      "epoch": 1.3551384361769885,
      "grad_norm": 0.07687513530254364,
      "learning_rate": 2.5832501849304103e-05,
      "loss": 0.0913,
      "step": 36170
    },
    {
      "epoch": 1.35551309430145,
      "grad_norm": 0.08554747700691223,
      "learning_rate": 2.5805390800861633e-05,
      "loss": 0.0867,
      "step": 36180
    },
    {
      "epoch": 1.3558877524259114,
      "grad_norm": 0.1070374920964241,
      "learning_rate": 2.577828903737277e-05,
      "loss": 0.0898,
      "step": 36190
    },
    {
      "epoch": 1.3562624105503729,
      "grad_norm": 0.09092771261930466,
      "learning_rate": 2.5751196569238122e-05,
      "loss": 0.0893,
      "step": 36200
    },
    {
      "epoch": 1.3566370686748341,
      "grad_norm": 0.18392398953437805,
      "learning_rate": 2.572411340685471e-05,
      "loss": 0.091,
      "step": 36210
    },
    {
      "epoch": 1.3570117267992956,
      "grad_norm": 0.0889216735959053,
      "learning_rate": 2.5697039560616025e-05,
      "loss": 0.0894,
      "step": 36220
    },
    {
      "epoch": 1.357386384923757,
      "grad_norm": 0.0913216844201088,
      "learning_rate": 2.5669975040911947e-05,
      "loss": 0.0917,
      "step": 36230
    },
    {
      "epoch": 1.3577610430482185,
      "grad_norm": 0.08664830774068832,
      "learning_rate": 2.5642919858128794e-05,
      "loss": 0.0916,
      "step": 36240
    },
    {
      "epoch": 1.35813570117268,
      "grad_norm": 0.08287055790424347,
      "learning_rate": 2.5615874022649294e-05,
      "loss": 0.0871,
      "step": 36250
    },
    {
      "epoch": 1.3585103592971413,
      "grad_norm": 0.08463834971189499,
      "learning_rate": 2.5588837544852594e-05,
      "loss": 0.0875,
      "step": 36260
    },
    {
      "epoch": 1.3588850174216027,
      "grad_norm": 0.15055447816848755,
      "learning_rate": 2.5561810435114263e-05,
      "loss": 0.0885,
      "step": 36270
    },
    {
      "epoch": 1.3592596755460642,
      "grad_norm": 0.07648163288831711,
      "learning_rate": 2.5534792703806225e-05,
      "loss": 0.091,
      "step": 36280
    },
    {
      "epoch": 1.3596343336705257,
      "grad_norm": 0.08570654690265656,
      "learning_rate": 2.5507784361296838e-05,
      "loss": 0.0872,
      "step": 36290
    },
    {
      "epoch": 1.3600089917949871,
      "grad_norm": 0.09421300143003464,
      "learning_rate": 2.548078541795091e-05,
      "loss": 0.0887,
      "step": 36300
    },
    {
      "epoch": 1.3603836499194486,
      "grad_norm": 0.08216113597154617,
      "learning_rate": 2.5453795884129527e-05,
      "loss": 0.0905,
      "step": 36310
    },
    {
      "epoch": 1.36075830804391,
      "grad_norm": 0.07136223465204239,
      "learning_rate": 2.542681577019026e-05,
      "loss": 0.088,
      "step": 36320
    },
    {
      "epoch": 1.3611329661683713,
      "grad_norm": 0.08456464856863022,
      "learning_rate": 2.5399845086487028e-05,
      "loss": 0.0874,
      "step": 36330
    },
    {
      "epoch": 1.3615076242928328,
      "grad_norm": 0.07969772070646286,
      "learning_rate": 2.5372883843370122e-05,
      "loss": 0.0909,
      "step": 36340
    },
    {
      "epoch": 1.3618822824172943,
      "grad_norm": 0.0872115045785904,
      "learning_rate": 2.534593205118624e-05,
      "loss": 0.0866,
      "step": 36350
    },
    {
      "epoch": 1.3622569405417557,
      "grad_norm": 0.08977746963500977,
      "learning_rate": 2.531898972027842e-05,
      "loss": 0.0909,
      "step": 36360
    },
    {
      "epoch": 1.362631598666217,
      "grad_norm": 0.08395390212535858,
      "learning_rate": 2.529205686098609e-05,
      "loss": 0.0905,
      "step": 36370
    },
    {
      "epoch": 1.3630062567906784,
      "grad_norm": 0.07933425158262253,
      "learning_rate": 2.5265133483645042e-05,
      "loss": 0.0915,
      "step": 36380
    },
    {
      "epoch": 1.36338091491514,
      "grad_norm": 0.08890902251005173,
      "learning_rate": 2.523821959858742e-05,
      "loss": 0.0897,
      "step": 36390
    },
    {
      "epoch": 1.3637555730396014,
      "grad_norm": 0.11143455654382706,
      "learning_rate": 2.521131521614174e-05,
      "loss": 0.089,
      "step": 36400
    },
    {
      "epoch": 1.3641302311640628,
      "grad_norm": 0.07133599370718002,
      "learning_rate": 2.518442034663287e-05,
      "loss": 0.0905,
      "step": 36410
    },
    {
      "epoch": 1.3645048892885243,
      "grad_norm": 0.09684021025896072,
      "learning_rate": 2.515753500038198e-05,
      "loss": 0.0936,
      "step": 36420
    },
    {
      "epoch": 1.3648795474129858,
      "grad_norm": 0.07478845119476318,
      "learning_rate": 2.5130659187706636e-05,
      "loss": 0.0862,
      "step": 36430
    },
    {
      "epoch": 1.365254205537447,
      "grad_norm": 0.11398660391569138,
      "learning_rate": 2.5103792918920776e-05,
      "loss": 0.0944,
      "step": 36440
    },
    {
      "epoch": 1.3656288636619085,
      "grad_norm": 0.07440771162509918,
      "learning_rate": 2.5076936204334588e-05,
      "loss": 0.0934,
      "step": 36450
    },
    {
      "epoch": 1.36600352178637,
      "grad_norm": 0.09343214333057404,
      "learning_rate": 2.5050089054254648e-05,
      "loss": 0.0939,
      "step": 36460
    },
    {
      "epoch": 1.3663781799108314,
      "grad_norm": 0.07538861781358719,
      "learning_rate": 2.502325147898386e-05,
      "loss": 0.0911,
      "step": 36470
    },
    {
      "epoch": 1.3667528380352927,
      "grad_norm": 0.12793558835983276,
      "learning_rate": 2.4996423488821446e-05,
      "loss": 0.0907,
      "step": 36480
    },
    {
      "epoch": 1.3671274961597542,
      "grad_norm": 0.09788242727518082,
      "learning_rate": 2.4969605094062944e-05,
      "loss": 0.0863,
      "step": 36490
    },
    {
      "epoch": 1.3675021542842156,
      "grad_norm": 0.09102971851825714,
      "learning_rate": 2.494279630500021e-05,
      "loss": 0.0889,
      "step": 36500
    },
    {
      "epoch": 1.367876812408677,
      "grad_norm": 0.099639393389225,
      "learning_rate": 2.491599713192145e-05,
      "loss": 0.092,
      "step": 36510
    },
    {
      "epoch": 1.3682514705331386,
      "grad_norm": 0.06269904226064682,
      "learning_rate": 2.488920758511108e-05,
      "loss": 0.0876,
      "step": 36520
    },
    {
      "epoch": 1.3686261286576,
      "grad_norm": 0.08717460185289383,
      "learning_rate": 2.486242767484995e-05,
      "loss": 0.0894,
      "step": 36530
    },
    {
      "epoch": 1.3690007867820615,
      "grad_norm": 0.12641635537147522,
      "learning_rate": 2.4835657411415164e-05,
      "loss": 0.0898,
      "step": 36540
    },
    {
      "epoch": 1.3693754449065227,
      "grad_norm": 0.10678138583898544,
      "learning_rate": 2.480889680508006e-05,
      "loss": 0.0907,
      "step": 36550
    },
    {
      "epoch": 1.3697501030309842,
      "grad_norm": 0.06614997237920761,
      "learning_rate": 2.4782145866114344e-05,
      "loss": 0.0874,
      "step": 36560
    },
    {
      "epoch": 1.3701247611554457,
      "grad_norm": 0.08652042597532272,
      "learning_rate": 2.4755404604783972e-05,
      "loss": 0.0885,
      "step": 36570
    },
    {
      "epoch": 1.3704994192799071,
      "grad_norm": 0.06866782903671265,
      "learning_rate": 2.4728673031351263e-05,
      "loss": 0.0861,
      "step": 36580
    },
    {
      "epoch": 1.3708740774043684,
      "grad_norm": 0.0915914922952652,
      "learning_rate": 2.47019511560747e-05,
      "loss": 0.0915,
      "step": 36590
    },
    {
      "epoch": 1.3712487355288299,
      "grad_norm": 0.08460221439599991,
      "learning_rate": 2.467523898920912e-05,
      "loss": 0.0913,
      "step": 36600
    },
    {
      "epoch": 1.3716233936532913,
      "grad_norm": 0.15484467148780823,
      "learning_rate": 2.4648536541005617e-05,
      "loss": 0.0898,
      "step": 36610
    },
    {
      "epoch": 1.3719980517777528,
      "grad_norm": 0.09253361076116562,
      "learning_rate": 2.4621843821711564e-05,
      "loss": 0.0923,
      "step": 36620
    },
    {
      "epoch": 1.3723727099022143,
      "grad_norm": 0.08961928635835648,
      "learning_rate": 2.4595160841570593e-05,
      "loss": 0.0889,
      "step": 36630
    },
    {
      "epoch": 1.3727473680266757,
      "grad_norm": 0.0882679745554924,
      "learning_rate": 2.4568487610822582e-05,
      "loss": 0.089,
      "step": 36640
    },
    {
      "epoch": 1.3731220261511372,
      "grad_norm": 0.08402931690216064,
      "learning_rate": 2.4541824139703724e-05,
      "loss": 0.0921,
      "step": 36650
    },
    {
      "epoch": 1.3734966842755985,
      "grad_norm": 0.08859581500291824,
      "learning_rate": 2.451517043844635e-05,
      "loss": 0.095,
      "step": 36660
    },
    {
      "epoch": 1.37387134240006,
      "grad_norm": 0.0704246535897255,
      "learning_rate": 2.4488526517279188e-05,
      "loss": 0.0887,
      "step": 36670
    },
    {
      "epoch": 1.3742460005245214,
      "grad_norm": 0.06706589460372925,
      "learning_rate": 2.446189238642714e-05,
      "loss": 0.0908,
      "step": 36680
    },
    {
      "epoch": 1.3746206586489829,
      "grad_norm": 0.07951086759567261,
      "learning_rate": 2.4435268056111303e-05,
      "loss": 0.0898,
      "step": 36690
    },
    {
      "epoch": 1.374995316773444,
      "grad_norm": 0.07411562651395798,
      "learning_rate": 2.4408653536549104e-05,
      "loss": 0.0915,
      "step": 36700
    },
    {
      "epoch": 1.3753699748979056,
      "grad_norm": 0.070901058614254,
      "learning_rate": 2.4382048837954146e-05,
      "loss": 0.0886,
      "step": 36710
    },
    {
      "epoch": 1.375744633022367,
      "grad_norm": 0.07140680402517319,
      "learning_rate": 2.435545397053629e-05,
      "loss": 0.0876,
      "step": 36720
    },
    {
      "epoch": 1.3761192911468285,
      "grad_norm": 0.07935065031051636,
      "learning_rate": 2.432886894450162e-05,
      "loss": 0.0901,
      "step": 36730
    },
    {
      "epoch": 1.37649394927129,
      "grad_norm": 0.08660417795181274,
      "learning_rate": 2.4302293770052425e-05,
      "loss": 0.0899,
      "step": 36740
    },
    {
      "epoch": 1.3768686073957515,
      "grad_norm": 0.07220492511987686,
      "learning_rate": 2.4275728457387243e-05,
      "loss": 0.0831,
      "step": 36750
    },
    {
      "epoch": 1.377243265520213,
      "grad_norm": 0.10486385226249695,
      "learning_rate": 2.4249173016700804e-05,
      "loss": 0.0878,
      "step": 36760
    },
    {
      "epoch": 1.3776179236446742,
      "grad_norm": 0.0873013362288475,
      "learning_rate": 2.4222627458184056e-05,
      "loss": 0.0909,
      "step": 36770
    },
    {
      "epoch": 1.3779925817691356,
      "grad_norm": 0.06901217997074127,
      "learning_rate": 2.4196091792024174e-05,
      "loss": 0.0871,
      "step": 36780
    },
    {
      "epoch": 1.378367239893597,
      "grad_norm": 0.09985294193029404,
      "learning_rate": 2.4169566028404484e-05,
      "loss": 0.0913,
      "step": 36790
    },
    {
      "epoch": 1.3787418980180586,
      "grad_norm": 0.08402175456285477,
      "learning_rate": 2.414305017750455e-05,
      "loss": 0.0868,
      "step": 36800
    },
    {
      "epoch": 1.3791165561425198,
      "grad_norm": 0.09241822361946106,
      "learning_rate": 2.4116544249500157e-05,
      "loss": 0.0887,
      "step": 36810
    },
    {
      "epoch": 1.3794912142669813,
      "grad_norm": 0.0648314580321312,
      "learning_rate": 2.4090048254563263e-05,
      "loss": 0.0877,
      "step": 36820
    },
    {
      "epoch": 1.3798658723914428,
      "grad_norm": 0.07945011556148529,
      "learning_rate": 2.4063562202861955e-05,
      "loss": 0.0891,
      "step": 36830
    },
    {
      "epoch": 1.3802405305159042,
      "grad_norm": 0.07205728441476822,
      "learning_rate": 2.4037086104560585e-05,
      "loss": 0.0918,
      "step": 36840
    },
    {
      "epoch": 1.3806151886403657,
      "grad_norm": 0.09088271856307983,
      "learning_rate": 2.401061996981964e-05,
      "loss": 0.0879,
      "step": 36850
    },
    {
      "epoch": 1.3809898467648272,
      "grad_norm": 0.08521629869937897,
      "learning_rate": 2.3984163808795812e-05,
      "loss": 0.0889,
      "step": 36860
    },
    {
      "epoch": 1.3813645048892886,
      "grad_norm": 0.11408576369285583,
      "learning_rate": 2.395771763164194e-05,
      "loss": 0.089,
      "step": 36870
    },
    {
      "epoch": 1.3817391630137499,
      "grad_norm": 0.37453243136405945,
      "learning_rate": 2.393128144850705e-05,
      "loss": 0.0909,
      "step": 36880
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 0.06574957072734833,
      "learning_rate": 2.3904855269536315e-05,
      "loss": 0.0906,
      "step": 36890
    },
    {
      "epoch": 1.3824884792626728,
      "grad_norm": 0.09079492837190628,
      "learning_rate": 2.387843910487108e-05,
      "loss": 0.0889,
      "step": 36900
    },
    {
      "epoch": 1.3828631373871343,
      "grad_norm": 0.07715237885713577,
      "learning_rate": 2.3852032964648858e-05,
      "loss": 0.0876,
      "step": 36910
    },
    {
      "epoch": 1.3832377955115955,
      "grad_norm": 0.08374431729316711,
      "learning_rate": 2.3825636859003308e-05,
      "loss": 0.0909,
      "step": 36920
    },
    {
      "epoch": 1.383612453636057,
      "grad_norm": 0.08337900042533875,
      "learning_rate": 2.3799250798064198e-05,
      "loss": 0.0916,
      "step": 36930
    },
    {
      "epoch": 1.3839871117605185,
      "grad_norm": 0.08123259991407394,
      "learning_rate": 2.377287479195748e-05,
      "loss": 0.0926,
      "step": 36940
    },
    {
      "epoch": 1.38436176988498,
      "grad_norm": 0.07290194183588028,
      "learning_rate": 2.3746508850805292e-05,
      "loss": 0.087,
      "step": 36950
    },
    {
      "epoch": 1.3847364280094414,
      "grad_norm": 0.0687507688999176,
      "learning_rate": 2.372015298472582e-05,
      "loss": 0.0914,
      "step": 36960
    },
    {
      "epoch": 1.3851110861339029,
      "grad_norm": 0.09096384793519974,
      "learning_rate": 2.3693807203833423e-05,
      "loss": 0.0899,
      "step": 36970
    },
    {
      "epoch": 1.3854857442583643,
      "grad_norm": 0.07387751340866089,
      "learning_rate": 2.3667471518238603e-05,
      "loss": 0.0913,
      "step": 36980
    },
    {
      "epoch": 1.3858604023828256,
      "grad_norm": 0.08357047289609909,
      "learning_rate": 2.3641145938047975e-05,
      "loss": 0.0895,
      "step": 36990
    },
    {
      "epoch": 1.386235060507287,
      "grad_norm": 0.073510080575943,
      "learning_rate": 2.3614830473364273e-05,
      "loss": 0.0917,
      "step": 37000
    },
    {
      "epoch": 1.3866097186317485,
      "grad_norm": 0.08414394408464432,
      "learning_rate": 2.3588525134286355e-05,
      "loss": 0.0867,
      "step": 37010
    },
    {
      "epoch": 1.38698437675621,
      "grad_norm": 0.1037738174200058,
      "learning_rate": 2.3562229930909215e-05,
      "loss": 0.0907,
      "step": 37020
    },
    {
      "epoch": 1.3873590348806712,
      "grad_norm": 0.09837797284126282,
      "learning_rate": 2.353594487332386e-05,
      "loss": 0.0895,
      "step": 37030
    },
    {
      "epoch": 1.3877336930051327,
      "grad_norm": 0.09027814865112305,
      "learning_rate": 2.3509669971617553e-05,
      "loss": 0.0908,
      "step": 37040
    },
    {
      "epoch": 1.3881083511295942,
      "grad_norm": 0.08883419632911682,
      "learning_rate": 2.3483405235873553e-05,
      "loss": 0.088,
      "step": 37050
    },
    {
      "epoch": 1.3884830092540557,
      "grad_norm": 0.06913083046674728,
      "learning_rate": 2.345715067617127e-05,
      "loss": 0.0919,
      "step": 37060
    },
    {
      "epoch": 1.3888576673785171,
      "grad_norm": 0.11510403454303741,
      "learning_rate": 2.3430906302586148e-05,
      "loss": 0.0884,
      "step": 37070
    },
    {
      "epoch": 1.3892323255029786,
      "grad_norm": 0.09643633663654327,
      "learning_rate": 2.3404672125189765e-05,
      "loss": 0.0885,
      "step": 37080
    },
    {
      "epoch": 1.38960698362744,
      "grad_norm": 0.08286207914352417,
      "learning_rate": 2.3378448154049832e-05,
      "loss": 0.091,
      "step": 37090
    },
    {
      "epoch": 1.3899816417519013,
      "grad_norm": 0.0691254511475563,
      "learning_rate": 2.335223439923004e-05,
      "loss": 0.093,
      "step": 37100
    },
    {
      "epoch": 1.3903562998763628,
      "grad_norm": 0.08490384370088577,
      "learning_rate": 2.3326030870790228e-05,
      "loss": 0.0921,
      "step": 37110
    },
    {
      "epoch": 1.3907309580008242,
      "grad_norm": 0.09318285435438156,
      "learning_rate": 2.3299837578786298e-05,
      "loss": 0.0959,
      "step": 37120
    },
    {
      "epoch": 1.3911056161252857,
      "grad_norm": 0.07657378911972046,
      "learning_rate": 2.3273654533270224e-05,
      "loss": 0.0893,
      "step": 37130
    },
    {
      "epoch": 1.3914802742497472,
      "grad_norm": 0.06371092796325684,
      "learning_rate": 2.3247481744290038e-05,
      "loss": 0.0943,
      "step": 37140
    },
    {
      "epoch": 1.3918549323742084,
      "grad_norm": 0.0831713154911995,
      "learning_rate": 2.3221319221889848e-05,
      "loss": 0.0903,
      "step": 37150
    },
    {
      "epoch": 1.39222959049867,
      "grad_norm": 0.07440435886383057,
      "learning_rate": 2.3195166976109826e-05,
      "loss": 0.0924,
      "step": 37160
    },
    {
      "epoch": 1.3926042486231314,
      "grad_norm": 0.07627617567777634,
      "learning_rate": 2.316902501698615e-05,
      "loss": 0.0899,
      "step": 37170
    },
    {
      "epoch": 1.3929789067475928,
      "grad_norm": 0.08916016668081284,
      "learning_rate": 2.314289335455113e-05,
      "loss": 0.0932,
      "step": 37180
    },
    {
      "epoch": 1.3933535648720543,
      "grad_norm": 0.10587392747402191,
      "learning_rate": 2.31167719988331e-05,
      "loss": 0.0886,
      "step": 37190
    },
    {
      "epoch": 1.3937282229965158,
      "grad_norm": 0.06975886225700378,
      "learning_rate": 2.3090660959856373e-05,
      "loss": 0.0919,
      "step": 37200
    },
    {
      "epoch": 1.3941028811209772,
      "grad_norm": 0.06975201517343521,
      "learning_rate": 2.3064560247641388e-05,
      "loss": 0.0887,
      "step": 37210
    },
    {
      "epoch": 1.3944775392454385,
      "grad_norm": 0.09779784083366394,
      "learning_rate": 2.3038469872204576e-05,
      "loss": 0.0899,
      "step": 37220
    },
    {
      "epoch": 1.3948521973699,
      "grad_norm": 0.07461610436439514,
      "learning_rate": 2.301238984355841e-05,
      "loss": 0.0858,
      "step": 37230
    },
    {
      "epoch": 1.3952268554943614,
      "grad_norm": 0.09030592441558838,
      "learning_rate": 2.2986320171711412e-05,
      "loss": 0.0883,
      "step": 37240
    },
    {
      "epoch": 1.395601513618823,
      "grad_norm": 0.10788148641586304,
      "learning_rate": 2.2960260866668094e-05,
      "loss": 0.0896,
      "step": 37250
    },
    {
      "epoch": 1.3959761717432841,
      "grad_norm": 0.08522523939609528,
      "learning_rate": 2.293421193842903e-05,
      "loss": 0.0914,
      "step": 37260
    },
    {
      "epoch": 1.3963508298677456,
      "grad_norm": 0.07850922644138336,
      "learning_rate": 2.2908173396990734e-05,
      "loss": 0.0881,
      "step": 37270
    },
    {
      "epoch": 1.396725487992207,
      "grad_norm": 0.10156164318323135,
      "learning_rate": 2.2882145252345856e-05,
      "loss": 0.0919,
      "step": 37280
    },
    {
      "epoch": 1.3971001461166686,
      "grad_norm": 0.07598433643579483,
      "learning_rate": 2.2856127514482957e-05,
      "loss": 0.0889,
      "step": 37290
    },
    {
      "epoch": 1.39747480424113,
      "grad_norm": 0.08678463846445084,
      "learning_rate": 2.2830120193386656e-05,
      "loss": 0.0912,
      "step": 37300
    },
    {
      "epoch": 1.3978494623655915,
      "grad_norm": 0.0843053087592125,
      "learning_rate": 2.2804123299037505e-05,
      "loss": 0.0885,
      "step": 37310
    },
    {
      "epoch": 1.398224120490053,
      "grad_norm": 0.08398512750864029,
      "learning_rate": 2.2778136841412162e-05,
      "loss": 0.0923,
      "step": 37320
    },
    {
      "epoch": 1.3985987786145142,
      "grad_norm": 0.10171341896057129,
      "learning_rate": 2.275216083048322e-05,
      "loss": 0.0892,
      "step": 37330
    },
    {
      "epoch": 1.3989734367389757,
      "grad_norm": 0.0746261402964592,
      "learning_rate": 2.2726195276219232e-05,
      "loss": 0.0913,
      "step": 37340
    },
    {
      "epoch": 1.3993480948634371,
      "grad_norm": 0.07290264964103699,
      "learning_rate": 2.270024018858478e-05,
      "loss": 0.0915,
      "step": 37350
    },
    {
      "epoch": 1.3997227529878986,
      "grad_norm": 0.09058935940265656,
      "learning_rate": 2.2674295577540434e-05,
      "loss": 0.0902,
      "step": 37360
    },
    {
      "epoch": 1.4000974111123599,
      "grad_norm": 0.06176423281431198,
      "learning_rate": 2.2648361453042716e-05,
      "loss": 0.0877,
      "step": 37370
    },
    {
      "epoch": 1.4004720692368213,
      "grad_norm": 0.0678049698472023,
      "learning_rate": 2.262243782504415e-05,
      "loss": 0.0892,
      "step": 37380
    },
    {
      "epoch": 1.4008467273612828,
      "grad_norm": 0.0855301022529602,
      "learning_rate": 2.259652470349322e-05,
      "loss": 0.0874,
      "step": 37390
    },
    {
      "epoch": 1.4012213854857443,
      "grad_norm": 0.0651029422879219,
      "learning_rate": 2.2570622098334386e-05,
      "loss": 0.0864,
      "step": 37400
    },
    {
      "epoch": 1.4015960436102057,
      "grad_norm": 0.07269492000341415,
      "learning_rate": 2.254473001950802e-05,
      "loss": 0.0871,
      "step": 37410
    },
    {
      "epoch": 1.4019707017346672,
      "grad_norm": 0.08357894420623779,
      "learning_rate": 2.251884847695055e-05,
      "loss": 0.0907,
      "step": 37420
    },
    {
      "epoch": 1.4023453598591287,
      "grad_norm": 0.09370109438896179,
      "learning_rate": 2.2492977480594307e-05,
      "loss": 0.0918,
      "step": 37430
    },
    {
      "epoch": 1.40272001798359,
      "grad_norm": 0.07293644547462463,
      "learning_rate": 2.2467117040367547e-05,
      "loss": 0.0892,
      "step": 37440
    },
    {
      "epoch": 1.4030946761080514,
      "grad_norm": 0.07698111236095428,
      "learning_rate": 2.2441267166194496e-05,
      "loss": 0.0893,
      "step": 37450
    },
    {
      "epoch": 1.4034693342325129,
      "grad_norm": 0.08184162527322769,
      "learning_rate": 2.2415427867995393e-05,
      "loss": 0.0901,
      "step": 37460
    },
    {
      "epoch": 1.4038439923569743,
      "grad_norm": 0.0885045975446701,
      "learning_rate": 2.238959915568631e-05,
      "loss": 0.0886,
      "step": 37470
    },
    {
      "epoch": 1.4042186504814356,
      "grad_norm": 0.10635276883840561,
      "learning_rate": 2.236378103917931e-05,
      "loss": 0.0895,
      "step": 37480
    },
    {
      "epoch": 1.404593308605897,
      "grad_norm": 0.07289307564496994,
      "learning_rate": 2.23379735283824e-05,
      "loss": 0.0892,
      "step": 37490
    },
    {
      "epoch": 1.4049679667303585,
      "grad_norm": 0.11422918736934662,
      "learning_rate": 2.23121766331995e-05,
      "loss": 0.0884,
      "step": 37500
    },
    {
      "epoch": 1.40534262485482,
      "grad_norm": 0.09195181727409363,
      "learning_rate": 2.2286390363530457e-05,
      "loss": 0.0913,
      "step": 37510
    },
    {
      "epoch": 1.4057172829792814,
      "grad_norm": 0.0918794795870781,
      "learning_rate": 2.226061472927104e-05,
      "loss": 0.0924,
      "step": 37520
    },
    {
      "epoch": 1.406091941103743,
      "grad_norm": 0.10577225685119629,
      "learning_rate": 2.2234849740312942e-05,
      "loss": 0.0874,
      "step": 37530
    },
    {
      "epoch": 1.4064665992282044,
      "grad_norm": 0.07937005907297134,
      "learning_rate": 2.2209095406543763e-05,
      "loss": 0.0879,
      "step": 37540
    },
    {
      "epoch": 1.4068412573526656,
      "grad_norm": 0.06578762084245682,
      "learning_rate": 2.2183351737847023e-05,
      "loss": 0.0894,
      "step": 37550
    },
    {
      "epoch": 1.407215915477127,
      "grad_norm": 0.08450046181678772,
      "learning_rate": 2.2157618744102142e-05,
      "loss": 0.0896,
      "step": 37560
    },
    {
      "epoch": 1.4075905736015886,
      "grad_norm": 0.08908797055482864,
      "learning_rate": 2.2131896435184474e-05,
      "loss": 0.0908,
      "step": 37570
    },
    {
      "epoch": 1.40796523172605,
      "grad_norm": 0.11884354799985886,
      "learning_rate": 2.210618482096519e-05,
      "loss": 0.0916,
      "step": 37580
    },
    {
      "epoch": 1.4083398898505113,
      "grad_norm": 0.36270299553871155,
      "learning_rate": 2.2080483911311428e-05,
      "loss": 0.0906,
      "step": 37590
    },
    {
      "epoch": 1.4087145479749728,
      "grad_norm": 0.09231024980545044,
      "learning_rate": 2.2054793716086243e-05,
      "loss": 0.0883,
      "step": 37600
    },
    {
      "epoch": 1.4090892060994342,
      "grad_norm": 0.10171959549188614,
      "learning_rate": 2.202911424514849e-05,
      "loss": 0.0926,
      "step": 37610
    },
    {
      "epoch": 1.4094638642238957,
      "grad_norm": 0.10228992998600006,
      "learning_rate": 2.2003445508352973e-05,
      "loss": 0.093,
      "step": 37620
    },
    {
      "epoch": 1.4098385223483572,
      "grad_norm": 0.09452894330024719,
      "learning_rate": 2.1977787515550357e-05,
      "loss": 0.0876,
      "step": 37630
    },
    {
      "epoch": 1.4102131804728186,
      "grad_norm": 0.07557310909032822,
      "learning_rate": 2.1952140276587186e-05,
      "loss": 0.0895,
      "step": 37640
    },
    {
      "epoch": 1.41058783859728,
      "grad_norm": 0.08533558249473572,
      "learning_rate": 2.192650380130587e-05,
      "loss": 0.0883,
      "step": 37650
    },
    {
      "epoch": 1.4109624967217413,
      "grad_norm": 0.08146151900291443,
      "learning_rate": 2.1900878099544707e-05,
      "loss": 0.0892,
      "step": 37660
    },
    {
      "epoch": 1.4113371548462028,
      "grad_norm": 0.08987803757190704,
      "learning_rate": 2.1875263181137863e-05,
      "loss": 0.0905,
      "step": 37670
    },
    {
      "epoch": 1.4117118129706643,
      "grad_norm": 0.07922853529453278,
      "learning_rate": 2.184965905591529e-05,
      "loss": 0.0884,
      "step": 37680
    },
    {
      "epoch": 1.4120864710951258,
      "grad_norm": 0.08412953466176987,
      "learning_rate": 2.1824065733702915e-05,
      "loss": 0.0885,
      "step": 37690
    },
    {
      "epoch": 1.412461129219587,
      "grad_norm": 0.08882597088813782,
      "learning_rate": 2.179848322432245e-05,
      "loss": 0.0918,
      "step": 37700
    },
    {
      "epoch": 1.4128357873440485,
      "grad_norm": 0.10130085051059723,
      "learning_rate": 2.1772911537591496e-05,
      "loss": 0.0905,
      "step": 37710
    },
    {
      "epoch": 1.41321044546851,
      "grad_norm": 0.10916250944137573,
      "learning_rate": 2.174735068332342e-05,
      "loss": 0.0923,
      "step": 37720
    },
    {
      "epoch": 1.4135851035929714,
      "grad_norm": 0.07496986538171768,
      "learning_rate": 2.17218006713275e-05,
      "loss": 0.0883,
      "step": 37730
    },
    {
      "epoch": 1.4139597617174329,
      "grad_norm": 0.09227342158555984,
      "learning_rate": 2.16962615114089e-05,
      "loss": 0.0921,
      "step": 37740
    },
    {
      "epoch": 1.4143344198418943,
      "grad_norm": 0.07944832742214203,
      "learning_rate": 2.1670733213368493e-05,
      "loss": 0.0895,
      "step": 37750
    },
    {
      "epoch": 1.4147090779663558,
      "grad_norm": 0.08767774701118469,
      "learning_rate": 2.1645215787003077e-05,
      "loss": 0.0884,
      "step": 37760
    },
    {
      "epoch": 1.415083736090817,
      "grad_norm": 0.0829600989818573,
      "learning_rate": 2.1619709242105252e-05,
      "loss": 0.0935,
      "step": 37770
    },
    {
      "epoch": 1.4154583942152785,
      "grad_norm": 0.0898377075791359,
      "learning_rate": 2.1594213588463434e-05,
      "loss": 0.0926,
      "step": 37780
    },
    {
      "epoch": 1.41583305233974,
      "grad_norm": 0.07928434759378433,
      "learning_rate": 2.1568728835861872e-05,
      "loss": 0.0893,
      "step": 37790
    },
    {
      "epoch": 1.4162077104642015,
      "grad_norm": 0.08014921098947525,
      "learning_rate": 2.154325499408063e-05,
      "loss": 0.0884,
      "step": 37800
    },
    {
      "epoch": 1.4165823685886627,
      "grad_norm": 0.10665428638458252,
      "learning_rate": 2.15177920728956e-05,
      "loss": 0.0927,
      "step": 37810
    },
    {
      "epoch": 1.4169570267131242,
      "grad_norm": 0.11215991526842117,
      "learning_rate": 2.14923400820784e-05,
      "loss": 0.0897,
      "step": 37820
    },
    {
      "epoch": 1.4173316848375856,
      "grad_norm": 0.07146412879228592,
      "learning_rate": 2.1466899031396587e-05,
      "loss": 0.0903,
      "step": 37830
    },
    {
      "epoch": 1.4177063429620471,
      "grad_norm": 0.08426647633314133,
      "learning_rate": 2.1441468930613436e-05,
      "loss": 0.0884,
      "step": 37840
    },
    {
      "epoch": 1.4180810010865086,
      "grad_norm": 0.21205313503742218,
      "learning_rate": 2.1416049789488006e-05,
      "loss": 0.0888,
      "step": 37850
    },
    {
      "epoch": 1.41845565921097,
      "grad_norm": 0.0918654203414917,
      "learning_rate": 2.1390641617775193e-05,
      "loss": 0.0897,
      "step": 37860
    },
    {
      "epoch": 1.4188303173354315,
      "grad_norm": 0.1304018646478653,
      "learning_rate": 2.1365244425225678e-05,
      "loss": 0.0889,
      "step": 37870
    },
    {
      "epoch": 1.4192049754598928,
      "grad_norm": 0.0689280703663826,
      "learning_rate": 2.1339858221585907e-05,
      "loss": 0.0917,
      "step": 37880
    },
    {
      "epoch": 1.4195796335843542,
      "grad_norm": 0.06288717687129974,
      "learning_rate": 2.131448301659813e-05,
      "loss": 0.0867,
      "step": 37890
    },
    {
      "epoch": 1.4199542917088157,
      "grad_norm": 0.1152205765247345,
      "learning_rate": 2.1289118820000363e-05,
      "loss": 0.0875,
      "step": 37900
    },
    {
      "epoch": 1.4203289498332772,
      "grad_norm": 0.11947725713253021,
      "learning_rate": 2.1263765641526422e-05,
      "loss": 0.0896,
      "step": 37910
    },
    {
      "epoch": 1.4207036079577386,
      "grad_norm": 0.11420642584562302,
      "learning_rate": 2.1238423490905825e-05,
      "loss": 0.0905,
      "step": 37920
    },
    {
      "epoch": 1.4210782660822,
      "grad_norm": 0.11252742260694504,
      "learning_rate": 2.1213092377863946e-05,
      "loss": 0.0907,
      "step": 37930
    },
    {
      "epoch": 1.4214529242066614,
      "grad_norm": 0.09344097226858139,
      "learning_rate": 2.1187772312121884e-05,
      "loss": 0.0922,
      "step": 37940
    },
    {
      "epoch": 1.4218275823311228,
      "grad_norm": 0.07158556580543518,
      "learning_rate": 2.1162463303396514e-05,
      "loss": 0.09,
      "step": 37950
    },
    {
      "epoch": 1.4222022404555843,
      "grad_norm": 0.07050390541553497,
      "learning_rate": 2.113716536140039e-05,
      "loss": 0.0895,
      "step": 37960
    },
    {
      "epoch": 1.4225768985800458,
      "grad_norm": 0.07654320448637009,
      "learning_rate": 2.1111878495841947e-05,
      "loss": 0.0914,
      "step": 37970
    },
    {
      "epoch": 1.4229515567045072,
      "grad_norm": 0.08249533921480179,
      "learning_rate": 2.1086602716425296e-05,
      "loss": 0.0885,
      "step": 37980
    },
    {
      "epoch": 1.4233262148289685,
      "grad_norm": 0.08649876713752747,
      "learning_rate": 2.1061338032850264e-05,
      "loss": 0.0919,
      "step": 37990
    },
    {
      "epoch": 1.42370087295343,
      "grad_norm": 0.09051372855901718,
      "learning_rate": 2.1036084454812487e-05,
      "loss": 0.0889,
      "step": 38000
    },
    {
      "epoch": 1.4240755310778914,
      "grad_norm": 0.07103478908538818,
      "learning_rate": 2.1010841992003305e-05,
      "loss": 0.0872,
      "step": 38010
    },
    {
      "epoch": 1.424450189202353,
      "grad_norm": 0.0881793424487114,
      "learning_rate": 2.0985610654109788e-05,
      "loss": 0.0884,
      "step": 38020
    },
    {
      "epoch": 1.4248248473268144,
      "grad_norm": 0.08576977998018265,
      "learning_rate": 2.096039045081476e-05,
      "loss": 0.0897,
      "step": 38030
    },
    {
      "epoch": 1.4251995054512756,
      "grad_norm": 0.08738872408866882,
      "learning_rate": 2.093518139179675e-05,
      "loss": 0.088,
      "step": 38040
    },
    {
      "epoch": 1.425574163575737,
      "grad_norm": 0.09689055383205414,
      "learning_rate": 2.0909983486730032e-05,
      "loss": 0.0887,
      "step": 38050
    },
    {
      "epoch": 1.4259488217001985,
      "grad_norm": 0.09100235253572464,
      "learning_rate": 2.088479674528454e-05,
      "loss": 0.0897,
      "step": 38060
    },
    {
      "epoch": 1.42632347982466,
      "grad_norm": 0.10258488357067108,
      "learning_rate": 2.0859621177126028e-05,
      "loss": 0.0892,
      "step": 38070
    },
    {
      "epoch": 1.4266981379491215,
      "grad_norm": 0.09527096897363663,
      "learning_rate": 2.0834456791915885e-05,
      "loss": 0.0885,
      "step": 38080
    },
    {
      "epoch": 1.427072796073583,
      "grad_norm": 0.07325224578380585,
      "learning_rate": 2.080930359931121e-05,
      "loss": 0.0876,
      "step": 38090
    },
    {
      "epoch": 1.4274474541980444,
      "grad_norm": 0.0873946025967598,
      "learning_rate": 2.0784161608964813e-05,
      "loss": 0.0896,
      "step": 38100
    },
    {
      "epoch": 1.4278221123225057,
      "grad_norm": 0.08386486768722534,
      "learning_rate": 2.0759030830525272e-05,
      "loss": 0.0925,
      "step": 38110
    },
    {
      "epoch": 1.4281967704469671,
      "grad_norm": 0.07021252065896988,
      "learning_rate": 2.0733911273636757e-05,
      "loss": 0.088,
      "step": 38120
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.09123361110687256,
      "learning_rate": 2.0708802947939192e-05,
      "loss": 0.0918,
      "step": 38130
    },
    {
      "epoch": 1.42894608669589,
      "grad_norm": 0.0885687991976738,
      "learning_rate": 2.0683705863068182e-05,
      "loss": 0.0885,
      "step": 38140
    },
    {
      "epoch": 1.4293207448203513,
      "grad_norm": 0.06125950068235397,
      "learning_rate": 2.0658620028655017e-05,
      "loss": 0.0927,
      "step": 38150
    },
    {
      "epoch": 1.4296954029448128,
      "grad_norm": 0.06506627053022385,
      "learning_rate": 2.0633545454326665e-05,
      "loss": 0.0903,
      "step": 38160
    },
    {
      "epoch": 1.4300700610692743,
      "grad_norm": 0.0656951367855072,
      "learning_rate": 2.0608482149705784e-05,
      "loss": 0.0895,
      "step": 38170
    },
    {
      "epoch": 1.4304447191937357,
      "grad_norm": 0.07691771537065506,
      "learning_rate": 2.0583430124410684e-05,
      "loss": 0.0883,
      "step": 38180
    },
    {
      "epoch": 1.4308193773181972,
      "grad_norm": 0.08941812813282013,
      "learning_rate": 2.0558389388055398e-05,
      "loss": 0.0885,
      "step": 38190
    },
    {
      "epoch": 1.4311940354426587,
      "grad_norm": 0.07904063910245895,
      "learning_rate": 2.0533359950249526e-05,
      "loss": 0.0872,
      "step": 38200
    },
    {
      "epoch": 1.4315686935671201,
      "grad_norm": 0.06265529990196228,
      "learning_rate": 2.0508341820598452e-05,
      "loss": 0.0895,
      "step": 38210
    },
    {
      "epoch": 1.4319433516915814,
      "grad_norm": 0.06712011992931366,
      "learning_rate": 2.0483335008703165e-05,
      "loss": 0.0858,
      "step": 38220
    },
    {
      "epoch": 1.4323180098160428,
      "grad_norm": 0.07905445247888565,
      "learning_rate": 2.0458339524160275e-05,
      "loss": 0.087,
      "step": 38230
    },
    {
      "epoch": 1.4326926679405043,
      "grad_norm": 0.1040571928024292,
      "learning_rate": 2.043335537656208e-05,
      "loss": 0.0892,
      "step": 38240
    },
    {
      "epoch": 1.4330673260649658,
      "grad_norm": 0.06384596228599548,
      "learning_rate": 2.0408382575496583e-05,
      "loss": 0.0918,
      "step": 38250
    },
    {
      "epoch": 1.433441984189427,
      "grad_norm": 0.0954122468829155,
      "learning_rate": 2.038342113054732e-05,
      "loss": 0.0897,
      "step": 38260
    },
    {
      "epoch": 1.4338166423138885,
      "grad_norm": 0.09406110644340515,
      "learning_rate": 2.035847105129355e-05,
      "loss": 0.0891,
      "step": 38270
    },
    {
      "epoch": 1.43419130043835,
      "grad_norm": 0.07183333486318588,
      "learning_rate": 2.033353234731014e-05,
      "loss": 0.0924,
      "step": 38280
    },
    {
      "epoch": 1.4345659585628114,
      "grad_norm": 0.11447730660438538,
      "learning_rate": 2.0308605028167604e-05,
      "loss": 0.09,
      "step": 38290
    },
    {
      "epoch": 1.434940616687273,
      "grad_norm": 0.07898419350385666,
      "learning_rate": 2.028368910343208e-05,
      "loss": 0.091,
      "step": 38300
    },
    {
      "epoch": 1.4353152748117344,
      "grad_norm": 0.07322238385677338,
      "learning_rate": 2.0258784582665334e-05,
      "loss": 0.0885,
      "step": 38310
    },
    {
      "epoch": 1.4356899329361958,
      "grad_norm": 0.08894947171211243,
      "learning_rate": 2.0233891475424772e-05,
      "loss": 0.0905,
      "step": 38320
    },
    {
      "epoch": 1.436064591060657,
      "grad_norm": 0.08907197415828705,
      "learning_rate": 2.0209009791263357e-05,
      "loss": 0.0892,
      "step": 38330
    },
    {
      "epoch": 1.4364392491851186,
      "grad_norm": 0.0737539604306221,
      "learning_rate": 2.018413953972976e-05,
      "loss": 0.0853,
      "step": 38340
    },
    {
      "epoch": 1.43681390730958,
      "grad_norm": 0.0845630094408989,
      "learning_rate": 2.0159280730368223e-05,
      "loss": 0.0912,
      "step": 38350
    },
    {
      "epoch": 1.4371885654340415,
      "grad_norm": 0.0806894600391388,
      "learning_rate": 2.0134433372718565e-05,
      "loss": 0.0923,
      "step": 38360
    },
    {
      "epoch": 1.4375632235585027,
      "grad_norm": 0.08550377935171127,
      "learning_rate": 2.010959747631625e-05,
      "loss": 0.089,
      "step": 38370
    },
    {
      "epoch": 1.4379378816829642,
      "grad_norm": 0.07705315202474594,
      "learning_rate": 2.0084773050692314e-05,
      "loss": 0.0914,
      "step": 38380
    },
    {
      "epoch": 1.4383125398074257,
      "grad_norm": 0.3284752070903778,
      "learning_rate": 2.0059960105373465e-05,
      "loss": 0.0901,
      "step": 38390
    },
    {
      "epoch": 1.4386871979318872,
      "grad_norm": 0.07959844172000885,
      "learning_rate": 2.00351586498819e-05,
      "loss": 0.0878,
      "step": 38400
    },
    {
      "epoch": 1.4390618560563486,
      "grad_norm": 0.06417552381753922,
      "learning_rate": 2.001036869373547e-05,
      "loss": 0.0902,
      "step": 38410
    },
    {
      "epoch": 1.43943651418081,
      "grad_norm": 0.12591584026813507,
      "learning_rate": 1.9985590246447604e-05,
      "loss": 0.0896,
      "step": 38420
    },
    {
      "epoch": 1.4398111723052716,
      "grad_norm": 0.08470486849546432,
      "learning_rate": 1.9960823317527305e-05,
      "loss": 0.089,
      "step": 38430
    },
    {
      "epoch": 1.4401858304297328,
      "grad_norm": 0.08098940551280975,
      "learning_rate": 1.9936067916479166e-05,
      "loss": 0.0913,
      "step": 38440
    },
    {
      "epoch": 1.4405604885541943,
      "grad_norm": 0.09166958928108215,
      "learning_rate": 1.991132405280335e-05,
      "loss": 0.0937,
      "step": 38450
    },
    {
      "epoch": 1.4409351466786557,
      "grad_norm": 0.078369140625,
      "learning_rate": 1.9886591735995604e-05,
      "loss": 0.0909,
      "step": 38460
    },
    {
      "epoch": 1.4413098048031172,
      "grad_norm": 0.08645608276128769,
      "learning_rate": 1.9861870975547186e-05,
      "loss": 0.0931,
      "step": 38470
    },
    {
      "epoch": 1.4416844629275785,
      "grad_norm": 0.09997051954269409,
      "learning_rate": 1.9837161780945018e-05,
      "loss": 0.0918,
      "step": 38480
    },
    {
      "epoch": 1.44205912105204,
      "grad_norm": 0.07592833042144775,
      "learning_rate": 1.9812464161671534e-05,
      "loss": 0.0882,
      "step": 38490
    },
    {
      "epoch": 1.4424337791765014,
      "grad_norm": 0.08453758805990219,
      "learning_rate": 1.978777812720468e-05,
      "loss": 0.0913,
      "step": 38500
    },
    {
      "epoch": 1.4428084373009629,
      "grad_norm": 0.08390580117702484,
      "learning_rate": 1.976310368701803e-05,
      "loss": 0.0877,
      "step": 38510
    },
    {
      "epoch": 1.4431830954254243,
      "grad_norm": 0.08444932103157043,
      "learning_rate": 1.973844085058067e-05,
      "loss": 0.091,
      "step": 38520
    },
    {
      "epoch": 1.4435577535498858,
      "grad_norm": 0.10175731033086777,
      "learning_rate": 1.9713789627357237e-05,
      "loss": 0.0892,
      "step": 38530
    },
    {
      "epoch": 1.4439324116743473,
      "grad_norm": 0.10629238188266754,
      "learning_rate": 1.9689150026807923e-05,
      "loss": 0.0935,
      "step": 38540
    },
    {
      "epoch": 1.4443070697988085,
      "grad_norm": 0.08642904460430145,
      "learning_rate": 1.9664522058388446e-05,
      "loss": 0.0869,
      "step": 38550
    },
    {
      "epoch": 1.44468172792327,
      "grad_norm": 0.07034308463335037,
      "learning_rate": 1.963990573155009e-05,
      "loss": 0.0903,
      "step": 38560
    },
    {
      "epoch": 1.4450563860477315,
      "grad_norm": 0.06973941624164581,
      "learning_rate": 1.9615301055739594e-05,
      "loss": 0.089,
      "step": 38570
    },
    {
      "epoch": 1.445431044172193,
      "grad_norm": 0.06707300245761871,
      "learning_rate": 1.9590708040399332e-05,
      "loss": 0.0899,
      "step": 38580
    },
    {
      "epoch": 1.4458057022966542,
      "grad_norm": 0.08928891271352768,
      "learning_rate": 1.956612669496714e-05,
      "loss": 0.0878,
      "step": 38590
    },
    {
      "epoch": 1.4461803604211156,
      "grad_norm": 0.06880156695842743,
      "learning_rate": 1.9541557028876368e-05,
      "loss": 0.0896,
      "step": 38600
    },
    {
      "epoch": 1.446555018545577,
      "grad_norm": 0.10435472428798676,
      "learning_rate": 1.9516999051555884e-05,
      "loss": 0.0858,
      "step": 38610
    },
    {
      "epoch": 1.4469296766700386,
      "grad_norm": 0.08544891327619553,
      "learning_rate": 1.9492452772430136e-05,
      "loss": 0.0911,
      "step": 38620
    },
    {
      "epoch": 1.4473043347945,
      "grad_norm": 0.08772572129964828,
      "learning_rate": 1.9467918200919034e-05,
      "loss": 0.0858,
      "step": 38630
    },
    {
      "epoch": 1.4476789929189615,
      "grad_norm": 0.06754256039857864,
      "learning_rate": 1.944339534643796e-05,
      "loss": 0.0843,
      "step": 38640
    },
    {
      "epoch": 1.448053651043423,
      "grad_norm": 0.0884331464767456,
      "learning_rate": 1.941888421839784e-05,
      "loss": 0.0906,
      "step": 38650
    },
    {
      "epoch": 1.4484283091678842,
      "grad_norm": 0.07516973465681076,
      "learning_rate": 1.939438482620512e-05,
      "loss": 0.086,
      "step": 38660
    },
    {
      "epoch": 1.4488029672923457,
      "grad_norm": 0.10982967913150787,
      "learning_rate": 1.9369897179261697e-05,
      "loss": 0.0865,
      "step": 38670
    },
    {
      "epoch": 1.4491776254168072,
      "grad_norm": 0.0927799791097641,
      "learning_rate": 1.9345421286964988e-05,
      "loss": 0.0894,
      "step": 38680
    },
    {
      "epoch": 1.4495522835412686,
      "grad_norm": 0.4539504051208496,
      "learning_rate": 1.932095715870789e-05,
      "loss": 0.092,
      "step": 38690
    },
    {
      "epoch": 1.4499269416657299,
      "grad_norm": 0.09630027413368225,
      "learning_rate": 1.9296504803878812e-05,
      "loss": 0.0895,
      "step": 38700
    },
    {
      "epoch": 1.4503015997901914,
      "grad_norm": 0.08851872384548187,
      "learning_rate": 1.9272064231861564e-05,
      "loss": 0.0926,
      "step": 38710
    },
    {
      "epoch": 1.4506762579146528,
      "grad_norm": 0.08385337144136429,
      "learning_rate": 1.924763545203554e-05,
      "loss": 0.091,
      "step": 38720
    },
    {
      "epoch": 1.4510509160391143,
      "grad_norm": 0.07533279806375504,
      "learning_rate": 1.9223218473775555e-05,
      "loss": 0.0917,
      "step": 38730
    },
    {
      "epoch": 1.4514255741635758,
      "grad_norm": 0.08756725490093231,
      "learning_rate": 1.919881330645188e-05,
      "loss": 0.0879,
      "step": 38740
    },
    {
      "epoch": 1.4518002322880372,
      "grad_norm": 0.07439375668764114,
      "learning_rate": 1.917441995943026e-05,
      "loss": 0.0919,
      "step": 38750
    },
    {
      "epoch": 1.4521748904124987,
      "grad_norm": 0.08454812318086624,
      "learning_rate": 1.9150038442071972e-05,
      "loss": 0.0918,
      "step": 38760
    },
    {
      "epoch": 1.45254954853696,
      "grad_norm": 0.0727701187133789,
      "learning_rate": 1.9125668763733645e-05,
      "loss": 0.0872,
      "step": 38770
    },
    {
      "epoch": 1.4529242066614214,
      "grad_norm": 0.08045339584350586,
      "learning_rate": 1.9101310933767434e-05,
      "loss": 0.0881,
      "step": 38780
    },
    {
      "epoch": 1.4532988647858829,
      "grad_norm": 0.07352455705404282,
      "learning_rate": 1.9076964961520938e-05,
      "loss": 0.0903,
      "step": 38790
    },
    {
      "epoch": 1.4536735229103444,
      "grad_norm": 0.12611129879951477,
      "learning_rate": 1.905263085633719e-05,
      "loss": 0.0926,
      "step": 38800
    },
    {
      "epoch": 1.4540481810348058,
      "grad_norm": 0.10594616830348969,
      "learning_rate": 1.9028308627554677e-05,
      "loss": 0.0918,
      "step": 38810
    },
    {
      "epoch": 1.454422839159267,
      "grad_norm": 0.08912711590528488,
      "learning_rate": 1.9003998284507323e-05,
      "loss": 0.0895,
      "step": 38820
    },
    {
      "epoch": 1.4547974972837285,
      "grad_norm": 0.11433696746826172,
      "learning_rate": 1.897969983652453e-05,
      "loss": 0.0915,
      "step": 38830
    },
    {
      "epoch": 1.45517215540819,
      "grad_norm": 0.07077518850564957,
      "learning_rate": 1.8955413292931044e-05,
      "loss": 0.0913,
      "step": 38840
    },
    {
      "epoch": 1.4555468135326515,
      "grad_norm": 0.06753788888454437,
      "learning_rate": 1.893113866304712e-05,
      "loss": 0.0935,
      "step": 38850
    },
    {
      "epoch": 1.455921471657113,
      "grad_norm": 0.08479983359575272,
      "learning_rate": 1.8906875956188445e-05,
      "loss": 0.0915,
      "step": 38860
    },
    {
      "epoch": 1.4562961297815744,
      "grad_norm": 0.09691601991653442,
      "learning_rate": 1.8882625181666107e-05,
      "loss": 0.0917,
      "step": 38870
    },
    {
      "epoch": 1.4566707879060359,
      "grad_norm": 0.10415980964899063,
      "learning_rate": 1.8858386348786583e-05,
      "loss": 0.0917,
      "step": 38880
    },
    {
      "epoch": 1.4570454460304971,
      "grad_norm": 0.07078926265239716,
      "learning_rate": 1.8834159466851785e-05,
      "loss": 0.0922,
      "step": 38890
    },
    {
      "epoch": 1.4574201041549586,
      "grad_norm": 0.11100372672080994,
      "learning_rate": 1.880994454515912e-05,
      "loss": 0.0924,
      "step": 38900
    },
    {
      "epoch": 1.45779476227942,
      "grad_norm": 0.0787564143538475,
      "learning_rate": 1.8785741593001276e-05,
      "loss": 0.0878,
      "step": 38910
    },
    {
      "epoch": 1.4581694204038815,
      "grad_norm": 0.07569710165262222,
      "learning_rate": 1.876155061966643e-05,
      "loss": 0.0899,
      "step": 38920
    },
    {
      "epoch": 1.4585440785283428,
      "grad_norm": 0.06896937638521194,
      "learning_rate": 1.8737371634438132e-05,
      "loss": 0.0873,
      "step": 38930
    },
    {
      "epoch": 1.4589187366528042,
      "grad_norm": 0.07233670353889465,
      "learning_rate": 1.871320464659535e-05,
      "loss": 0.0917,
      "step": 38940
    },
    {
      "epoch": 1.4592933947772657,
      "grad_norm": 0.08216873556375504,
      "learning_rate": 1.8689049665412428e-05,
      "loss": 0.0891,
      "step": 38950
    },
    {
      "epoch": 1.4596680529017272,
      "grad_norm": 0.09113826602697372,
      "learning_rate": 1.866490670015912e-05,
      "loss": 0.0924,
      "step": 38960
    },
    {
      "epoch": 1.4600427110261887,
      "grad_norm": 0.08186259120702744,
      "learning_rate": 1.8640775760100575e-05,
      "loss": 0.0922,
      "step": 38970
    },
    {
      "epoch": 1.4604173691506501,
      "grad_norm": 0.08629851788282394,
      "learning_rate": 1.861665685449726e-05,
      "loss": 0.0914,
      "step": 38980
    },
    {
      "epoch": 1.4607920272751116,
      "grad_norm": 0.08263614028692245,
      "learning_rate": 1.859254999260513e-05,
      "loss": 0.0908,
      "step": 38990
    },
    {
      "epoch": 1.4611666853995728,
      "grad_norm": 0.0991126075387001,
      "learning_rate": 1.856845518367546e-05,
      "loss": 0.0885,
      "step": 39000
    },
    {
      "epoch": 1.4615413435240343,
      "grad_norm": 0.08167418092489243,
      "learning_rate": 1.8544372436954877e-05,
      "loss": 0.0906,
      "step": 39010
    },
    {
      "epoch": 1.4619160016484958,
      "grad_norm": 0.09998410940170288,
      "learning_rate": 1.8520301761685415e-05,
      "loss": 0.0907,
      "step": 39020
    },
    {
      "epoch": 1.4622906597729572,
      "grad_norm": 0.0692288726568222,
      "learning_rate": 1.8496243167104478e-05,
      "loss": 0.0881,
      "step": 39030
    },
    {
      "epoch": 1.4626653178974185,
      "grad_norm": 0.07360547035932541,
      "learning_rate": 1.8472196662444808e-05,
      "loss": 0.0882,
      "step": 39040
    },
    {
      "epoch": 1.46303997602188,
      "grad_norm": 0.1193593442440033,
      "learning_rate": 1.8448162256934532e-05,
      "loss": 0.0925,
      "step": 39050
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.07535877078771591,
      "learning_rate": 1.8424139959797128e-05,
      "loss": 0.091,
      "step": 39060
    },
    {
      "epoch": 1.463789292270803,
      "grad_norm": 0.18798570334911346,
      "learning_rate": 1.8400129780251406e-05,
      "loss": 0.091,
      "step": 39070
    },
    {
      "epoch": 1.4641639503952644,
      "grad_norm": 0.07150204479694366,
      "learning_rate": 1.837613172751156e-05,
      "loss": 0.0917,
      "step": 39080
    },
    {
      "epoch": 1.4645386085197258,
      "grad_norm": 0.06982622295618057,
      "learning_rate": 1.8352145810787096e-05,
      "loss": 0.0889,
      "step": 39090
    },
    {
      "epoch": 1.4649132666441873,
      "grad_norm": 0.08455988019704819,
      "learning_rate": 1.83281720392829e-05,
      "loss": 0.092,
      "step": 39100
    },
    {
      "epoch": 1.4652879247686486,
      "grad_norm": 0.09438871592283249,
      "learning_rate": 1.8304210422199175e-05,
      "loss": 0.0907,
      "step": 39110
    },
    {
      "epoch": 1.46566258289311,
      "grad_norm": 0.07606765627861023,
      "learning_rate": 1.828026096873142e-05,
      "loss": 0.0898,
      "step": 39120
    },
    {
      "epoch": 1.4660372410175715,
      "grad_norm": 0.0694027841091156,
      "learning_rate": 1.8256323688070558e-05,
      "loss": 0.0879,
      "step": 39130
    },
    {
      "epoch": 1.466411899142033,
      "grad_norm": 0.11729713529348373,
      "learning_rate": 1.8232398589402787e-05,
      "loss": 0.0891,
      "step": 39140
    },
    {
      "epoch": 1.4667865572664942,
      "grad_norm": 0.08679380267858505,
      "learning_rate": 1.82084856819096e-05,
      "loss": 0.0892,
      "step": 39150
    },
    {
      "epoch": 1.4671612153909557,
      "grad_norm": 0.08087529242038727,
      "learning_rate": 1.818458497476786e-05,
      "loss": 0.0912,
      "step": 39160
    },
    {
      "epoch": 1.4675358735154171,
      "grad_norm": 0.08266109973192215,
      "learning_rate": 1.8160696477149735e-05,
      "loss": 0.0867,
      "step": 39170
    },
    {
      "epoch": 1.4679105316398786,
      "grad_norm": 0.07215851545333862,
      "learning_rate": 1.8136820198222705e-05,
      "loss": 0.0874,
      "step": 39180
    },
    {
      "epoch": 1.46828518976434,
      "grad_norm": 0.08570482581853867,
      "learning_rate": 1.8112956147149556e-05,
      "loss": 0.088,
      "step": 39190
    },
    {
      "epoch": 1.4686598478888016,
      "grad_norm": 0.07317615300416946,
      "learning_rate": 1.8089104333088392e-05,
      "loss": 0.0912,
      "step": 39200
    },
    {
      "epoch": 1.469034506013263,
      "grad_norm": 0.08616526424884796,
      "learning_rate": 1.806526476519263e-05,
      "loss": 0.089,
      "step": 39210
    },
    {
      "epoch": 1.4694091641377243,
      "grad_norm": 0.08115368336439133,
      "learning_rate": 1.804143745261092e-05,
      "loss": 0.0861,
      "step": 39220
    },
    {
      "epoch": 1.4697838222621857,
      "grad_norm": 0.13309583067893982,
      "learning_rate": 1.801762240448731e-05,
      "loss": 0.0874,
      "step": 39230
    },
    {
      "epoch": 1.4701584803866472,
      "grad_norm": 0.08300447463989258,
      "learning_rate": 1.79938196299611e-05,
      "loss": 0.0911,
      "step": 39240
    },
    {
      "epoch": 1.4705331385111087,
      "grad_norm": 0.06707693636417389,
      "learning_rate": 1.7970029138166837e-05,
      "loss": 0.0936,
      "step": 39250
    },
    {
      "epoch": 1.47090779663557,
      "grad_norm": 0.07356882095336914,
      "learning_rate": 1.794625093823439e-05,
      "loss": 0.0911,
      "step": 39260
    },
    {
      "epoch": 1.4712824547600314,
      "grad_norm": 0.08006275445222855,
      "learning_rate": 1.7922485039288955e-05,
      "loss": 0.0901,
      "step": 39270
    },
    {
      "epoch": 1.4716571128844929,
      "grad_norm": 0.085082046687603,
      "learning_rate": 1.789873145045092e-05,
      "loss": 0.0921,
      "step": 39280
    },
    {
      "epoch": 1.4720317710089543,
      "grad_norm": 0.09006848186254501,
      "learning_rate": 1.7874990180835998e-05,
      "loss": 0.0911,
      "step": 39290
    },
    {
      "epoch": 1.4724064291334158,
      "grad_norm": 0.08846398442983627,
      "learning_rate": 1.7851261239555185e-05,
      "loss": 0.0882,
      "step": 39300
    },
    {
      "epoch": 1.4727810872578773,
      "grad_norm": 0.09242882579565048,
      "learning_rate": 1.7827544635714722e-05,
      "loss": 0.0883,
      "step": 39310
    },
    {
      "epoch": 1.4731557453823387,
      "grad_norm": 0.08196773380041122,
      "learning_rate": 1.7803840378416116e-05,
      "loss": 0.0882,
      "step": 39320
    },
    {
      "epoch": 1.4735304035068,
      "grad_norm": 0.08433501422405243,
      "learning_rate": 1.7780148476756147e-05,
      "loss": 0.0926,
      "step": 39330
    },
    {
      "epoch": 1.4739050616312614,
      "grad_norm": 0.08321794867515564,
      "learning_rate": 1.7756468939826854e-05,
      "loss": 0.09,
      "step": 39340
    },
    {
      "epoch": 1.474279719755723,
      "grad_norm": 0.06782093644142151,
      "learning_rate": 1.773280177671554e-05,
      "loss": 0.0939,
      "step": 39350
    },
    {
      "epoch": 1.4746543778801844,
      "grad_norm": 0.08525868505239487,
      "learning_rate": 1.770914699650469e-05,
      "loss": 0.0873,
      "step": 39360
    },
    {
      "epoch": 1.4750290360046456,
      "grad_norm": 0.08999866992235184,
      "learning_rate": 1.7685504608272147e-05,
      "loss": 0.0911,
      "step": 39370
    },
    {
      "epoch": 1.475403694129107,
      "grad_norm": 0.05954713001847267,
      "learning_rate": 1.7661874621090947e-05,
      "loss": 0.0923,
      "step": 39380
    },
    {
      "epoch": 1.4757783522535686,
      "grad_norm": 0.09453148394823074,
      "learning_rate": 1.7638257044029317e-05,
      "loss": 0.0876,
      "step": 39390
    },
    {
      "epoch": 1.47615301037803,
      "grad_norm": 0.0774114653468132,
      "learning_rate": 1.761465188615078e-05,
      "loss": 0.0927,
      "step": 39400
    },
    {
      "epoch": 1.4765276685024915,
      "grad_norm": 0.10184744745492935,
      "learning_rate": 1.7591059156514127e-05,
      "loss": 0.0866,
      "step": 39410
    },
    {
      "epoch": 1.476902326626953,
      "grad_norm": 0.08668730407953262,
      "learning_rate": 1.7567478864173293e-05,
      "loss": 0.0896,
      "step": 39420
    },
    {
      "epoch": 1.4772769847514144,
      "grad_norm": 0.08137121051549911,
      "learning_rate": 1.754391101817748e-05,
      "loss": 0.0868,
      "step": 39430
    },
    {
      "epoch": 1.4776516428758757,
      "grad_norm": 0.07314591854810715,
      "learning_rate": 1.752035562757112e-05,
      "loss": 0.0899,
      "step": 39440
    },
    {
      "epoch": 1.4780263010003372,
      "grad_norm": 0.11783327907323837,
      "learning_rate": 1.7496812701393866e-05,
      "loss": 0.0973,
      "step": 39450
    },
    {
      "epoch": 1.4784009591247986,
      "grad_norm": 0.07385530322790146,
      "learning_rate": 1.7473282248680577e-05,
      "loss": 0.0906,
      "step": 39460
    },
    {
      "epoch": 1.47877561724926,
      "grad_norm": 0.09464925527572632,
      "learning_rate": 1.7449764278461334e-05,
      "loss": 0.0887,
      "step": 39470
    },
    {
      "epoch": 1.4791502753737213,
      "grad_norm": 0.07626738399267197,
      "learning_rate": 1.742625879976143e-05,
      "loss": 0.0917,
      "step": 39480
    },
    {
      "epoch": 1.4795249334981828,
      "grad_norm": 0.09931595623493195,
      "learning_rate": 1.7402765821601325e-05,
      "loss": 0.0902,
      "step": 39490
    },
    {
      "epoch": 1.4798995916226443,
      "grad_norm": 0.08647594600915909,
      "learning_rate": 1.7379285352996722e-05,
      "loss": 0.0904,
      "step": 39500
    },
    {
      "epoch": 1.4802742497471058,
      "grad_norm": 0.07318922132253647,
      "learning_rate": 1.7355817402958534e-05,
      "loss": 0.0931,
      "step": 39510
    },
    {
      "epoch": 1.4806489078715672,
      "grad_norm": 0.07460997253656387,
      "learning_rate": 1.733236198049287e-05,
      "loss": 0.0911,
      "step": 39520
    },
    {
      "epoch": 1.4810235659960287,
      "grad_norm": 0.1213410273194313,
      "learning_rate": 1.7308919094600968e-05,
      "loss": 0.0926,
      "step": 39530
    },
    {
      "epoch": 1.4813982241204902,
      "grad_norm": 0.10304446518421173,
      "learning_rate": 1.72854887542793e-05,
      "loss": 0.0897,
      "step": 39540
    },
    {
      "epoch": 1.4817728822449514,
      "grad_norm": 0.09283453226089478,
      "learning_rate": 1.7262070968519573e-05,
      "loss": 0.0905,
      "step": 39550
    },
    {
      "epoch": 1.4821475403694129,
      "grad_norm": 0.07328509539365768,
      "learning_rate": 1.7238665746308576e-05,
      "loss": 0.0875,
      "step": 39560
    },
    {
      "epoch": 1.4825221984938743,
      "grad_norm": 0.09551026672124863,
      "learning_rate": 1.7215273096628343e-05,
      "loss": 0.0898,
      "step": 39570
    },
    {
      "epoch": 1.4828968566183358,
      "grad_norm": 0.09782732278108597,
      "learning_rate": 1.7191893028456063e-05,
      "loss": 0.0905,
      "step": 39580
    },
    {
      "epoch": 1.483271514742797,
      "grad_norm": 0.07019739598035812,
      "learning_rate": 1.7168525550764107e-05,
      "loss": 0.088,
      "step": 39590
    },
    {
      "epoch": 1.4836461728672585,
      "grad_norm": 0.0780642181634903,
      "learning_rate": 1.714517067252001e-05,
      "loss": 0.0933,
      "step": 39600
    },
    {
      "epoch": 1.48402083099172,
      "grad_norm": 0.08146350830793381,
      "learning_rate": 1.7121828402686462e-05,
      "loss": 0.0863,
      "step": 39610
    },
    {
      "epoch": 1.4843954891161815,
      "grad_norm": 0.10034140944480896,
      "learning_rate": 1.7098498750221337e-05,
      "loss": 0.0873,
      "step": 39620
    },
    {
      "epoch": 1.484770147240643,
      "grad_norm": 0.08710813522338867,
      "learning_rate": 1.7075181724077628e-05,
      "loss": 0.0887,
      "step": 39630
    },
    {
      "epoch": 1.4851448053651044,
      "grad_norm": 0.13642089068889618,
      "learning_rate": 1.70518773332035e-05,
      "loss": 0.0931,
      "step": 39640
    },
    {
      "epoch": 1.4855194634895659,
      "grad_norm": 0.07259907573461533,
      "learning_rate": 1.7028585586542317e-05,
      "loss": 0.0858,
      "step": 39650
    },
    {
      "epoch": 1.4858941216140271,
      "grad_norm": 0.1101110577583313,
      "learning_rate": 1.7005306493032513e-05,
      "loss": 0.0913,
      "step": 39660
    },
    {
      "epoch": 1.4862687797384886,
      "grad_norm": 0.10201188921928406,
      "learning_rate": 1.698204006160772e-05,
      "loss": 0.0895,
      "step": 39670
    },
    {
      "epoch": 1.48664343786295,
      "grad_norm": 0.09854031354188919,
      "learning_rate": 1.695878630119668e-05,
      "loss": 0.091,
      "step": 39680
    },
    {
      "epoch": 1.4870180959874115,
      "grad_norm": 0.07228489220142365,
      "learning_rate": 1.6935545220723288e-05,
      "loss": 0.0873,
      "step": 39690
    },
    {
      "epoch": 1.487392754111873,
      "grad_norm": 0.06567110866308212,
      "learning_rate": 1.6912316829106577e-05,
      "loss": 0.0855,
      "step": 39700
    },
    {
      "epoch": 1.4877674122363342,
      "grad_norm": 0.1227169930934906,
      "learning_rate": 1.6889101135260706e-05,
      "loss": 0.0897,
      "step": 39710
    },
    {
      "epoch": 1.4881420703607957,
      "grad_norm": 0.0870942622423172,
      "learning_rate": 1.6865898148094968e-05,
      "loss": 0.0882,
      "step": 39720
    },
    {
      "epoch": 1.4885167284852572,
      "grad_norm": 0.06495935469865799,
      "learning_rate": 1.684270787651372e-05,
      "loss": 0.0932,
      "step": 39730
    },
    {
      "epoch": 1.4888913866097186,
      "grad_norm": 0.07110661268234253,
      "learning_rate": 1.681953032941654e-05,
      "loss": 0.0862,
      "step": 39740
    },
    {
      "epoch": 1.4892660447341801,
      "grad_norm": 0.09275495260953903,
      "learning_rate": 1.6796365515698058e-05,
      "loss": 0.0923,
      "step": 39750
    },
    {
      "epoch": 1.4896407028586416,
      "grad_norm": 0.0690528154373169,
      "learning_rate": 1.6773213444248055e-05,
      "loss": 0.0893,
      "step": 39760
    },
    {
      "epoch": 1.490015360983103,
      "grad_norm": 0.10106014460325241,
      "learning_rate": 1.6750074123951342e-05,
      "loss": 0.0917,
      "step": 39770
    },
    {
      "epoch": 1.4903900191075643,
      "grad_norm": 0.0767718255519867,
      "learning_rate": 1.672694756368794e-05,
      "loss": 0.0903,
      "step": 39780
    },
    {
      "epoch": 1.4907646772320258,
      "grad_norm": 0.060165856033563614,
      "learning_rate": 1.6703833772332933e-05,
      "loss": 0.0909,
      "step": 39790
    },
    {
      "epoch": 1.4911393353564872,
      "grad_norm": 0.08368325978517532,
      "learning_rate": 1.6680732758756466e-05,
      "loss": 0.0887,
      "step": 39800
    },
    {
      "epoch": 1.4915139934809487,
      "grad_norm": 0.06909637153148651,
      "learning_rate": 1.6657644531823824e-05,
      "loss": 0.0875,
      "step": 39810
    },
    {
      "epoch": 1.49188865160541,
      "grad_norm": 0.10894325375556946,
      "learning_rate": 1.663456910039538e-05,
      "loss": 0.091,
      "step": 39820
    },
    {
      "epoch": 1.4922633097298714,
      "grad_norm": 0.07463017106056213,
      "learning_rate": 1.6611506473326586e-05,
      "loss": 0.0879,
      "step": 39830
    },
    {
      "epoch": 1.492637967854333,
      "grad_norm": 0.07535533607006073,
      "learning_rate": 1.658845665946799e-05,
      "loss": 0.0894,
      "step": 39840
    },
    {
      "epoch": 1.4930126259787944,
      "grad_norm": 0.06988506019115448,
      "learning_rate": 1.656541966766521e-05,
      "loss": 0.0895,
      "step": 39850
    },
    {
      "epoch": 1.4933872841032558,
      "grad_norm": 0.07414839416742325,
      "learning_rate": 1.6542395506758974e-05,
      "loss": 0.0925,
      "step": 39860
    },
    {
      "epoch": 1.4937619422277173,
      "grad_norm": 0.07481714338064194,
      "learning_rate": 1.6519384185585006e-05,
      "loss": 0.0871,
      "step": 39870
    },
    {
      "epoch": 1.4941366003521788,
      "grad_norm": 0.0931420847773552,
      "learning_rate": 1.6496385712974217e-05,
      "loss": 0.0902,
      "step": 39880
    },
    {
      "epoch": 1.49451125847664,
      "grad_norm": 0.0749664306640625,
      "learning_rate": 1.647340009775253e-05,
      "loss": 0.091,
      "step": 39890
    },
    {
      "epoch": 1.4948859166011015,
      "grad_norm": 0.09331227093935013,
      "learning_rate": 1.6450427348740888e-05,
      "loss": 0.093,
      "step": 39900
    },
    {
      "epoch": 1.495260574725563,
      "grad_norm": 0.11333504319190979,
      "learning_rate": 1.6427467474755358e-05,
      "loss": 0.0946,
      "step": 39910
    },
    {
      "epoch": 1.4956352328500244,
      "grad_norm": 0.08033459633588791,
      "learning_rate": 1.640452048460709e-05,
      "loss": 0.0883,
      "step": 39920
    },
    {
      "epoch": 1.4960098909744857,
      "grad_norm": 0.09238141030073166,
      "learning_rate": 1.6381586387102206e-05,
      "loss": 0.0875,
      "step": 39930
    },
    {
      "epoch": 1.4963845490989471,
      "grad_norm": 0.07411590963602066,
      "learning_rate": 1.6358665191041933e-05,
      "loss": 0.09,
      "step": 39940
    },
    {
      "epoch": 1.4967592072234086,
      "grad_norm": 0.10594002902507782,
      "learning_rate": 1.6335756905222542e-05,
      "loss": 0.093,
      "step": 39950
    },
    {
      "epoch": 1.49713386534787,
      "grad_norm": 0.0963989719748497,
      "learning_rate": 1.6312861538435352e-05,
      "loss": 0.0901,
      "step": 39960
    },
    {
      "epoch": 1.4975085234723315,
      "grad_norm": 0.08187178522348404,
      "learning_rate": 1.6289979099466707e-05,
      "loss": 0.0926,
      "step": 39970
    },
    {
      "epoch": 1.497883181596793,
      "grad_norm": 0.08753754198551178,
      "learning_rate": 1.6267109597098007e-05,
      "loss": 0.0923,
      "step": 39980
    },
    {
      "epoch": 1.4982578397212545,
      "grad_norm": 0.09612475335597992,
      "learning_rate": 1.624425304010568e-05,
      "loss": 0.0898,
      "step": 39990
    },
    {
      "epoch": 1.4986324978457157,
      "grad_norm": 0.08554362505674362,
      "learning_rate": 1.6221409437261197e-05,
      "loss": 0.0914,
      "step": 40000
    },
    {
      "epoch": 1.4990071559701772,
      "grad_norm": 0.06560219824314117,
      "learning_rate": 1.619857879733101e-05,
      "loss": 0.0882,
      "step": 40010
    },
    {
      "epoch": 1.4993818140946387,
      "grad_norm": 0.10470511764287949,
      "learning_rate": 1.6175761129076673e-05,
      "loss": 0.0902,
      "step": 40020
    },
    {
      "epoch": 1.4997564722191001,
      "grad_norm": 0.08062567561864853,
      "learning_rate": 1.6152956441254728e-05,
      "loss": 0.0924,
      "step": 40030
    },
    {
      "epoch": 1.5001311303435614,
      "grad_norm": 0.07157012820243835,
      "learning_rate": 1.61301647426167e-05,
      "loss": 0.0869,
      "step": 40040
    },
    {
      "epoch": 1.5005057884680228,
      "grad_norm": 0.0937017947435379,
      "learning_rate": 1.6107386041909157e-05,
      "loss": 0.0915,
      "step": 40050
    },
    {
      "epoch": 1.5008804465924843,
      "grad_norm": 0.09929195046424866,
      "learning_rate": 1.6084620347873724e-05,
      "loss": 0.0894,
      "step": 40060
    },
    {
      "epoch": 1.5012551047169458,
      "grad_norm": 0.10835715383291245,
      "learning_rate": 1.6061867669246955e-05,
      "loss": 0.0912,
      "step": 40070
    },
    {
      "epoch": 1.5016297628414073,
      "grad_norm": 0.1432236284017563,
      "learning_rate": 1.6039128014760464e-05,
      "loss": 0.0905,
      "step": 40080
    },
    {
      "epoch": 1.5020044209658687,
      "grad_norm": 0.13168266415596008,
      "learning_rate": 1.6016401393140833e-05,
      "loss": 0.0881,
      "step": 40090
    },
    {
      "epoch": 1.5023790790903302,
      "grad_norm": 0.09031569212675095,
      "learning_rate": 1.5993687813109674e-05,
      "loss": 0.0936,
      "step": 40100
    },
    {
      "epoch": 1.5027537372147917,
      "grad_norm": 0.07924957573413849,
      "learning_rate": 1.597098728338357e-05,
      "loss": 0.0872,
      "step": 40110
    },
    {
      "epoch": 1.503128395339253,
      "grad_norm": 0.07972666621208191,
      "learning_rate": 1.5948299812674118e-05,
      "loss": 0.09,
      "step": 40120
    },
    {
      "epoch": 1.5035030534637144,
      "grad_norm": 0.07426273077726364,
      "learning_rate": 1.592562540968789e-05,
      "loss": 0.0917,
      "step": 40130
    },
    {
      "epoch": 1.5038777115881756,
      "grad_norm": 0.0852106586098671,
      "learning_rate": 1.590296408312642e-05,
      "loss": 0.0884,
      "step": 40140
    },
    {
      "epoch": 1.504252369712637,
      "grad_norm": 0.08304060250520706,
      "learning_rate": 1.5880315841686244e-05,
      "loss": 0.0881,
      "step": 40150
    },
    {
      "epoch": 1.5046270278370986,
      "grad_norm": 0.10442209988832474,
      "learning_rate": 1.585768069405893e-05,
      "loss": 0.0887,
      "step": 40160
    },
    {
      "epoch": 1.50500168596156,
      "grad_norm": 0.08928465098142624,
      "learning_rate": 1.5835058648930923e-05,
      "loss": 0.0902,
      "step": 40170
    },
    {
      "epoch": 1.5053763440860215,
      "grad_norm": 0.08683357387781143,
      "learning_rate": 1.5812449714983708e-05,
      "loss": 0.0913,
      "step": 40180
    },
    {
      "epoch": 1.505751002210483,
      "grad_norm": 0.1082390770316124,
      "learning_rate": 1.5789853900893686e-05,
      "loss": 0.0879,
      "step": 40190
    },
    {
      "epoch": 1.5061256603349444,
      "grad_norm": 0.12610462307929993,
      "learning_rate": 1.5767271215332317e-05,
      "loss": 0.0895,
      "step": 40200
    },
    {
      "epoch": 1.506500318459406,
      "grad_norm": 0.12592117488384247,
      "learning_rate": 1.574470166696591e-05,
      "loss": 0.0899,
      "step": 40210
    },
    {
      "epoch": 1.5068749765838674,
      "grad_norm": 0.08287074416875839,
      "learning_rate": 1.57221452644558e-05,
      "loss": 0.0899,
      "step": 40220
    },
    {
      "epoch": 1.5072496347083286,
      "grad_norm": 0.07775837182998657,
      "learning_rate": 1.5699602016458253e-05,
      "loss": 0.0853,
      "step": 40230
    },
    {
      "epoch": 1.50762429283279,
      "grad_norm": 0.08243976533412933,
      "learning_rate": 1.5677071931624505e-05,
      "loss": 0.0916,
      "step": 40240
    },
    {
      "epoch": 1.5079989509572516,
      "grad_norm": 0.09741930663585663,
      "learning_rate": 1.5654555018600726e-05,
      "loss": 0.0869,
      "step": 40250
    },
    {
      "epoch": 1.5083736090817128,
      "grad_norm": 0.05713864415884018,
      "learning_rate": 1.5632051286028025e-05,
      "loss": 0.0911,
      "step": 40260
    },
    {
      "epoch": 1.5087482672061743,
      "grad_norm": 0.08593679219484329,
      "learning_rate": 1.5609560742542496e-05,
      "loss": 0.0918,
      "step": 40270
    },
    {
      "epoch": 1.5091229253306357,
      "grad_norm": 0.11309045553207397,
      "learning_rate": 1.5587083396775087e-05,
      "loss": 0.0913,
      "step": 40280
    },
    {
      "epoch": 1.5094975834550972,
      "grad_norm": 0.0971418172121048,
      "learning_rate": 1.556461925735175e-05,
      "loss": 0.0895,
      "step": 40290
    },
    {
      "epoch": 1.5098722415795587,
      "grad_norm": 0.07985914498567581,
      "learning_rate": 1.5542168332893392e-05,
      "loss": 0.0915,
      "step": 40300
    },
    {
      "epoch": 1.5102468997040202,
      "grad_norm": 0.09726453572511673,
      "learning_rate": 1.5519730632015756e-05,
      "loss": 0.0904,
      "step": 40310
    },
    {
      "epoch": 1.5106215578284816,
      "grad_norm": 0.09808401018381119,
      "learning_rate": 1.5497306163329583e-05,
      "loss": 0.0865,
      "step": 40320
    },
    {
      "epoch": 1.510996215952943,
      "grad_norm": 0.1045648455619812,
      "learning_rate": 1.5474894935440514e-05,
      "loss": 0.091,
      "step": 40330
    },
    {
      "epoch": 1.5113708740774043,
      "grad_norm": 0.0819440707564354,
      "learning_rate": 1.545249695694911e-05,
      "loss": 0.0902,
      "step": 40340
    },
    {
      "epoch": 1.5117455322018658,
      "grad_norm": 0.11386105418205261,
      "learning_rate": 1.543011223645085e-05,
      "loss": 0.0873,
      "step": 40350
    },
    {
      "epoch": 1.5121201903263273,
      "grad_norm": 0.09077037870883942,
      "learning_rate": 1.540774078253612e-05,
      "loss": 0.093,
      "step": 40360
    },
    {
      "epoch": 1.5124948484507885,
      "grad_norm": 0.08785600960254669,
      "learning_rate": 1.5385382603790226e-05,
      "loss": 0.0916,
      "step": 40370
    },
    {
      "epoch": 1.51286950657525,
      "grad_norm": 0.07635008543729782,
      "learning_rate": 1.5363037708793338e-05,
      "loss": 0.0881,
      "step": 40380
    },
    {
      "epoch": 1.5132441646997115,
      "grad_norm": 0.07920560240745544,
      "learning_rate": 1.53407061061206e-05,
      "loss": 0.089,
      "step": 40390
    },
    {
      "epoch": 1.513618822824173,
      "grad_norm": 0.1101083755493164,
      "learning_rate": 1.5318387804342016e-05,
      "loss": 0.0943,
      "step": 40400
    },
    {
      "epoch": 1.5139934809486344,
      "grad_norm": 0.06413683295249939,
      "learning_rate": 1.529608281202245e-05,
      "loss": 0.0891,
      "step": 40410
    },
    {
      "epoch": 1.5143681390730959,
      "grad_norm": 0.0908556655049324,
      "learning_rate": 1.527379113772172e-05,
      "loss": 0.0915,
      "step": 40420
    },
    {
      "epoch": 1.5147427971975573,
      "grad_norm": 0.11781454086303711,
      "learning_rate": 1.5251512789994483e-05,
      "loss": 0.0918,
      "step": 40430
    },
    {
      "epoch": 1.5151174553220188,
      "grad_norm": 0.09787321090698242,
      "learning_rate": 1.5229247777390354e-05,
      "loss": 0.0913,
      "step": 40440
    },
    {
      "epoch": 1.51549211344648,
      "grad_norm": 0.0960405096411705,
      "learning_rate": 1.5206996108453742e-05,
      "loss": 0.0917,
      "step": 40450
    },
    {
      "epoch": 1.5158667715709415,
      "grad_norm": 0.06953053921461105,
      "learning_rate": 1.518475779172398e-05,
      "loss": 0.0881,
      "step": 40460
    },
    {
      "epoch": 1.516241429695403,
      "grad_norm": 0.08150763064622879,
      "learning_rate": 1.5162532835735276e-05,
      "loss": 0.0889,
      "step": 40470
    },
    {
      "epoch": 1.5166160878198642,
      "grad_norm": 0.08916190266609192,
      "learning_rate": 1.5140321249016715e-05,
      "loss": 0.088,
      "step": 40480
    },
    {
      "epoch": 1.5169907459443257,
      "grad_norm": 0.08240412175655365,
      "learning_rate": 1.511812304009223e-05,
      "loss": 0.0914,
      "step": 40490
    },
    {
      "epoch": 1.5173654040687872,
      "grad_norm": 0.06781024485826492,
      "learning_rate": 1.5095938217480649e-05,
      "loss": 0.0894,
      "step": 40500
    },
    {
      "epoch": 1.5177400621932486,
      "grad_norm": 0.07839763909578323,
      "learning_rate": 1.5073766789695649e-05,
      "loss": 0.0895,
      "step": 40510
    },
    {
      "epoch": 1.51811472031771,
      "grad_norm": 0.08095210045576096,
      "learning_rate": 1.5051608765245723e-05,
      "loss": 0.0901,
      "step": 40520
    },
    {
      "epoch": 1.5184893784421716,
      "grad_norm": 0.10005202144384384,
      "learning_rate": 1.5029464152634314e-05,
      "loss": 0.0911,
      "step": 40530
    },
    {
      "epoch": 1.518864036566633,
      "grad_norm": 0.16582104563713074,
      "learning_rate": 1.500733296035966e-05,
      "loss": 0.0895,
      "step": 40540
    },
    {
      "epoch": 1.5192386946910945,
      "grad_norm": 0.07893107831478119,
      "learning_rate": 1.4985215196914826e-05,
      "loss": 0.0894,
      "step": 40550
    },
    {
      "epoch": 1.5196133528155558,
      "grad_norm": 0.07798276096582413,
      "learning_rate": 1.4963110870787756e-05,
      "loss": 0.0907,
      "step": 40560
    },
    {
      "epoch": 1.5199880109400172,
      "grad_norm": 0.08598059415817261,
      "learning_rate": 1.4941019990461275e-05,
      "loss": 0.0869,
      "step": 40570
    },
    {
      "epoch": 1.5203626690644787,
      "grad_norm": 0.10735493153333664,
      "learning_rate": 1.4918942564412963e-05,
      "loss": 0.0916,
      "step": 40580
    },
    {
      "epoch": 1.52073732718894,
      "grad_norm": 0.08593898266553879,
      "learning_rate": 1.4896878601115294e-05,
      "loss": 0.0855,
      "step": 40590
    },
    {
      "epoch": 1.5211119853134014,
      "grad_norm": 0.08878771215677261,
      "learning_rate": 1.4874828109035565e-05,
      "loss": 0.0915,
      "step": 40600
    },
    {
      "epoch": 1.5214866434378629,
      "grad_norm": 0.11275731027126312,
      "learning_rate": 1.4852791096635903e-05,
      "loss": 0.0899,
      "step": 40610
    },
    {
      "epoch": 1.5218613015623244,
      "grad_norm": 0.14811092615127563,
      "learning_rate": 1.4830767572373256e-05,
      "loss": 0.0891,
      "step": 40620
    },
    {
      "epoch": 1.5222359596867858,
      "grad_norm": 0.10167571902275085,
      "learning_rate": 1.4808757544699397e-05,
      "loss": 0.0871,
      "step": 40630
    },
    {
      "epoch": 1.5226106178112473,
      "grad_norm": 0.08333350718021393,
      "learning_rate": 1.4786761022060941e-05,
      "loss": 0.089,
      "step": 40640
    },
    {
      "epoch": 1.5229852759357088,
      "grad_norm": 0.06757216155529022,
      "learning_rate": 1.4764778012899266e-05,
      "loss": 0.0875,
      "step": 40650
    },
    {
      "epoch": 1.5233599340601702,
      "grad_norm": 0.09766467660665512,
      "learning_rate": 1.474280852565061e-05,
      "loss": 0.09,
      "step": 40660
    },
    {
      "epoch": 1.5237345921846315,
      "grad_norm": 0.08985378593206406,
      "learning_rate": 1.472085256874603e-05,
      "loss": 0.0903,
      "step": 40670
    },
    {
      "epoch": 1.524109250309093,
      "grad_norm": 0.10172932595014572,
      "learning_rate": 1.4698910150611379e-05,
      "loss": 0.0913,
      "step": 40680
    },
    {
      "epoch": 1.5244839084335544,
      "grad_norm": 0.08453468978404999,
      "learning_rate": 1.4676981279667269e-05,
      "loss": 0.089,
      "step": 40690
    },
    {
      "epoch": 1.5248585665580157,
      "grad_norm": 0.07764405012130737,
      "learning_rate": 1.4655065964329157e-05,
      "loss": 0.0847,
      "step": 40700
    },
    {
      "epoch": 1.5252332246824771,
      "grad_norm": 0.08776929974555969,
      "learning_rate": 1.4633164213007333e-05,
      "loss": 0.0908,
      "step": 40710
    },
    {
      "epoch": 1.5256078828069386,
      "grad_norm": 0.09878423064947128,
      "learning_rate": 1.4611276034106803e-05,
      "loss": 0.093,
      "step": 40720
    },
    {
      "epoch": 1.5259825409314,
      "grad_norm": 0.10753887891769409,
      "learning_rate": 1.4589401436027405e-05,
      "loss": 0.09,
      "step": 40730
    },
    {
      "epoch": 1.5263571990558615,
      "grad_norm": 0.10100670903921127,
      "learning_rate": 1.4567540427163779e-05,
      "loss": 0.0878,
      "step": 40740
    },
    {
      "epoch": 1.526731857180323,
      "grad_norm": 0.11081292480230331,
      "learning_rate": 1.4545693015905315e-05,
      "loss": 0.0908,
      "step": 40750
    },
    {
      "epoch": 1.5271065153047845,
      "grad_norm": 0.09258614480495453,
      "learning_rate": 1.4523859210636214e-05,
      "loss": 0.0916,
      "step": 40760
    },
    {
      "epoch": 1.527481173429246,
      "grad_norm": 0.08507488667964935,
      "learning_rate": 1.4502039019735447e-05,
      "loss": 0.0879,
      "step": 40770
    },
    {
      "epoch": 1.5278558315537072,
      "grad_norm": 0.08996699750423431,
      "learning_rate": 1.4480232451576764e-05,
      "loss": 0.0929,
      "step": 40780
    },
    {
      "epoch": 1.5282304896781687,
      "grad_norm": 0.08967641741037369,
      "learning_rate": 1.4458439514528655e-05,
      "loss": 0.0875,
      "step": 40790
    },
    {
      "epoch": 1.5286051478026301,
      "grad_norm": 0.06910089403390884,
      "learning_rate": 1.44366602169544e-05,
      "loss": 0.0873,
      "step": 40800
    },
    {
      "epoch": 1.5289798059270914,
      "grad_norm": 0.11254207044839859,
      "learning_rate": 1.4414894567212107e-05,
      "loss": 0.0939,
      "step": 40810
    },
    {
      "epoch": 1.5293544640515528,
      "grad_norm": 0.09885277599096298,
      "learning_rate": 1.4393142573654528e-05,
      "loss": 0.0848,
      "step": 40820
    },
    {
      "epoch": 1.5297291221760143,
      "grad_norm": 0.09936301410198212,
      "learning_rate": 1.4371404244629267e-05,
      "loss": 0.0918,
      "step": 40830
    },
    {
      "epoch": 1.5301037803004758,
      "grad_norm": 0.07662241905927658,
      "learning_rate": 1.4349679588478637e-05,
      "loss": 0.088,
      "step": 40840
    },
    {
      "epoch": 1.5304784384249372,
      "grad_norm": 0.09401235729455948,
      "learning_rate": 1.4327968613539733e-05,
      "loss": 0.0937,
      "step": 40850
    },
    {
      "epoch": 1.5308530965493987,
      "grad_norm": 0.0896931067109108,
      "learning_rate": 1.4306271328144377e-05,
      "loss": 0.0913,
      "step": 40860
    },
    {
      "epoch": 1.5312277546738602,
      "grad_norm": 0.07064312696456909,
      "learning_rate": 1.428458774061915e-05,
      "loss": 0.0875,
      "step": 40870
    },
    {
      "epoch": 1.5316024127983217,
      "grad_norm": 0.08626799285411835,
      "learning_rate": 1.4262917859285369e-05,
      "loss": 0.0874,
      "step": 40880
    },
    {
      "epoch": 1.5319770709227831,
      "grad_norm": 0.07811125367879868,
      "learning_rate": 1.4241261692459096e-05,
      "loss": 0.088,
      "step": 40890
    },
    {
      "epoch": 1.5323517290472444,
      "grad_norm": 0.09362536668777466,
      "learning_rate": 1.4219619248451132e-05,
      "loss": 0.091,
      "step": 40900
    },
    {
      "epoch": 1.5327263871717058,
      "grad_norm": 0.08628316968679428,
      "learning_rate": 1.4197990535567008e-05,
      "loss": 0.0885,
      "step": 40910
    },
    {
      "epoch": 1.533101045296167,
      "grad_norm": 0.10219354927539825,
      "learning_rate": 1.4176375562107002e-05,
      "loss": 0.0901,
      "step": 40920
    },
    {
      "epoch": 1.5334757034206286,
      "grad_norm": 0.07797727733850479,
      "learning_rate": 1.4154774336366066e-05,
      "loss": 0.091,
      "step": 40930
    },
    {
      "epoch": 1.53385036154509,
      "grad_norm": 0.07787350565195084,
      "learning_rate": 1.4133186866633924e-05,
      "loss": 0.0891,
      "step": 40940
    },
    {
      "epoch": 1.5342250196695515,
      "grad_norm": 0.0856454148888588,
      "learning_rate": 1.4111613161195048e-05,
      "loss": 0.0896,
      "step": 40950
    },
    {
      "epoch": 1.534599677794013,
      "grad_norm": 0.09891956299543381,
      "learning_rate": 1.4090053228328548e-05,
      "loss": 0.0884,
      "step": 40960
    },
    {
      "epoch": 1.5349743359184744,
      "grad_norm": 0.07194090634584427,
      "learning_rate": 1.4068507076308302e-05,
      "loss": 0.0892,
      "step": 40970
    },
    {
      "epoch": 1.535348994042936,
      "grad_norm": 0.08677150309085846,
      "learning_rate": 1.4046974713402893e-05,
      "loss": 0.0917,
      "step": 40980
    },
    {
      "epoch": 1.5357236521673974,
      "grad_norm": 0.08988925069570541,
      "learning_rate": 1.402545614787561e-05,
      "loss": 0.0902,
      "step": 40990
    },
    {
      "epoch": 1.5360983102918588,
      "grad_norm": 0.08474014699459076,
      "learning_rate": 1.4003951387984437e-05,
      "loss": 0.0884,
      "step": 41000
    },
    {
      "epoch": 1.53647296841632,
      "grad_norm": 0.08396445959806442,
      "learning_rate": 1.3982460441982076e-05,
      "loss": 0.0903,
      "step": 41010
    },
    {
      "epoch": 1.5368476265407816,
      "grad_norm": 0.08836477249860764,
      "learning_rate": 1.3960983318115928e-05,
      "loss": 0.0918,
      "step": 41020
    },
    {
      "epoch": 1.5372222846652428,
      "grad_norm": 0.09206701815128326,
      "learning_rate": 1.393952002462804e-05,
      "loss": 0.0877,
      "step": 41030
    },
    {
      "epoch": 1.5375969427897043,
      "grad_norm": 0.09812416881322861,
      "learning_rate": 1.3918070569755232e-05,
      "loss": 0.0897,
      "step": 41040
    },
    {
      "epoch": 1.5379716009141657,
      "grad_norm": 0.1955193132162094,
      "learning_rate": 1.3896634961728977e-05,
      "loss": 0.089,
      "step": 41050
    },
    {
      "epoch": 1.5383462590386272,
      "grad_norm": 0.0743347704410553,
      "learning_rate": 1.3875213208775401e-05,
      "loss": 0.088,
      "step": 41060
    },
    {
      "epoch": 1.5387209171630887,
      "grad_norm": 0.07266002893447876,
      "learning_rate": 1.3853805319115365e-05,
      "loss": 0.088,
      "step": 41070
    },
    {
      "epoch": 1.5390955752875501,
      "grad_norm": 0.08183998614549637,
      "learning_rate": 1.383241130096436e-05,
      "loss": 0.0917,
      "step": 41080
    },
    {
      "epoch": 1.5394702334120116,
      "grad_norm": 0.09816820174455643,
      "learning_rate": 1.3811031162532628e-05,
      "loss": 0.0876,
      "step": 41090
    },
    {
      "epoch": 1.539844891536473,
      "grad_norm": 0.09751109033823013,
      "learning_rate": 1.3789664912025002e-05,
      "loss": 0.0877,
      "step": 41100
    },
    {
      "epoch": 1.5402195496609346,
      "grad_norm": 0.09392248839139938,
      "learning_rate": 1.3768312557641033e-05,
      "loss": 0.0859,
      "step": 41110
    },
    {
      "epoch": 1.5405942077853958,
      "grad_norm": 0.07514937222003937,
      "learning_rate": 1.3746974107574922e-05,
      "loss": 0.0912,
      "step": 41120
    },
    {
      "epoch": 1.5409688659098573,
      "grad_norm": 0.09356444329023361,
      "learning_rate": 1.3725649570015542e-05,
      "loss": 0.0938,
      "step": 41130
    },
    {
      "epoch": 1.5413435240343187,
      "grad_norm": 0.08292005211114883,
      "learning_rate": 1.3704338953146424e-05,
      "loss": 0.0945,
      "step": 41140
    },
    {
      "epoch": 1.54171818215878,
      "grad_norm": 0.09283996373414993,
      "learning_rate": 1.3683042265145757e-05,
      "loss": 0.0869,
      "step": 41150
    },
    {
      "epoch": 1.5420928402832415,
      "grad_norm": 0.07427652180194855,
      "learning_rate": 1.3661759514186407e-05,
      "loss": 0.0919,
      "step": 41160
    },
    {
      "epoch": 1.542467498407703,
      "grad_norm": 0.09489966183900833,
      "learning_rate": 1.3640490708435811e-05,
      "loss": 0.0903,
      "step": 41170
    },
    {
      "epoch": 1.5428421565321644,
      "grad_norm": 0.08816933631896973,
      "learning_rate": 1.3619235856056168e-05,
      "loss": 0.0902,
      "step": 41180
    },
    {
      "epoch": 1.5432168146566259,
      "grad_norm": 0.08705469965934753,
      "learning_rate": 1.3597994965204263e-05,
      "loss": 0.0928,
      "step": 41190
    },
    {
      "epoch": 1.5435914727810873,
      "grad_norm": 0.07554151117801666,
      "learning_rate": 1.3576768044031496e-05,
      "loss": 0.088,
      "step": 41200
    },
    {
      "epoch": 1.5439661309055488,
      "grad_norm": 0.08048667758703232,
      "learning_rate": 1.3555555100683953e-05,
      "loss": 0.0912,
      "step": 41210
    },
    {
      "epoch": 1.5443407890300103,
      "grad_norm": 0.10862204432487488,
      "learning_rate": 1.353435614330234e-05,
      "loss": 0.0906,
      "step": 41220
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 0.10721125453710556,
      "learning_rate": 1.3513171180022e-05,
      "loss": 0.0908,
      "step": 41230
    },
    {
      "epoch": 1.545090105278933,
      "grad_norm": 0.14101392030715942,
      "learning_rate": 1.349200021897289e-05,
      "loss": 0.0905,
      "step": 41240
    },
    {
      "epoch": 1.5454647634033944,
      "grad_norm": 0.08277207612991333,
      "learning_rate": 1.3470843268279615e-05,
      "loss": 0.0923,
      "step": 41250
    },
    {
      "epoch": 1.5458394215278557,
      "grad_norm": 0.11052071303129196,
      "learning_rate": 1.3449700336061383e-05,
      "loss": 0.0891,
      "step": 41260
    },
    {
      "epoch": 1.5462140796523172,
      "grad_norm": 0.09177961200475693,
      "learning_rate": 1.3428571430432046e-05,
      "loss": 0.0891,
      "step": 41270
    },
    {
      "epoch": 1.5465887377767786,
      "grad_norm": 0.07646944373846054,
      "learning_rate": 1.3407456559500047e-05,
      "loss": 0.0908,
      "step": 41280
    },
    {
      "epoch": 1.54696339590124,
      "grad_norm": 0.09407643228769302,
      "learning_rate": 1.3386355731368477e-05,
      "loss": 0.0911,
      "step": 41290
    },
    {
      "epoch": 1.5473380540257016,
      "grad_norm": 0.12486577779054642,
      "learning_rate": 1.3365268954134984e-05,
      "loss": 0.0905,
      "step": 41300
    },
    {
      "epoch": 1.547712712150163,
      "grad_norm": 0.08974636346101761,
      "learning_rate": 1.3344196235891853e-05,
      "loss": 0.0935,
      "step": 41310
    },
    {
      "epoch": 1.5480873702746245,
      "grad_norm": 0.06542903184890747,
      "learning_rate": 1.3323137584726014e-05,
      "loss": 0.0854,
      "step": 41320
    },
    {
      "epoch": 1.548462028399086,
      "grad_norm": 0.07829921692609787,
      "learning_rate": 1.3302093008718957e-05,
      "loss": 0.0888,
      "step": 41330
    },
    {
      "epoch": 1.5488366865235472,
      "grad_norm": 0.09022033214569092,
      "learning_rate": 1.3281062515946747e-05,
      "loss": 0.0917,
      "step": 41340
    },
    {
      "epoch": 1.5492113446480087,
      "grad_norm": 0.07901976257562637,
      "learning_rate": 1.3260046114480068e-05,
      "loss": 0.0919,
      "step": 41350
    },
    {
      "epoch": 1.5495860027724702,
      "grad_norm": 0.11563621461391449,
      "learning_rate": 1.323904381238425e-05,
      "loss": 0.093,
      "step": 41360
    },
    {
      "epoch": 1.5499606608969314,
      "grad_norm": 0.08722379803657532,
      "learning_rate": 1.3218055617719116e-05,
      "loss": 0.0885,
      "step": 41370
    },
    {
      "epoch": 1.5503353190213929,
      "grad_norm": 0.10117173194885254,
      "learning_rate": 1.319708153853914e-05,
      "loss": 0.0896,
      "step": 41380
    },
    {
      "epoch": 1.5507099771458543,
      "grad_norm": 0.08534803241491318,
      "learning_rate": 1.3176121582893353e-05,
      "loss": 0.0922,
      "step": 41390
    },
    {
      "epoch": 1.5510846352703158,
      "grad_norm": 0.21341805160045624,
      "learning_rate": 1.3155175758825383e-05,
      "loss": 0.0896,
      "step": 41400
    },
    {
      "epoch": 1.5514592933947773,
      "grad_norm": 0.06503210961818695,
      "learning_rate": 1.313424407437342e-05,
      "loss": 0.0892,
      "step": 41410
    },
    {
      "epoch": 1.5518339515192388,
      "grad_norm": 0.07773618400096893,
      "learning_rate": 1.3113326537570236e-05,
      "loss": 0.089,
      "step": 41420
    },
    {
      "epoch": 1.5522086096437002,
      "grad_norm": 0.08511598408222198,
      "learning_rate": 1.3092423156443184e-05,
      "loss": 0.0897,
      "step": 41430
    },
    {
      "epoch": 1.5525832677681617,
      "grad_norm": 0.08056078851222992,
      "learning_rate": 1.3071533939014146e-05,
      "loss": 0.0905,
      "step": 41440
    },
    {
      "epoch": 1.552957925892623,
      "grad_norm": 0.07834175229072571,
      "learning_rate": 1.3050658893299588e-05,
      "loss": 0.0916,
      "step": 41450
    },
    {
      "epoch": 1.5533325840170844,
      "grad_norm": 0.07259795069694519,
      "learning_rate": 1.3029798027310597e-05,
      "loss": 0.0878,
      "step": 41460
    },
    {
      "epoch": 1.5537072421415459,
      "grad_norm": 0.06845732778310776,
      "learning_rate": 1.300895134905271e-05,
      "loss": 0.0915,
      "step": 41470
    },
    {
      "epoch": 1.5540819002660071,
      "grad_norm": 0.09257365763187408,
      "learning_rate": 1.29881188665261e-05,
      "loss": 0.088,
      "step": 41480
    },
    {
      "epoch": 1.5544565583904686,
      "grad_norm": 0.07827170193195343,
      "learning_rate": 1.2967300587725456e-05,
      "loss": 0.0897,
      "step": 41490
    },
    {
      "epoch": 1.55483121651493,
      "grad_norm": 0.0796983540058136,
      "learning_rate": 1.2946496520640039e-05,
      "loss": 0.0885,
      "step": 41500
    },
    {
      "epoch": 1.5552058746393915,
      "grad_norm": 0.08115816116333008,
      "learning_rate": 1.2925706673253635e-05,
      "loss": 0.0943,
      "step": 41510
    },
    {
      "epoch": 1.555580532763853,
      "grad_norm": 0.09534361213445663,
      "learning_rate": 1.2904931053544578e-05,
      "loss": 0.089,
      "step": 41520
    },
    {
      "epoch": 1.5559551908883145,
      "grad_norm": 0.1090099886059761,
      "learning_rate": 1.2884169669485774e-05,
      "loss": 0.0914,
      "step": 41530
    },
    {
      "epoch": 1.556329849012776,
      "grad_norm": 0.0939192920923233,
      "learning_rate": 1.2863422529044583e-05,
      "loss": 0.0892,
      "step": 41540
    },
    {
      "epoch": 1.5567045071372374,
      "grad_norm": 0.09739382565021515,
      "learning_rate": 1.2842689640183004e-05,
      "loss": 0.0883,
      "step": 41550
    },
    {
      "epoch": 1.5570791652616987,
      "grad_norm": 0.07470755279064178,
      "learning_rate": 1.2821971010857498e-05,
      "loss": 0.0895,
      "step": 41560
    },
    {
      "epoch": 1.5574538233861601,
      "grad_norm": 0.0940723866224289,
      "learning_rate": 1.2801266649019083e-05,
      "loss": 0.0912,
      "step": 41570
    },
    {
      "epoch": 1.5578284815106216,
      "grad_norm": 0.09412296861410141,
      "learning_rate": 1.2780576562613272e-05,
      "loss": 0.0926,
      "step": 41580
    },
    {
      "epoch": 1.5582031396350828,
      "grad_norm": 0.07211196422576904,
      "learning_rate": 1.2759900759580112e-05,
      "loss": 0.0892,
      "step": 41590
    },
    {
      "epoch": 1.5585777977595443,
      "grad_norm": 0.08435660600662231,
      "learning_rate": 1.2739239247854224e-05,
      "loss": 0.0889,
      "step": 41600
    },
    {
      "epoch": 1.5589524558840058,
      "grad_norm": 0.11613783985376358,
      "learning_rate": 1.271859203536464e-05,
      "loss": 0.0886,
      "step": 41610
    },
    {
      "epoch": 1.5593271140084672,
      "grad_norm": 0.08489833772182465,
      "learning_rate": 1.2697959130034986e-05,
      "loss": 0.0892,
      "step": 41620
    },
    {
      "epoch": 1.5597017721329287,
      "grad_norm": 0.0903540551662445,
      "learning_rate": 1.2677340539783366e-05,
      "loss": 0.0907,
      "step": 41630
    },
    {
      "epoch": 1.5600764302573902,
      "grad_norm": 0.08710683882236481,
      "learning_rate": 1.2656736272522402e-05,
      "loss": 0.0876,
      "step": 41640
    },
    {
      "epoch": 1.5604510883818516,
      "grad_norm": 0.06253251433372498,
      "learning_rate": 1.263614633615921e-05,
      "loss": 0.0861,
      "step": 41650
    },
    {
      "epoch": 1.5608257465063131,
      "grad_norm": 0.09915074706077576,
      "learning_rate": 1.2615570738595406e-05,
      "loss": 0.0896,
      "step": 41660
    },
    {
      "epoch": 1.5612004046307744,
      "grad_norm": 0.09988227486610413,
      "learning_rate": 1.2595009487727133e-05,
      "loss": 0.0941,
      "step": 41670
    },
    {
      "epoch": 1.5615750627552358,
      "grad_norm": 0.08841563016176224,
      "learning_rate": 1.257446259144494e-05,
      "loss": 0.0899,
      "step": 41680
    },
    {
      "epoch": 1.5619497208796973,
      "grad_norm": 0.11125785112380981,
      "learning_rate": 1.2553930057633994e-05,
      "loss": 0.0924,
      "step": 41690
    },
    {
      "epoch": 1.5623243790041585,
      "grad_norm": 0.07273828983306885,
      "learning_rate": 1.2533411894173874e-05,
      "loss": 0.0886,
      "step": 41700
    },
    {
      "epoch": 1.56269903712862,
      "grad_norm": 0.08080311119556427,
      "learning_rate": 1.2512908108938625e-05,
      "loss": 0.089,
      "step": 41710
    },
    {
      "epoch": 1.5630736952530815,
      "grad_norm": 0.10833536833524704,
      "learning_rate": 1.2492418709796832e-05,
      "loss": 0.087,
      "step": 41720
    },
    {
      "epoch": 1.563448353377543,
      "grad_norm": 0.11200827360153198,
      "learning_rate": 1.2471943704611516e-05,
      "loss": 0.0949,
      "step": 41730
    },
    {
      "epoch": 1.5638230115020044,
      "grad_norm": 0.07189105451107025,
      "learning_rate": 1.2451483101240208e-05,
      "loss": 0.0876,
      "step": 41740
    },
    {
      "epoch": 1.564197669626466,
      "grad_norm": 0.09254709631204605,
      "learning_rate": 1.2431036907534881e-05,
      "loss": 0.0892,
      "step": 41750
    },
    {
      "epoch": 1.5645723277509274,
      "grad_norm": 0.10490267723798752,
      "learning_rate": 1.2410605131342001e-05,
      "loss": 0.0917,
      "step": 41760
    },
    {
      "epoch": 1.5649469858753888,
      "grad_norm": 0.15988963842391968,
      "learning_rate": 1.2390187780502488e-05,
      "loss": 0.0934,
      "step": 41770
    },
    {
      "epoch": 1.5653216439998503,
      "grad_norm": 0.09960032999515533,
      "learning_rate": 1.236978486285173e-05,
      "loss": 0.0871,
      "step": 41780
    },
    {
      "epoch": 1.5656963021243115,
      "grad_norm": 0.08528060466051102,
      "learning_rate": 1.2349396386219575e-05,
      "loss": 0.0896,
      "step": 41790
    },
    {
      "epoch": 1.566070960248773,
      "grad_norm": 0.08546972274780273,
      "learning_rate": 1.2329022358430337e-05,
      "loss": 0.0906,
      "step": 41800
    },
    {
      "epoch": 1.5664456183732343,
      "grad_norm": 0.08448272943496704,
      "learning_rate": 1.2308662787302788e-05,
      "loss": 0.0887,
      "step": 41810
    },
    {
      "epoch": 1.5668202764976957,
      "grad_norm": 0.09777967631816864,
      "learning_rate": 1.2288317680650096e-05,
      "loss": 0.089,
      "step": 41820
    },
    {
      "epoch": 1.5671949346221572,
      "grad_norm": 0.07177960872650146,
      "learning_rate": 1.2267987046279972e-05,
      "loss": 0.0893,
      "step": 41830
    },
    {
      "epoch": 1.5675695927466187,
      "grad_norm": 0.08401990681886673,
      "learning_rate": 1.2247670891994528e-05,
      "loss": 0.087,
      "step": 41840
    },
    {
      "epoch": 1.5679442508710801,
      "grad_norm": 0.08008918166160583,
      "learning_rate": 1.2227369225590284e-05,
      "loss": 0.0909,
      "step": 41850
    },
    {
      "epoch": 1.5683189089955416,
      "grad_norm": 0.12049494683742523,
      "learning_rate": 1.220708205485825e-05,
      "loss": 0.0891,
      "step": 41860
    },
    {
      "epoch": 1.568693567120003,
      "grad_norm": 0.0762873962521553,
      "learning_rate": 1.2186809387583853e-05,
      "loss": 0.087,
      "step": 41870
    },
    {
      "epoch": 1.5690682252444645,
      "grad_norm": 0.07378288358449936,
      "learning_rate": 1.2166551231546958e-05,
      "loss": 0.0901,
      "step": 41880
    },
    {
      "epoch": 1.569442883368926,
      "grad_norm": 0.08502879738807678,
      "learning_rate": 1.2146307594521867e-05,
      "loss": 0.0906,
      "step": 41890
    },
    {
      "epoch": 1.5698175414933873,
      "grad_norm": 0.09907007217407227,
      "learning_rate": 1.2126078484277292e-05,
      "loss": 0.0901,
      "step": 41900
    },
    {
      "epoch": 1.5701921996178487,
      "grad_norm": 0.0738934800028801,
      "learning_rate": 1.2105863908576392e-05,
      "loss": 0.0897,
      "step": 41910
    },
    {
      "epoch": 1.5705668577423102,
      "grad_norm": 0.07616784423589706,
      "learning_rate": 1.2085663875176734e-05,
      "loss": 0.0881,
      "step": 41920
    },
    {
      "epoch": 1.5709415158667714,
      "grad_norm": 0.10792863368988037,
      "learning_rate": 1.2065478391830304e-05,
      "loss": 0.0887,
      "step": 41930
    },
    {
      "epoch": 1.571316173991233,
      "grad_norm": 0.09155973047018051,
      "learning_rate": 1.2045307466283528e-05,
      "loss": 0.0912,
      "step": 41940
    },
    {
      "epoch": 1.5716908321156944,
      "grad_norm": 0.08907346427440643,
      "learning_rate": 1.2025151106277199e-05,
      "loss": 0.0866,
      "step": 41950
    },
    {
      "epoch": 1.5720654902401558,
      "grad_norm": 0.08384589850902557,
      "learning_rate": 1.2005009319546544e-05,
      "loss": 0.0895,
      "step": 41960
    },
    {
      "epoch": 1.5724401483646173,
      "grad_norm": 0.07027386128902435,
      "learning_rate": 1.1984882113821238e-05,
      "loss": 0.0925,
      "step": 41970
    },
    {
      "epoch": 1.5728148064890788,
      "grad_norm": 0.07953903079032898,
      "learning_rate": 1.196476949682529e-05,
      "loss": 0.0924,
      "step": 41980
    },
    {
      "epoch": 1.5731894646135403,
      "grad_norm": 0.07759702950716019,
      "learning_rate": 1.1944671476277153e-05,
      "loss": 0.0911,
      "step": 41990
    },
    {
      "epoch": 1.5735641227380017,
      "grad_norm": 0.11579301208257675,
      "learning_rate": 1.1924588059889652e-05,
      "loss": 0.0925,
      "step": 42000
    },
    {
      "epoch": 1.573938780862463,
      "grad_norm": 0.08396016061306,
      "learning_rate": 1.1904519255370067e-05,
      "loss": 0.089,
      "step": 42010
    },
    {
      "epoch": 1.5743134389869244,
      "grad_norm": 0.07811175286769867,
      "learning_rate": 1.1884465070419987e-05,
      "loss": 0.0868,
      "step": 42020
    },
    {
      "epoch": 1.574688097111386,
      "grad_norm": 0.08861840516328812,
      "learning_rate": 1.1864425512735444e-05,
      "loss": 0.0892,
      "step": 42030
    },
    {
      "epoch": 1.5750627552358472,
      "grad_norm": 0.08594051748514175,
      "learning_rate": 1.1844400590006849e-05,
      "loss": 0.094,
      "step": 42040
    },
    {
      "epoch": 1.5754374133603086,
      "grad_norm": 0.09804422408342361,
      "learning_rate": 1.1824390309918982e-05,
      "loss": 0.0907,
      "step": 42050
    },
    {
      "epoch": 1.57581207148477,
      "grad_norm": 0.06793516874313354,
      "learning_rate": 1.1804394680151021e-05,
      "loss": 0.0891,
      "step": 42060
    },
    {
      "epoch": 1.5761867296092316,
      "grad_norm": 0.15079812705516815,
      "learning_rate": 1.1784413708376512e-05,
      "loss": 0.0909,
      "step": 42070
    },
    {
      "epoch": 1.576561387733693,
      "grad_norm": 0.0681595504283905,
      "learning_rate": 1.176444740226339e-05,
      "loss": 0.0886,
      "step": 42080
    },
    {
      "epoch": 1.5769360458581545,
      "grad_norm": 0.08335226029157639,
      "learning_rate": 1.1744495769473924e-05,
      "loss": 0.0875,
      "step": 42090
    },
    {
      "epoch": 1.577310703982616,
      "grad_norm": 0.12274006009101868,
      "learning_rate": 1.1724558817664776e-05,
      "loss": 0.0905,
      "step": 42100
    },
    {
      "epoch": 1.5776853621070774,
      "grad_norm": 0.0951157808303833,
      "learning_rate": 1.170463655448702e-05,
      "loss": 0.0872,
      "step": 42110
    },
    {
      "epoch": 1.5780600202315387,
      "grad_norm": 0.07644515484571457,
      "learning_rate": 1.1684728987586001e-05,
      "loss": 0.0867,
      "step": 42120
    },
    {
      "epoch": 1.5784346783560002,
      "grad_norm": 0.1023029237985611,
      "learning_rate": 1.1664836124601492e-05,
      "loss": 0.0913,
      "step": 42130
    },
    {
      "epoch": 1.5788093364804616,
      "grad_norm": 0.08369139581918716,
      "learning_rate": 1.1644957973167603e-05,
      "loss": 0.0892,
      "step": 42140
    },
    {
      "epoch": 1.5791839946049229,
      "grad_norm": 0.06856879591941833,
      "learning_rate": 1.1625094540912796e-05,
      "loss": 0.0881,
      "step": 42150
    },
    {
      "epoch": 1.5795586527293843,
      "grad_norm": 0.08322227001190186,
      "learning_rate": 1.1605245835459882e-05,
      "loss": 0.0918,
      "step": 42160
    },
    {
      "epoch": 1.5799333108538458,
      "grad_norm": 0.07324967533349991,
      "learning_rate": 1.1585411864426033e-05,
      "loss": 0.0893,
      "step": 42170
    },
    {
      "epoch": 1.5803079689783073,
      "grad_norm": 0.07348291575908661,
      "learning_rate": 1.1565592635422773e-05,
      "loss": 0.0916,
      "step": 42180
    },
    {
      "epoch": 1.5806826271027687,
      "grad_norm": 0.07727884501218796,
      "learning_rate": 1.1545788156055903e-05,
      "loss": 0.0883,
      "step": 42190
    },
    {
      "epoch": 1.5810572852272302,
      "grad_norm": 0.12092284113168716,
      "learning_rate": 1.1525998433925667e-05,
      "loss": 0.092,
      "step": 42200
    },
    {
      "epoch": 1.5814319433516917,
      "grad_norm": 0.08756863325834274,
      "learning_rate": 1.1506223476626588e-05,
      "loss": 0.0914,
      "step": 42210
    },
    {
      "epoch": 1.5818066014761532,
      "grad_norm": 0.08535409718751907,
      "learning_rate": 1.1486463291747501e-05,
      "loss": 0.0904,
      "step": 42220
    },
    {
      "epoch": 1.5821812596006144,
      "grad_norm": 0.07194846868515015,
      "learning_rate": 1.146671788687162e-05,
      "loss": 0.0898,
      "step": 42230
    },
    {
      "epoch": 1.5825559177250759,
      "grad_norm": 0.06571821123361588,
      "learning_rate": 1.1446987269576442e-05,
      "loss": 0.0877,
      "step": 42240
    },
    {
      "epoch": 1.5829305758495373,
      "grad_norm": 0.08363016694784164,
      "learning_rate": 1.1427271447433857e-05,
      "loss": 0.0838,
      "step": 42250
    },
    {
      "epoch": 1.5833052339739986,
      "grad_norm": 0.09375505894422531,
      "learning_rate": 1.140757042801e-05,
      "loss": 0.0906,
      "step": 42260
    },
    {
      "epoch": 1.58367989209846,
      "grad_norm": 0.10105344653129578,
      "learning_rate": 1.1387884218865363e-05,
      "loss": 0.0928,
      "step": 42270
    },
    {
      "epoch": 1.5840545502229215,
      "grad_norm": 0.07820441573858261,
      "learning_rate": 1.1368212827554759e-05,
      "loss": 0.0873,
      "step": 42280
    },
    {
      "epoch": 1.584429208347383,
      "grad_norm": 0.11453598737716675,
      "learning_rate": 1.1348556261627296e-05,
      "loss": 0.0911,
      "step": 42290
    },
    {
      "epoch": 1.5848038664718445,
      "grad_norm": 0.0807190015912056,
      "learning_rate": 1.1328914528626416e-05,
      "loss": 0.088,
      "step": 42300
    },
    {
      "epoch": 1.585178524596306,
      "grad_norm": 0.14796291291713715,
      "learning_rate": 1.1309287636089844e-05,
      "loss": 0.0912,
      "step": 42310
    },
    {
      "epoch": 1.5855531827207674,
      "grad_norm": 0.07707329094409943,
      "learning_rate": 1.1289675591549636e-05,
      "loss": 0.0917,
      "step": 42320
    },
    {
      "epoch": 1.5859278408452289,
      "grad_norm": 0.09142806380987167,
      "learning_rate": 1.1270078402532097e-05,
      "loss": 0.0903,
      "step": 42330
    },
    {
      "epoch": 1.5863024989696901,
      "grad_norm": 0.0855415090918541,
      "learning_rate": 1.125049607655791e-05,
      "loss": 0.088,
      "step": 42340
    },
    {
      "epoch": 1.5866771570941516,
      "grad_norm": 0.09663551300764084,
      "learning_rate": 1.1230928621142013e-05,
      "loss": 0.0887,
      "step": 42350
    },
    {
      "epoch": 1.587051815218613,
      "grad_norm": 0.11037066578865051,
      "learning_rate": 1.1211376043793602e-05,
      "loss": 0.0931,
      "step": 42360
    },
    {
      "epoch": 1.5874264733430743,
      "grad_norm": 0.07588891685009003,
      "learning_rate": 1.1191838352016226e-05,
      "loss": 0.0886,
      "step": 42370
    },
    {
      "epoch": 1.5878011314675358,
      "grad_norm": 0.07200472801923752,
      "learning_rate": 1.1172315553307684e-05,
      "loss": 0.089,
      "step": 42380
    },
    {
      "epoch": 1.5881757895919972,
      "grad_norm": 0.09394826740026474,
      "learning_rate": 1.115280765516007e-05,
      "loss": 0.0922,
      "step": 42390
    },
    {
      "epoch": 1.5885504477164587,
      "grad_norm": 0.08902347832918167,
      "learning_rate": 1.1133314665059769e-05,
      "loss": 0.0865,
      "step": 42400
    },
    {
      "epoch": 1.5889251058409202,
      "grad_norm": 0.08243303745985031,
      "learning_rate": 1.111383659048742e-05,
      "loss": 0.0879,
      "step": 42410
    },
    {
      "epoch": 1.5892997639653816,
      "grad_norm": 0.08752914518117905,
      "learning_rate": 1.1094373438917965e-05,
      "loss": 0.0875,
      "step": 42420
    },
    {
      "epoch": 1.589674422089843,
      "grad_norm": 0.07473006099462509,
      "learning_rate": 1.1074925217820609e-05,
      "loss": 0.0911,
      "step": 42430
    },
    {
      "epoch": 1.5900490802143046,
      "grad_norm": 0.10490384697914124,
      "learning_rate": 1.1055491934658819e-05,
      "loss": 0.0927,
      "step": 42440
    },
    {
      "epoch": 1.5904237383387658,
      "grad_norm": 0.09922125190496445,
      "learning_rate": 1.103607359689035e-05,
      "loss": 0.0874,
      "step": 42450
    },
    {
      "epoch": 1.5907983964632273,
      "grad_norm": 0.07399806380271912,
      "learning_rate": 1.1016670211967183e-05,
      "loss": 0.0879,
      "step": 42460
    },
    {
      "epoch": 1.5911730545876888,
      "grad_norm": 0.09709414094686508,
      "learning_rate": 1.0997281787335583e-05,
      "loss": 0.0895,
      "step": 42470
    },
    {
      "epoch": 1.59154771271215,
      "grad_norm": 0.1055009514093399,
      "learning_rate": 1.097790833043611e-05,
      "loss": 0.092,
      "step": 42480
    },
    {
      "epoch": 1.5919223708366115,
      "grad_norm": 0.09750853478908539,
      "learning_rate": 1.0958549848703536e-05,
      "loss": 0.0912,
      "step": 42490
    },
    {
      "epoch": 1.592297028961073,
      "grad_norm": 0.11620694398880005,
      "learning_rate": 1.0939206349566877e-05,
      "loss": 0.0877,
      "step": 42500
    },
    {
      "epoch": 1.5926716870855344,
      "grad_norm": 0.09155216813087463,
      "learning_rate": 1.0919877840449427e-05,
      "loss": 0.0879,
      "step": 42510
    },
    {
      "epoch": 1.5930463452099959,
      "grad_norm": 0.08718432486057281,
      "learning_rate": 1.0900564328768725e-05,
      "loss": 0.0916,
      "step": 42520
    },
    {
      "epoch": 1.5934210033344574,
      "grad_norm": 0.07923999428749084,
      "learning_rate": 1.0881265821936543e-05,
      "loss": 0.0876,
      "step": 42530
    },
    {
      "epoch": 1.5937956614589188,
      "grad_norm": 0.07323035597801208,
      "learning_rate": 1.08619823273589e-05,
      "loss": 0.0897,
      "step": 42540
    },
    {
      "epoch": 1.5941703195833803,
      "grad_norm": 0.08450091630220413,
      "learning_rate": 1.084271385243606e-05,
      "loss": 0.0916,
      "step": 42550
    },
    {
      "epoch": 1.5945449777078418,
      "grad_norm": 0.10317816585302353,
      "learning_rate": 1.0823460404562508e-05,
      "loss": 0.0916,
      "step": 42560
    },
    {
      "epoch": 1.594919635832303,
      "grad_norm": 0.08864349126815796,
      "learning_rate": 1.0804221991126983e-05,
      "loss": 0.0902,
      "step": 42570
    },
    {
      "epoch": 1.5952942939567645,
      "grad_norm": 0.0781574696302414,
      "learning_rate": 1.0784998619512432e-05,
      "loss": 0.0878,
      "step": 42580
    },
    {
      "epoch": 1.5956689520812257,
      "grad_norm": 0.08531879633665085,
      "learning_rate": 1.0765790297096063e-05,
      "loss": 0.0875,
      "step": 42590
    },
    {
      "epoch": 1.5960436102056872,
      "grad_norm": 0.09813306480646133,
      "learning_rate": 1.0746597031249244e-05,
      "loss": 0.0916,
      "step": 42600
    },
    {
      "epoch": 1.5964182683301487,
      "grad_norm": 0.08189399540424347,
      "learning_rate": 1.0727418829337627e-05,
      "loss": 0.0882,
      "step": 42610
    },
    {
      "epoch": 1.5967929264546101,
      "grad_norm": 0.16970938444137573,
      "learning_rate": 1.0708255698721087e-05,
      "loss": 0.0862,
      "step": 42620
    },
    {
      "epoch": 1.5971675845790716,
      "grad_norm": 0.09176380932331085,
      "learning_rate": 1.068910764675366e-05,
      "loss": 0.0902,
      "step": 42630
    },
    {
      "epoch": 1.597542242703533,
      "grad_norm": 0.11759638786315918,
      "learning_rate": 1.0669974680783634e-05,
      "loss": 0.0895,
      "step": 42640
    },
    {
      "epoch": 1.5979169008279945,
      "grad_norm": 0.0757489874958992,
      "learning_rate": 1.0650856808153503e-05,
      "loss": 0.0873,
      "step": 42650
    },
    {
      "epoch": 1.598291558952456,
      "grad_norm": 0.09371630847454071,
      "learning_rate": 1.0631754036199965e-05,
      "loss": 0.0917,
      "step": 42660
    },
    {
      "epoch": 1.5986662170769175,
      "grad_norm": 0.09292569756507874,
      "learning_rate": 1.0612666372253927e-05,
      "loss": 0.0941,
      "step": 42670
    },
    {
      "epoch": 1.5990408752013787,
      "grad_norm": 0.08446525037288666,
      "learning_rate": 1.0593593823640491e-05,
      "loss": 0.0897,
      "step": 42680
    },
    {
      "epoch": 1.5994155333258402,
      "grad_norm": 0.08601167052984238,
      "learning_rate": 1.0574536397678964e-05,
      "loss": 0.0911,
      "step": 42690
    },
    {
      "epoch": 1.5997901914503014,
      "grad_norm": 0.09763367474079132,
      "learning_rate": 1.0555494101682844e-05,
      "loss": 0.0905,
      "step": 42700
    },
    {
      "epoch": 1.600164849574763,
      "grad_norm": 0.08710478246212006,
      "learning_rate": 1.0536466942959834e-05,
      "loss": 0.0875,
      "step": 42710
    },
    {
      "epoch": 1.6005395076992244,
      "grad_norm": 0.07753773778676987,
      "learning_rate": 1.051745492881181e-05,
      "loss": 0.0919,
      "step": 42720
    },
    {
      "epoch": 1.6009141658236858,
      "grad_norm": 0.10125850886106491,
      "learning_rate": 1.0498458066534866e-05,
      "loss": 0.0916,
      "step": 42730
    },
    {
      "epoch": 1.6012888239481473,
      "grad_norm": 0.10488154739141464,
      "learning_rate": 1.0479476363419238e-05,
      "loss": 0.0898,
      "step": 42740
    },
    {
      "epoch": 1.6016634820726088,
      "grad_norm": 0.0968247503042221,
      "learning_rate": 1.0460509826749365e-05,
      "loss": 0.0907,
      "step": 42750
    },
    {
      "epoch": 1.6020381401970702,
      "grad_norm": 0.09163545817136765,
      "learning_rate": 1.0441558463803907e-05,
      "loss": 0.0888,
      "step": 42760
    },
    {
      "epoch": 1.6024127983215317,
      "grad_norm": 0.08614398539066315,
      "learning_rate": 1.0422622281855627e-05,
      "loss": 0.0865,
      "step": 42770
    },
    {
      "epoch": 1.6027874564459932,
      "grad_norm": 0.07355374842882156,
      "learning_rate": 1.0403701288171503e-05,
      "loss": 0.0889,
      "step": 42780
    },
    {
      "epoch": 1.6031621145704544,
      "grad_norm": 0.09228452295064926,
      "learning_rate": 1.0384795490012689e-05,
      "loss": 0.0882,
      "step": 42790
    },
    {
      "epoch": 1.603536772694916,
      "grad_norm": 0.07026344537734985,
      "learning_rate": 1.0365904894634492e-05,
      "loss": 0.0917,
      "step": 42800
    },
    {
      "epoch": 1.6039114308193774,
      "grad_norm": 0.11665932089090347,
      "learning_rate": 1.0347029509286393e-05,
      "loss": 0.0906,
      "step": 42810
    },
    {
      "epoch": 1.6042860889438386,
      "grad_norm": 0.08908689767122269,
      "learning_rate": 1.0328169341212034e-05,
      "loss": 0.0898,
      "step": 42820
    },
    {
      "epoch": 1.6046607470683,
      "grad_norm": 0.11459752172231674,
      "learning_rate": 1.0309324397649233e-05,
      "loss": 0.0874,
      "step": 42830
    },
    {
      "epoch": 1.6050354051927616,
      "grad_norm": 0.08947622030973434,
      "learning_rate": 1.0290494685829898e-05,
      "loss": 0.0886,
      "step": 42840
    },
    {
      "epoch": 1.605410063317223,
      "grad_norm": 0.07869353890419006,
      "learning_rate": 1.0271680212980194e-05,
      "loss": 0.0902,
      "step": 42850
    },
    {
      "epoch": 1.6057847214416845,
      "grad_norm": 0.10713636875152588,
      "learning_rate": 1.0252880986320385e-05,
      "loss": 0.0908,
      "step": 42860
    },
    {
      "epoch": 1.606159379566146,
      "grad_norm": 0.057073548436164856,
      "learning_rate": 1.0234097013064852e-05,
      "loss": 0.0883,
      "step": 42870
    },
    {
      "epoch": 1.6065340376906074,
      "grad_norm": 0.08151879906654358,
      "learning_rate": 1.0215328300422178e-05,
      "loss": 0.0901,
      "step": 42880
    },
    {
      "epoch": 1.606908695815069,
      "grad_norm": 0.08359787613153458,
      "learning_rate": 1.0196574855595053e-05,
      "loss": 0.0919,
      "step": 42890
    },
    {
      "epoch": 1.6072833539395301,
      "grad_norm": 0.07868080586194992,
      "learning_rate": 1.0177836685780357e-05,
      "loss": 0.0891,
      "step": 42900
    },
    {
      "epoch": 1.6076580120639916,
      "grad_norm": 0.10121972858905792,
      "learning_rate": 1.0159113798169039e-05,
      "loss": 0.0867,
      "step": 42910
    },
    {
      "epoch": 1.608032670188453,
      "grad_norm": 0.09599096328020096,
      "learning_rate": 1.0140406199946229e-05,
      "loss": 0.0934,
      "step": 42920
    },
    {
      "epoch": 1.6084073283129143,
      "grad_norm": 0.07085999846458435,
      "learning_rate": 1.0121713898291179e-05,
      "loss": 0.0917,
      "step": 42930
    },
    {
      "epoch": 1.6087819864373758,
      "grad_norm": 0.08472651988267899,
      "learning_rate": 1.0103036900377266e-05,
      "loss": 0.0925,
      "step": 42940
    },
    {
      "epoch": 1.6091566445618373,
      "grad_norm": 0.09693066030740738,
      "learning_rate": 1.0084375213372005e-05,
      "loss": 0.0896,
      "step": 42950
    },
    {
      "epoch": 1.6095313026862987,
      "grad_norm": 0.08196955919265747,
      "learning_rate": 1.0065728844437028e-05,
      "loss": 0.088,
      "step": 42960
    },
    {
      "epoch": 1.6099059608107602,
      "grad_norm": 0.07001791894435883,
      "learning_rate": 1.00470978007281e-05,
      "loss": 0.0908,
      "step": 42970
    },
    {
      "epoch": 1.6102806189352217,
      "grad_norm": 0.09346616268157959,
      "learning_rate": 1.0028482089395047e-05,
      "loss": 0.0909,
      "step": 42980
    },
    {
      "epoch": 1.6106552770596831,
      "grad_norm": 0.07214033603668213,
      "learning_rate": 1.000988171758191e-05,
      "loss": 0.0898,
      "step": 42990
    },
    {
      "epoch": 1.6110299351841446,
      "grad_norm": 0.07755675166845322,
      "learning_rate": 9.991296692426782e-06,
      "loss": 0.0877,
      "step": 43000
    },
    {
      "epoch": 1.6114045933086059,
      "grad_norm": 0.08778201043605804,
      "learning_rate": 9.972727021061856e-06,
      "loss": 0.0891,
      "step": 43010
    },
    {
      "epoch": 1.6117792514330673,
      "grad_norm": 0.08117083460092545,
      "learning_rate": 9.954172710613457e-06,
      "loss": 0.0912,
      "step": 43020
    },
    {
      "epoch": 1.6121539095575288,
      "grad_norm": 0.07386570423841476,
      "learning_rate": 9.935633768202019e-06,
      "loss": 0.0894,
      "step": 43030
    },
    {
      "epoch": 1.61252856768199,
      "grad_norm": 0.17491194605827332,
      "learning_rate": 9.917110200942064e-06,
      "loss": 0.0932,
      "step": 43040
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 0.07565297931432724,
      "learning_rate": 9.898602015942221e-06,
      "loss": 0.0917,
      "step": 43050
    },
    {
      "epoch": 1.613277883930913,
      "grad_norm": 0.09536140412092209,
      "learning_rate": 9.880109220305218e-06,
      "loss": 0.0894,
      "step": 43060
    },
    {
      "epoch": 1.6136525420553745,
      "grad_norm": 0.08644682914018631,
      "learning_rate": 9.86163182112787e-06,
      "loss": 0.0896,
      "step": 43070
    },
    {
      "epoch": 1.614027200179836,
      "grad_norm": 0.12183154374361038,
      "learning_rate": 9.843169825501086e-06,
      "loss": 0.0938,
      "step": 43080
    },
    {
      "epoch": 1.6144018583042974,
      "grad_norm": 0.13670936226844788,
      "learning_rate": 9.824723240509864e-06,
      "loss": 0.0888,
      "step": 43090
    },
    {
      "epoch": 1.6147765164287589,
      "grad_norm": 0.07853105664253235,
      "learning_rate": 9.80629207323331e-06,
      "loss": 0.0883,
      "step": 43100
    },
    {
      "epoch": 1.6151511745532203,
      "grad_norm": 0.0759756863117218,
      "learning_rate": 9.787876330744556e-06,
      "loss": 0.0876,
      "step": 43110
    },
    {
      "epoch": 1.6155258326776816,
      "grad_norm": 0.07383731007575989,
      "learning_rate": 9.769476020110852e-06,
      "loss": 0.0878,
      "step": 43120
    },
    {
      "epoch": 1.615900490802143,
      "grad_norm": 0.08172353357076645,
      "learning_rate": 9.751091148393555e-06,
      "loss": 0.089,
      "step": 43130
    },
    {
      "epoch": 1.6162751489266045,
      "grad_norm": 0.07632467895746231,
      "learning_rate": 9.732721722648058e-06,
      "loss": 0.089,
      "step": 43140
    },
    {
      "epoch": 1.6166498070510658,
      "grad_norm": 0.0902969017624855,
      "learning_rate": 9.71436774992382e-06,
      "loss": 0.0886,
      "step": 43150
    },
    {
      "epoch": 1.6170244651755272,
      "grad_norm": 0.07016646862030029,
      "learning_rate": 9.696029237264392e-06,
      "loss": 0.0939,
      "step": 43160
    },
    {
      "epoch": 1.6173991232999887,
      "grad_norm": 0.11052768677473068,
      "learning_rate": 9.677706191707382e-06,
      "loss": 0.0898,
      "step": 43170
    },
    {
      "epoch": 1.6177737814244502,
      "grad_norm": 0.09003051370382309,
      "learning_rate": 9.65939862028447e-06,
      "loss": 0.0941,
      "step": 43180
    },
    {
      "epoch": 1.6181484395489116,
      "grad_norm": 0.07955921441316605,
      "learning_rate": 9.641106530021388e-06,
      "loss": 0.0894,
      "step": 43190
    },
    {
      "epoch": 1.618523097673373,
      "grad_norm": 0.070388063788414,
      "learning_rate": 9.622829927937938e-06,
      "loss": 0.091,
      "step": 43200
    },
    {
      "epoch": 1.6188977557978346,
      "grad_norm": 0.06669686734676361,
      "learning_rate": 9.604568821047966e-06,
      "loss": 0.0862,
      "step": 43210
    },
    {
      "epoch": 1.619272413922296,
      "grad_norm": 0.07340410351753235,
      "learning_rate": 9.586323216359377e-06,
      "loss": 0.0898,
      "step": 43220
    },
    {
      "epoch": 1.6196470720467573,
      "grad_norm": 0.08381638675928116,
      "learning_rate": 9.568093120874127e-06,
      "loss": 0.0868,
      "step": 43230
    },
    {
      "epoch": 1.6200217301712188,
      "grad_norm": 0.0719824731349945,
      "learning_rate": 9.549878541588241e-06,
      "loss": 0.0865,
      "step": 43240
    },
    {
      "epoch": 1.6203963882956802,
      "grad_norm": 0.1264246255159378,
      "learning_rate": 9.531679485491735e-06,
      "loss": 0.0914,
      "step": 43250
    },
    {
      "epoch": 1.6207710464201415,
      "grad_norm": 0.08306001871824265,
      "learning_rate": 9.513495959568702e-06,
      "loss": 0.0886,
      "step": 43260
    },
    {
      "epoch": 1.621145704544603,
      "grad_norm": 0.09439071267843246,
      "learning_rate": 9.495327970797318e-06,
      "loss": 0.0876,
      "step": 43270
    },
    {
      "epoch": 1.6215203626690644,
      "grad_norm": 0.07114731520414352,
      "learning_rate": 9.477175526149712e-06,
      "loss": 0.0931,
      "step": 43280
    },
    {
      "epoch": 1.6218950207935259,
      "grad_norm": 0.07124065607786179,
      "learning_rate": 9.459038632592104e-06,
      "loss": 0.0927,
      "step": 43290
    },
    {
      "epoch": 1.6222696789179873,
      "grad_norm": 0.09759224951267242,
      "learning_rate": 9.440917297084722e-06,
      "loss": 0.0849,
      "step": 43300
    },
    {
      "epoch": 1.6226443370424488,
      "grad_norm": 0.10554397851228714,
      "learning_rate": 9.422811526581843e-06,
      "loss": 0.0918,
      "step": 43310
    },
    {
      "epoch": 1.6230189951669103,
      "grad_norm": 0.09741190075874329,
      "learning_rate": 9.404721328031756e-06,
      "loss": 0.091,
      "step": 43320
    },
    {
      "epoch": 1.6233936532913718,
      "grad_norm": 0.07930371165275574,
      "learning_rate": 9.386646708376784e-06,
      "loss": 0.0907,
      "step": 43330
    },
    {
      "epoch": 1.623768311415833,
      "grad_norm": 0.08317481726408005,
      "learning_rate": 9.368587674553264e-06,
      "loss": 0.0891,
      "step": 43340
    },
    {
      "epoch": 1.6241429695402945,
      "grad_norm": 0.10737583041191101,
      "learning_rate": 9.350544233491532e-06,
      "loss": 0.0911,
      "step": 43350
    },
    {
      "epoch": 1.624517627664756,
      "grad_norm": 0.07022842019796371,
      "learning_rate": 9.332516392115991e-06,
      "loss": 0.0867,
      "step": 43360
    },
    {
      "epoch": 1.6248922857892172,
      "grad_norm": 0.08085349947214127,
      "learning_rate": 9.314504157345011e-06,
      "loss": 0.0905,
      "step": 43370
    },
    {
      "epoch": 1.6252669439136787,
      "grad_norm": 0.10975285619497299,
      "learning_rate": 9.296507536091015e-06,
      "loss": 0.0895,
      "step": 43380
    },
    {
      "epoch": 1.6256416020381401,
      "grad_norm": 0.09114275872707367,
      "learning_rate": 9.278526535260374e-06,
      "loss": 0.0841,
      "step": 43390
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 0.08147044479846954,
      "learning_rate": 9.2605611617535e-06,
      "loss": 0.0864,
      "step": 43400
    },
    {
      "epoch": 1.626390918287063,
      "grad_norm": 0.06517021358013153,
      "learning_rate": 9.242611422464854e-06,
      "loss": 0.085,
      "step": 43410
    },
    {
      "epoch": 1.6267655764115245,
      "grad_norm": 0.07616326212882996,
      "learning_rate": 9.224677324282798e-06,
      "loss": 0.0883,
      "step": 43420
    },
    {
      "epoch": 1.627140234535986,
      "grad_norm": 0.07420848309993744,
      "learning_rate": 9.206758874089771e-06,
      "loss": 0.0896,
      "step": 43430
    },
    {
      "epoch": 1.6275148926604475,
      "grad_norm": 0.08817698806524277,
      "learning_rate": 9.18885607876217e-06,
      "loss": 0.0922,
      "step": 43440
    },
    {
      "epoch": 1.627889550784909,
      "grad_norm": 0.08328794687986374,
      "learning_rate": 9.1709689451704e-06,
      "loss": 0.0929,
      "step": 43450
    },
    {
      "epoch": 1.6282642089093702,
      "grad_norm": 0.09943117201328278,
      "learning_rate": 9.153097480178847e-06,
      "loss": 0.0859,
      "step": 43460
    },
    {
      "epoch": 1.6286388670338316,
      "grad_norm": 0.08114880323410034,
      "learning_rate": 9.135241690645885e-06,
      "loss": 0.0898,
      "step": 43470
    },
    {
      "epoch": 1.629013525158293,
      "grad_norm": 0.11551395058631897,
      "learning_rate": 9.117401583423896e-06,
      "loss": 0.0892,
      "step": 43480
    },
    {
      "epoch": 1.6293881832827544,
      "grad_norm": 0.07543474435806274,
      "learning_rate": 9.099577165359168e-06,
      "loss": 0.0924,
      "step": 43490
    },
    {
      "epoch": 1.6297628414072158,
      "grad_norm": 0.07211825996637344,
      "learning_rate": 9.081768443292076e-06,
      "loss": 0.0895,
      "step": 43500
    },
    {
      "epoch": 1.6301374995316773,
      "grad_norm": 0.07258680462837219,
      "learning_rate": 9.06397542405691e-06,
      "loss": 0.0904,
      "step": 43510
    },
    {
      "epoch": 1.6305121576561388,
      "grad_norm": 0.06308132410049438,
      "learning_rate": 9.046198114481918e-06,
      "loss": 0.0865,
      "step": 43520
    },
    {
      "epoch": 1.6308868157806002,
      "grad_norm": 0.0952896922826767,
      "learning_rate": 9.02843652138935e-06,
      "loss": 0.0911,
      "step": 43530
    },
    {
      "epoch": 1.6312614739050617,
      "grad_norm": 0.09201042354106903,
      "learning_rate": 9.010690651595427e-06,
      "loss": 0.0906,
      "step": 43540
    },
    {
      "epoch": 1.6316361320295232,
      "grad_norm": 0.08248823136091232,
      "learning_rate": 8.99296051191032e-06,
      "loss": 0.0889,
      "step": 43550
    },
    {
      "epoch": 1.6320107901539846,
      "grad_norm": 0.08363141119480133,
      "learning_rate": 8.975246109138168e-06,
      "loss": 0.0914,
      "step": 43560
    },
    {
      "epoch": 1.632385448278446,
      "grad_norm": 0.12952658534049988,
      "learning_rate": 8.957547450077074e-06,
      "loss": 0.0918,
      "step": 43570
    },
    {
      "epoch": 1.6327601064029074,
      "grad_norm": 0.09562937170267105,
      "learning_rate": 8.939864541519099e-06,
      "loss": 0.0898,
      "step": 43580
    },
    {
      "epoch": 1.6331347645273686,
      "grad_norm": 0.09085556864738464,
      "learning_rate": 8.922197390250258e-06,
      "loss": 0.0915,
      "step": 43590
    },
    {
      "epoch": 1.63350942265183,
      "grad_norm": 0.07320961356163025,
      "learning_rate": 8.904546003050523e-06,
      "loss": 0.0907,
      "step": 43600
    },
    {
      "epoch": 1.6338840807762915,
      "grad_norm": 0.09620892256498337,
      "learning_rate": 8.886910386693809e-06,
      "loss": 0.0861,
      "step": 43610
    },
    {
      "epoch": 1.634258738900753,
      "grad_norm": 0.08257662504911423,
      "learning_rate": 8.86929054794799e-06,
      "loss": 0.0883,
      "step": 43620
    },
    {
      "epoch": 1.6346333970252145,
      "grad_norm": 0.09712359309196472,
      "learning_rate": 8.851686493574845e-06,
      "loss": 0.0928,
      "step": 43630
    },
    {
      "epoch": 1.635008055149676,
      "grad_norm": 0.07201575487852097,
      "learning_rate": 8.834098230330163e-06,
      "loss": 0.0914,
      "step": 43640
    },
    {
      "epoch": 1.6353827132741374,
      "grad_norm": 0.08315663784742355,
      "learning_rate": 8.81652576496364e-06,
      "loss": 0.0931,
      "step": 43650
    },
    {
      "epoch": 1.635757371398599,
      "grad_norm": 0.0713207945227623,
      "learning_rate": 8.798969104218885e-06,
      "loss": 0.0904,
      "step": 43660
    },
    {
      "epoch": 1.6361320295230604,
      "grad_norm": 0.09527146816253662,
      "learning_rate": 8.781428254833474e-06,
      "loss": 0.0884,
      "step": 43670
    },
    {
      "epoch": 1.6365066876475216,
      "grad_norm": 0.06695415079593658,
      "learning_rate": 8.763903223538899e-06,
      "loss": 0.0907,
      "step": 43680
    },
    {
      "epoch": 1.636881345771983,
      "grad_norm": 0.07016929984092712,
      "learning_rate": 8.746394017060599e-06,
      "loss": 0.0897,
      "step": 43690
    },
    {
      "epoch": 1.6372560038964445,
      "grad_norm": 0.09309996664524078,
      "learning_rate": 8.728900642117927e-06,
      "loss": 0.09,
      "step": 43700
    },
    {
      "epoch": 1.6376306620209058,
      "grad_norm": 0.0867047980427742,
      "learning_rate": 8.711423105424154e-06,
      "loss": 0.0899,
      "step": 43710
    },
    {
      "epoch": 1.6380053201453673,
      "grad_norm": 0.07157247513532639,
      "learning_rate": 8.69396141368649e-06,
      "loss": 0.0902,
      "step": 43720
    },
    {
      "epoch": 1.6383799782698287,
      "grad_norm": 0.07706113159656525,
      "learning_rate": 8.676515573606053e-06,
      "loss": 0.0868,
      "step": 43730
    },
    {
      "epoch": 1.6387546363942902,
      "grad_norm": 0.0744515210390091,
      "learning_rate": 8.65908559187788e-06,
      "loss": 0.0919,
      "step": 43740
    },
    {
      "epoch": 1.6391292945187517,
      "grad_norm": 0.109797902405262,
      "learning_rate": 8.641671475190933e-06,
      "loss": 0.0932,
      "step": 43750
    },
    {
      "epoch": 1.6395039526432131,
      "grad_norm": 0.08403672277927399,
      "learning_rate": 8.624273230228059e-06,
      "loss": 0.0887,
      "step": 43760
    },
    {
      "epoch": 1.6398786107676746,
      "grad_norm": 0.08970393985509872,
      "learning_rate": 8.606890863666018e-06,
      "loss": 0.0893,
      "step": 43770
    },
    {
      "epoch": 1.640253268892136,
      "grad_norm": 0.09796968847513199,
      "learning_rate": 8.589524382175535e-06,
      "loss": 0.0916,
      "step": 43780
    },
    {
      "epoch": 1.6406279270165973,
      "grad_norm": 0.1409115493297577,
      "learning_rate": 8.57217379242115e-06,
      "loss": 0.0889,
      "step": 43790
    },
    {
      "epoch": 1.6410025851410588,
      "grad_norm": 0.0818367674946785,
      "learning_rate": 8.554839101061363e-06,
      "loss": 0.0884,
      "step": 43800
    },
    {
      "epoch": 1.6413772432655203,
      "grad_norm": 0.08892890065908432,
      "learning_rate": 8.53752031474856e-06,
      "loss": 0.0933,
      "step": 43810
    },
    {
      "epoch": 1.6417519013899815,
      "grad_norm": 0.13605332374572754,
      "learning_rate": 8.520217440129014e-06,
      "loss": 0.0889,
      "step": 43820
    },
    {
      "epoch": 1.642126559514443,
      "grad_norm": 0.09815767407417297,
      "learning_rate": 8.502930483842902e-06,
      "loss": 0.0892,
      "step": 43830
    },
    {
      "epoch": 1.6425012176389044,
      "grad_norm": 0.09633184969425201,
      "learning_rate": 8.485659452524281e-06,
      "loss": 0.0897,
      "step": 43840
    },
    {
      "epoch": 1.642875875763366,
      "grad_norm": 0.11085013300180435,
      "learning_rate": 8.468404352801113e-06,
      "loss": 0.0903,
      "step": 43850
    },
    {
      "epoch": 1.6432505338878274,
      "grad_norm": 0.07669329643249512,
      "learning_rate": 8.451165191295234e-06,
      "loss": 0.0876,
      "step": 43860
    },
    {
      "epoch": 1.6436251920122888,
      "grad_norm": 0.09294892102479935,
      "learning_rate": 8.433941974622362e-06,
      "loss": 0.0888,
      "step": 43870
    },
    {
      "epoch": 1.6439998501367503,
      "grad_norm": 0.08733484148979187,
      "learning_rate": 8.416734709392104e-06,
      "loss": 0.0921,
      "step": 43880
    },
    {
      "epoch": 1.6443745082612118,
      "grad_norm": 0.08252398669719696,
      "learning_rate": 8.399543402207955e-06,
      "loss": 0.0899,
      "step": 43890
    },
    {
      "epoch": 1.644749166385673,
      "grad_norm": 0.08621900528669357,
      "learning_rate": 8.382368059667244e-06,
      "loss": 0.0907,
      "step": 43900
    },
    {
      "epoch": 1.6451238245101345,
      "grad_norm": 0.0854976549744606,
      "learning_rate": 8.365208688361209e-06,
      "loss": 0.0915,
      "step": 43910
    },
    {
      "epoch": 1.645498482634596,
      "grad_norm": 0.07545240968465805,
      "learning_rate": 8.34806529487498e-06,
      "loss": 0.0911,
      "step": 43920
    },
    {
      "epoch": 1.6458731407590572,
      "grad_norm": 0.08414491266012192,
      "learning_rate": 8.330937885787493e-06,
      "loss": 0.093,
      "step": 43930
    },
    {
      "epoch": 1.6462477988835187,
      "grad_norm": 0.07878812402486801,
      "learning_rate": 8.313826467671597e-06,
      "loss": 0.0873,
      "step": 43940
    },
    {
      "epoch": 1.6466224570079802,
      "grad_norm": 0.10905293375253677,
      "learning_rate": 8.296731047093998e-06,
      "loss": 0.0917,
      "step": 43950
    },
    {
      "epoch": 1.6469971151324416,
      "grad_norm": 0.08748800307512283,
      "learning_rate": 8.279651630615249e-06,
      "loss": 0.0896,
      "step": 43960
    },
    {
      "epoch": 1.647371773256903,
      "grad_norm": 0.11159230023622513,
      "learning_rate": 8.262588224789764e-06,
      "loss": 0.0903,
      "step": 43970
    },
    {
      "epoch": 1.6477464313813646,
      "grad_norm": 0.07975076884031296,
      "learning_rate": 8.245540836165827e-06,
      "loss": 0.0875,
      "step": 43980
    },
    {
      "epoch": 1.648121089505826,
      "grad_norm": 0.08292040973901749,
      "learning_rate": 8.228509471285573e-06,
      "loss": 0.0916,
      "step": 43990
    },
    {
      "epoch": 1.6484957476302875,
      "grad_norm": 0.08083315938711166,
      "learning_rate": 8.211494136684939e-06,
      "loss": 0.0932,
      "step": 44000
    },
    {
      "epoch": 1.6488704057547487,
      "grad_norm": 0.09405223280191422,
      "learning_rate": 8.194494838893795e-06,
      "loss": 0.0905,
      "step": 44010
    },
    {
      "epoch": 1.6492450638792102,
      "grad_norm": 0.08658961206674576,
      "learning_rate": 8.17751158443581e-06,
      "loss": 0.0911,
      "step": 44020
    },
    {
      "epoch": 1.6496197220036717,
      "grad_norm": 0.10094662010669708,
      "learning_rate": 8.160544379828472e-06,
      "loss": 0.0915,
      "step": 44030
    },
    {
      "epoch": 1.649994380128133,
      "grad_norm": 0.08714853972196579,
      "learning_rate": 8.143593231583157e-06,
      "loss": 0.089,
      "step": 44040
    },
    {
      "epoch": 1.6503690382525944,
      "grad_norm": 0.06574839353561401,
      "learning_rate": 8.126658146205035e-06,
      "loss": 0.0905,
      "step": 44050
    },
    {
      "epoch": 1.6507436963770559,
      "grad_norm": 0.09109190106391907,
      "learning_rate": 8.109739130193184e-06,
      "loss": 0.0896,
      "step": 44060
    },
    {
      "epoch": 1.6511183545015173,
      "grad_norm": 0.09987431019544601,
      "learning_rate": 8.09283619004042e-06,
      "loss": 0.0899,
      "step": 44070
    },
    {
      "epoch": 1.6514930126259788,
      "grad_norm": 0.08196788281202316,
      "learning_rate": 8.075949332233457e-06,
      "loss": 0.0874,
      "step": 44080
    },
    {
      "epoch": 1.6518676707504403,
      "grad_norm": 0.0703628808259964,
      "learning_rate": 8.059078563252809e-06,
      "loss": 0.0891,
      "step": 44090
    },
    {
      "epoch": 1.6522423288749017,
      "grad_norm": 0.08603335916996002,
      "learning_rate": 8.042223889572831e-06,
      "loss": 0.0885,
      "step": 44100
    },
    {
      "epoch": 1.6526169869993632,
      "grad_norm": 0.12798970937728882,
      "learning_rate": 8.025385317661687e-06,
      "loss": 0.0917,
      "step": 44110
    },
    {
      "epoch": 1.6529916451238245,
      "grad_norm": 0.06482637673616409,
      "learning_rate": 8.008562853981378e-06,
      "loss": 0.0867,
      "step": 44120
    },
    {
      "epoch": 1.653366303248286,
      "grad_norm": 0.0919976681470871,
      "learning_rate": 7.991756504987712e-06,
      "loss": 0.0889,
      "step": 44130
    },
    {
      "epoch": 1.6537409613727474,
      "grad_norm": 0.08277365565299988,
      "learning_rate": 7.974966277130291e-06,
      "loss": 0.088,
      "step": 44140
    },
    {
      "epoch": 1.6541156194972086,
      "grad_norm": 0.09857999533414841,
      "learning_rate": 7.958192176852586e-06,
      "loss": 0.0916,
      "step": 44150
    },
    {
      "epoch": 1.6544902776216701,
      "grad_norm": 0.07698529213666916,
      "learning_rate": 7.941434210591842e-06,
      "loss": 0.089,
      "step": 44160
    },
    {
      "epoch": 1.6548649357461316,
      "grad_norm": 0.08703500777482986,
      "learning_rate": 7.924692384779098e-06,
      "loss": 0.0909,
      "step": 44170
    },
    {
      "epoch": 1.655239593870593,
      "grad_norm": 0.06310146301984787,
      "learning_rate": 7.907966705839231e-06,
      "loss": 0.0835,
      "step": 44180
    },
    {
      "epoch": 1.6556142519950545,
      "grad_norm": 0.07226470112800598,
      "learning_rate": 7.891257180190909e-06,
      "loss": 0.0872,
      "step": 44190
    },
    {
      "epoch": 1.655988910119516,
      "grad_norm": 0.0869007259607315,
      "learning_rate": 7.874563814246594e-06,
      "loss": 0.0883,
      "step": 44200
    },
    {
      "epoch": 1.6563635682439775,
      "grad_norm": 0.08930941671133041,
      "learning_rate": 7.857886614412558e-06,
      "loss": 0.0909,
      "step": 44210
    },
    {
      "epoch": 1.656738226368439,
      "grad_norm": 0.07511875033378601,
      "learning_rate": 7.841225587088863e-06,
      "loss": 0.0892,
      "step": 44220
    },
    {
      "epoch": 1.6571128844929002,
      "grad_norm": 0.09069755673408508,
      "learning_rate": 7.82458073866938e-06,
      "loss": 0.0891,
      "step": 44230
    },
    {
      "epoch": 1.6574875426173616,
      "grad_norm": 0.07082796841859818,
      "learning_rate": 7.80795207554172e-06,
      "loss": 0.0867,
      "step": 44240
    },
    {
      "epoch": 1.657862200741823,
      "grad_norm": 0.10058474540710449,
      "learning_rate": 7.791339604087355e-06,
      "loss": 0.0899,
      "step": 44250
    },
    {
      "epoch": 1.6582368588662844,
      "grad_norm": 0.07035228610038757,
      "learning_rate": 7.774743330681494e-06,
      "loss": 0.09,
      "step": 44260
    },
    {
      "epoch": 1.6586115169907458,
      "grad_norm": 0.07940806448459625,
      "learning_rate": 7.758163261693157e-06,
      "loss": 0.0884,
      "step": 44270
    },
    {
      "epoch": 1.6589861751152073,
      "grad_norm": 0.10813751816749573,
      "learning_rate": 7.741599403485094e-06,
      "loss": 0.0914,
      "step": 44280
    },
    {
      "epoch": 1.6593608332396688,
      "grad_norm": 0.0823436751961708,
      "learning_rate": 7.725051762413915e-06,
      "loss": 0.0954,
      "step": 44290
    },
    {
      "epoch": 1.6597354913641302,
      "grad_norm": 0.06956756860017776,
      "learning_rate": 7.708520344829961e-06,
      "loss": 0.0906,
      "step": 44300
    },
    {
      "epoch": 1.6601101494885917,
      "grad_norm": 0.10071036964654922,
      "learning_rate": 7.69200515707732e-06,
      "loss": 0.0881,
      "step": 44310
    },
    {
      "epoch": 1.6604848076130532,
      "grad_norm": 0.083616241812706,
      "learning_rate": 7.675506205493899e-06,
      "loss": 0.0918,
      "step": 44320
    },
    {
      "epoch": 1.6608594657375146,
      "grad_norm": 0.0937652587890625,
      "learning_rate": 7.65902349641135e-06,
      "loss": 0.0915,
      "step": 44330
    },
    {
      "epoch": 1.661234123861976,
      "grad_norm": 0.07947313040494919,
      "learning_rate": 7.642557036155108e-06,
      "loss": 0.087,
      "step": 44340
    },
    {
      "epoch": 1.6616087819864374,
      "grad_norm": 0.0689043328166008,
      "learning_rate": 7.626106831044349e-06,
      "loss": 0.0895,
      "step": 44350
    },
    {
      "epoch": 1.6619834401108988,
      "grad_norm": 0.08940703421831131,
      "learning_rate": 7.609672887392039e-06,
      "loss": 0.0877,
      "step": 44360
    },
    {
      "epoch": 1.66235809823536,
      "grad_norm": 0.06791771203279495,
      "learning_rate": 7.593255211504879e-06,
      "loss": 0.0897,
      "step": 44370
    },
    {
      "epoch": 1.6627327563598215,
      "grad_norm": 0.06885851174592972,
      "learning_rate": 7.576853809683343e-06,
      "loss": 0.0882,
      "step": 44380
    },
    {
      "epoch": 1.663107414484283,
      "grad_norm": 0.09697437286376953,
      "learning_rate": 7.5604686882216505e-06,
      "loss": 0.0882,
      "step": 44390
    },
    {
      "epoch": 1.6634820726087445,
      "grad_norm": 0.094214528799057,
      "learning_rate": 7.5440998534077825e-06,
      "loss": 0.0895,
      "step": 44400
    },
    {
      "epoch": 1.663856730733206,
      "grad_norm": 0.09314656257629395,
      "learning_rate": 7.527747311523448e-06,
      "loss": 0.0863,
      "step": 44410
    },
    {
      "epoch": 1.6642313888576674,
      "grad_norm": 0.1908903568983078,
      "learning_rate": 7.5114110688441205e-06,
      "loss": 0.0872,
      "step": 44420
    },
    {
      "epoch": 1.6646060469821289,
      "grad_norm": 0.14235155284404755,
      "learning_rate": 7.495091131639043e-06,
      "loss": 0.0873,
      "step": 44430
    },
    {
      "epoch": 1.6649807051065904,
      "grad_norm": 0.07273800671100616,
      "learning_rate": 7.478787506171148e-06,
      "loss": 0.0919,
      "step": 44440
    },
    {
      "epoch": 1.6653553632310518,
      "grad_norm": 0.0870308056473732,
      "learning_rate": 7.462500198697147e-06,
      "loss": 0.0887,
      "step": 44450
    },
    {
      "epoch": 1.665730021355513,
      "grad_norm": 0.07279017567634583,
      "learning_rate": 7.4462292154674704e-06,
      "loss": 0.0901,
      "step": 44460
    },
    {
      "epoch": 1.6661046794799745,
      "grad_norm": 0.07926659286022186,
      "learning_rate": 7.429974562726294e-06,
      "loss": 0.0867,
      "step": 44470
    },
    {
      "epoch": 1.666479337604436,
      "grad_norm": 0.10778764635324478,
      "learning_rate": 7.413736246711533e-06,
      "loss": 0.0855,
      "step": 44480
    },
    {
      "epoch": 1.6668539957288973,
      "grad_norm": 0.0902283787727356,
      "learning_rate": 7.39751427365481e-06,
      "loss": 0.09,
      "step": 44490
    },
    {
      "epoch": 1.6672286538533587,
      "grad_norm": 0.10468148440122604,
      "learning_rate": 7.381308649781498e-06,
      "loss": 0.0875,
      "step": 44500
    },
    {
      "epoch": 1.6676033119778202,
      "grad_norm": 0.10006488859653473,
      "learning_rate": 7.365119381310681e-06,
      "loss": 0.0895,
      "step": 44510
    },
    {
      "epoch": 1.6679779701022817,
      "grad_norm": 0.0772971585392952,
      "learning_rate": 7.348946474455182e-06,
      "loss": 0.0893,
      "step": 44520
    },
    {
      "epoch": 1.6683526282267431,
      "grad_norm": 0.1063908264040947,
      "learning_rate": 7.332789935421536e-06,
      "loss": 0.0909,
      "step": 44530
    },
    {
      "epoch": 1.6687272863512046,
      "grad_norm": 0.0805034190416336,
      "learning_rate": 7.316649770409995e-06,
      "loss": 0.0896,
      "step": 44540
    },
    {
      "epoch": 1.669101944475666,
      "grad_norm": 0.11309930682182312,
      "learning_rate": 7.30052598561452e-06,
      "loss": 0.0901,
      "step": 44550
    },
    {
      "epoch": 1.6694766026001275,
      "grad_norm": 0.0700041726231575,
      "learning_rate": 7.284418587222791e-06,
      "loss": 0.0881,
      "step": 44560
    },
    {
      "epoch": 1.6698512607245888,
      "grad_norm": 0.07283540070056915,
      "learning_rate": 7.268327581416229e-06,
      "loss": 0.083,
      "step": 44570
    },
    {
      "epoch": 1.6702259188490503,
      "grad_norm": 0.07037851214408875,
      "learning_rate": 7.252252974369922e-06,
      "loss": 0.0856,
      "step": 44580
    },
    {
      "epoch": 1.6706005769735117,
      "grad_norm": 0.10501429438591003,
      "learning_rate": 7.2361947722526745e-06,
      "loss": 0.09,
      "step": 44590
    },
    {
      "epoch": 1.670975235097973,
      "grad_norm": 0.11331284791231155,
      "learning_rate": 7.220152981227013e-06,
      "loss": 0.0908,
      "step": 44600
    },
    {
      "epoch": 1.6713498932224344,
      "grad_norm": 0.11904222518205643,
      "learning_rate": 7.204127607449151e-06,
      "loss": 0.0904,
      "step": 44610
    },
    {
      "epoch": 1.671724551346896,
      "grad_norm": 0.06959272176027298,
      "learning_rate": 7.1881186570690095e-06,
      "loss": 0.0883,
      "step": 44620
    },
    {
      "epoch": 1.6720992094713574,
      "grad_norm": 0.10035484284162521,
      "learning_rate": 7.172126136230206e-06,
      "loss": 0.0903,
      "step": 44630
    },
    {
      "epoch": 1.6724738675958188,
      "grad_norm": 0.11406587064266205,
      "learning_rate": 7.156150051070048e-06,
      "loss": 0.0888,
      "step": 44640
    },
    {
      "epoch": 1.6728485257202803,
      "grad_norm": 0.08860410004854202,
      "learning_rate": 7.140190407719522e-06,
      "loss": 0.0899,
      "step": 44650
    },
    {
      "epoch": 1.6732231838447418,
      "grad_norm": 0.08070001751184464,
      "learning_rate": 7.124247212303342e-06,
      "loss": 0.0895,
      "step": 44660
    },
    {
      "epoch": 1.6735978419692032,
      "grad_norm": 0.08401384204626083,
      "learning_rate": 7.108320470939894e-06,
      "loss": 0.0884,
      "step": 44670
    },
    {
      "epoch": 1.6739725000936645,
      "grad_norm": 0.10455173254013062,
      "learning_rate": 7.092410189741222e-06,
      "loss": 0.0909,
      "step": 44680
    },
    {
      "epoch": 1.674347158218126,
      "grad_norm": 0.0913734883069992,
      "learning_rate": 7.07651637481308e-06,
      "loss": 0.0909,
      "step": 44690
    },
    {
      "epoch": 1.6747218163425874,
      "grad_norm": 0.083926722407341,
      "learning_rate": 7.060639032254895e-06,
      "loss": 0.0899,
      "step": 44700
    },
    {
      "epoch": 1.6750964744670487,
      "grad_norm": 0.07537201046943665,
      "learning_rate": 7.04477816815981e-06,
      "loss": 0.0915,
      "step": 44710
    },
    {
      "epoch": 1.6754711325915101,
      "grad_norm": 0.0958389863371849,
      "learning_rate": 7.028933788614572e-06,
      "loss": 0.0843,
      "step": 44720
    },
    {
      "epoch": 1.6758457907159716,
      "grad_norm": 0.0854002833366394,
      "learning_rate": 7.013105899699651e-06,
      "loss": 0.0917,
      "step": 44730
    },
    {
      "epoch": 1.676220448840433,
      "grad_norm": 0.111874520778656,
      "learning_rate": 6.997294507489183e-06,
      "loss": 0.0883,
      "step": 44740
    },
    {
      "epoch": 1.6765951069648946,
      "grad_norm": 0.07443786412477493,
      "learning_rate": 6.981499618050958e-06,
      "loss": 0.0913,
      "step": 44750
    },
    {
      "epoch": 1.676969765089356,
      "grad_norm": 0.11557745933532715,
      "learning_rate": 6.965721237446454e-06,
      "loss": 0.0903,
      "step": 44760
    },
    {
      "epoch": 1.6773444232138175,
      "grad_norm": 0.14724938571453094,
      "learning_rate": 6.949959371730791e-06,
      "loss": 0.0889,
      "step": 44770
    },
    {
      "epoch": 1.677719081338279,
      "grad_norm": 0.0991126075387001,
      "learning_rate": 6.9342140269527845e-06,
      "loss": 0.0939,
      "step": 44780
    },
    {
      "epoch": 1.6780937394627402,
      "grad_norm": 0.13033123314380646,
      "learning_rate": 6.918485209154846e-06,
      "loss": 0.0912,
      "step": 44790
    },
    {
      "epoch": 1.6784683975872017,
      "grad_norm": 0.08662614971399307,
      "learning_rate": 6.902772924373125e-06,
      "loss": 0.0904,
      "step": 44800
    },
    {
      "epoch": 1.6788430557116631,
      "grad_norm": 0.09638919681310654,
      "learning_rate": 6.887077178637385e-06,
      "loss": 0.085,
      "step": 44810
    },
    {
      "epoch": 1.6792177138361244,
      "grad_norm": 0.09222204983234406,
      "learning_rate": 6.871397977971022e-06,
      "loss": 0.092,
      "step": 44820
    },
    {
      "epoch": 1.6795923719605859,
      "grad_norm": 0.06823205947875977,
      "learning_rate": 6.855735328391122e-06,
      "loss": 0.0901,
      "step": 44830
    },
    {
      "epoch": 1.6799670300850473,
      "grad_norm": 0.12549443542957306,
      "learning_rate": 6.840089235908392e-06,
      "loss": 0.0943,
      "step": 44840
    },
    {
      "epoch": 1.6803416882095088,
      "grad_norm": 0.1117849349975586,
      "learning_rate": 6.824459706527208e-06,
      "loss": 0.0897,
      "step": 44850
    },
    {
      "epoch": 1.6807163463339703,
      "grad_norm": 0.08231128752231598,
      "learning_rate": 6.808846746245573e-06,
      "loss": 0.0869,
      "step": 44860
    },
    {
      "epoch": 1.6810910044584317,
      "grad_norm": 0.07570021599531174,
      "learning_rate": 6.793250361055136e-06,
      "loss": 0.089,
      "step": 44870
    },
    {
      "epoch": 1.6814656625828932,
      "grad_norm": 0.0773678719997406,
      "learning_rate": 6.777670556941196e-06,
      "loss": 0.0918,
      "step": 44880
    },
    {
      "epoch": 1.6818403207073547,
      "grad_norm": 0.10954415053129196,
      "learning_rate": 6.762107339882645e-06,
      "loss": 0.0931,
      "step": 44890
    },
    {
      "epoch": 1.682214978831816,
      "grad_norm": 0.10162199288606644,
      "learning_rate": 6.746560715852079e-06,
      "loss": 0.0848,
      "step": 44900
    },
    {
      "epoch": 1.6825896369562774,
      "grad_norm": 0.09699055552482605,
      "learning_rate": 6.731030690815687e-06,
      "loss": 0.0884,
      "step": 44910
    },
    {
      "epoch": 1.6829642950807389,
      "grad_norm": 0.08171539753675461,
      "learning_rate": 6.7155172707332685e-06,
      "loss": 0.0896,
      "step": 44920
    },
    {
      "epoch": 1.6833389532052,
      "grad_norm": 0.09662672877311707,
      "learning_rate": 6.700020461558276e-06,
      "loss": 0.09,
      "step": 44930
    },
    {
      "epoch": 1.6837136113296616,
      "grad_norm": 0.07360690832138062,
      "learning_rate": 6.684540269237805e-06,
      "loss": 0.0921,
      "step": 44940
    },
    {
      "epoch": 1.684088269454123,
      "grad_norm": 0.16522693634033203,
      "learning_rate": 6.669076699712551e-06,
      "loss": 0.0867,
      "step": 44950
    },
    {
      "epoch": 1.6844629275785845,
      "grad_norm": 0.09100854396820068,
      "learning_rate": 6.653629758916813e-06,
      "loss": 0.0907,
      "step": 44960
    },
    {
      "epoch": 1.684837585703046,
      "grad_norm": 0.07055283337831497,
      "learning_rate": 6.638199452778537e-06,
      "loss": 0.0887,
      "step": 44970
    },
    {
      "epoch": 1.6852122438275075,
      "grad_norm": 0.09932044893503189,
      "learning_rate": 6.622785787219282e-06,
      "loss": 0.0918,
      "step": 44980
    },
    {
      "epoch": 1.685586901951969,
      "grad_norm": 0.07919669151306152,
      "learning_rate": 6.60738876815421e-06,
      "loss": 0.0868,
      "step": 44990
    },
    {
      "epoch": 1.6859615600764304,
      "grad_norm": 0.11051055788993835,
      "learning_rate": 6.592008401492106e-06,
      "loss": 0.0864,
      "step": 45000
    },
    {
      "epoch": 1.6863362182008916,
      "grad_norm": 0.09482591599225998,
      "learning_rate": 6.576644693135353e-06,
      "loss": 0.0921,
      "step": 45010
    },
    {
      "epoch": 1.686710876325353,
      "grad_norm": 0.1141030490398407,
      "learning_rate": 6.561297648979953e-06,
      "loss": 0.0893,
      "step": 45020
    },
    {
      "epoch": 1.6870855344498146,
      "grad_norm": 0.09729325771331787,
      "learning_rate": 6.545967274915505e-06,
      "loss": 0.0904,
      "step": 45030
    },
    {
      "epoch": 1.6874601925742758,
      "grad_norm": 0.09120980650186539,
      "learning_rate": 6.530653576825208e-06,
      "loss": 0.0892,
      "step": 45040
    },
    {
      "epoch": 1.6878348506987373,
      "grad_norm": 0.11930578202009201,
      "learning_rate": 6.515356560585878e-06,
      "loss": 0.0903,
      "step": 45050
    },
    {
      "epoch": 1.6882095088231988,
      "grad_norm": 0.1265745311975479,
      "learning_rate": 6.500076232067903e-06,
      "loss": 0.0864,
      "step": 45060
    },
    {
      "epoch": 1.6885841669476602,
      "grad_norm": 0.08220040798187256,
      "learning_rate": 6.48481259713527e-06,
      "loss": 0.0865,
      "step": 45070
    },
    {
      "epoch": 1.6889588250721217,
      "grad_norm": 0.06984470784664154,
      "learning_rate": 6.469565661645616e-06,
      "loss": 0.0904,
      "step": 45080
    },
    {
      "epoch": 1.6893334831965832,
      "grad_norm": 0.11583393812179565,
      "learning_rate": 6.454335431450076e-06,
      "loss": 0.093,
      "step": 45090
    },
    {
      "epoch": 1.6897081413210446,
      "grad_norm": 0.12085025012493134,
      "learning_rate": 6.439121912393447e-06,
      "loss": 0.0901,
      "step": 45100
    },
    {
      "epoch": 1.690082799445506,
      "grad_norm": 0.07111810892820358,
      "learning_rate": 6.423925110314083e-06,
      "loss": 0.0929,
      "step": 45110
    },
    {
      "epoch": 1.6904574575699676,
      "grad_norm": 0.06837742775678635,
      "learning_rate": 6.408745031043928e-06,
      "loss": 0.0887,
      "step": 45120
    },
    {
      "epoch": 1.6908321156944288,
      "grad_norm": 0.08176147937774658,
      "learning_rate": 6.393581680408506e-06,
      "loss": 0.0881,
      "step": 45130
    },
    {
      "epoch": 1.6912067738188903,
      "grad_norm": 0.10745009034872055,
      "learning_rate": 6.3784350642269234e-06,
      "loss": 0.0894,
      "step": 45140
    },
    {
      "epoch": 1.6915814319433515,
      "grad_norm": 0.09420195966959,
      "learning_rate": 6.3633051883118795e-06,
      "loss": 0.0881,
      "step": 45150
    },
    {
      "epoch": 1.691956090067813,
      "grad_norm": 0.10804101824760437,
      "learning_rate": 6.348192058469604e-06,
      "loss": 0.0903,
      "step": 45160
    },
    {
      "epoch": 1.6923307481922745,
      "grad_norm": 0.08762440085411072,
      "learning_rate": 6.333095680499956e-06,
      "loss": 0.0887,
      "step": 45170
    },
    {
      "epoch": 1.692705406316736,
      "grad_norm": 0.11767818033695221,
      "learning_rate": 6.31801606019633e-06,
      "loss": 0.093,
      "step": 45180
    },
    {
      "epoch": 1.6930800644411974,
      "grad_norm": 0.07994038611650467,
      "learning_rate": 6.302953203345718e-06,
      "loss": 0.0914,
      "step": 45190
    },
    {
      "epoch": 1.6934547225656589,
      "grad_norm": 0.09931477904319763,
      "learning_rate": 6.2879071157286305e-06,
      "loss": 0.0881,
      "step": 45200
    },
    {
      "epoch": 1.6938293806901203,
      "grad_norm": 0.09637124091386795,
      "learning_rate": 6.272877803119176e-06,
      "loss": 0.0871,
      "step": 45210
    },
    {
      "epoch": 1.6942040388145818,
      "grad_norm": 0.09532877802848816,
      "learning_rate": 6.257865271285063e-06,
      "loss": 0.0913,
      "step": 45220
    },
    {
      "epoch": 1.6945786969390433,
      "grad_norm": 0.09122905135154724,
      "learning_rate": 6.242869525987477e-06,
      "loss": 0.0917,
      "step": 45230
    },
    {
      "epoch": 1.6949533550635045,
      "grad_norm": 0.11645098030567169,
      "learning_rate": 6.227890572981215e-06,
      "loss": 0.0929,
      "step": 45240
    },
    {
      "epoch": 1.695328013187966,
      "grad_norm": 0.19067738950252533,
      "learning_rate": 6.212928418014624e-06,
      "loss": 0.087,
      "step": 45250
    },
    {
      "epoch": 1.6957026713124272,
      "grad_norm": 0.10251633822917938,
      "learning_rate": 6.197983066829599e-06,
      "loss": 0.0924,
      "step": 45260
    },
    {
      "epoch": 1.6960773294368887,
      "grad_norm": 0.10038183629512787,
      "learning_rate": 6.18305452516158e-06,
      "loss": 0.0887,
      "step": 45270
    },
    {
      "epoch": 1.6964519875613502,
      "grad_norm": 0.08383746445178986,
      "learning_rate": 6.168142798739573e-06,
      "loss": 0.0898,
      "step": 45280
    },
    {
      "epoch": 1.6968266456858117,
      "grad_norm": 0.06931030750274658,
      "learning_rate": 6.153247893286124e-06,
      "loss": 0.0883,
      "step": 45290
    },
    {
      "epoch": 1.6972013038102731,
      "grad_norm": 0.10558795183897018,
      "learning_rate": 6.138369814517286e-06,
      "loss": 0.0862,
      "step": 45300
    },
    {
      "epoch": 1.6975759619347346,
      "grad_norm": 0.09840752184391022,
      "learning_rate": 6.123508568142733e-06,
      "loss": 0.0894,
      "step": 45310
    },
    {
      "epoch": 1.697950620059196,
      "grad_norm": 0.0803590789437294,
      "learning_rate": 6.108664159865629e-06,
      "loss": 0.0895,
      "step": 45320
    },
    {
      "epoch": 1.6983252781836575,
      "grad_norm": 0.08491358906030655,
      "learning_rate": 6.093836595382657e-06,
      "loss": 0.0871,
      "step": 45330
    },
    {
      "epoch": 1.698699936308119,
      "grad_norm": 0.11484307050704956,
      "learning_rate": 6.079025880384082e-06,
      "loss": 0.0881,
      "step": 45340
    },
    {
      "epoch": 1.6990745944325802,
      "grad_norm": 0.0982801541686058,
      "learning_rate": 6.064232020553673e-06,
      "loss": 0.0889,
      "step": 45350
    },
    {
      "epoch": 1.6994492525570417,
      "grad_norm": 0.09123630076646805,
      "learning_rate": 6.049455021568745e-06,
      "loss": 0.0934,
      "step": 45360
    },
    {
      "epoch": 1.6998239106815032,
      "grad_norm": 0.09985742717981339,
      "learning_rate": 6.034694889100134e-06,
      "loss": 0.0896,
      "step": 45370
    },
    {
      "epoch": 1.7001985688059644,
      "grad_norm": 0.0856165736913681,
      "learning_rate": 6.019951628812215e-06,
      "loss": 0.0886,
      "step": 45380
    },
    {
      "epoch": 1.700573226930426,
      "grad_norm": 0.09851489216089249,
      "learning_rate": 6.005225246362883e-06,
      "loss": 0.0876,
      "step": 45390
    },
    {
      "epoch": 1.7009478850548874,
      "grad_norm": 0.08826559036970139,
      "learning_rate": 5.990515747403524e-06,
      "loss": 0.0907,
      "step": 45400
    },
    {
      "epoch": 1.7013225431793488,
      "grad_norm": 0.13183972239494324,
      "learning_rate": 5.975823137579106e-06,
      "loss": 0.09,
      "step": 45410
    },
    {
      "epoch": 1.7016972013038103,
      "grad_norm": 0.21026381850242615,
      "learning_rate": 5.961147422528074e-06,
      "loss": 0.0895,
      "step": 45420
    },
    {
      "epoch": 1.7020718594282718,
      "grad_norm": 0.10485897213220596,
      "learning_rate": 5.9464886078824055e-06,
      "loss": 0.089,
      "step": 45430
    },
    {
      "epoch": 1.7024465175527332,
      "grad_norm": 0.09653381258249283,
      "learning_rate": 5.931846699267557e-06,
      "loss": 0.0921,
      "step": 45440
    },
    {
      "epoch": 1.7028211756771947,
      "grad_norm": 0.08560419827699661,
      "learning_rate": 5.917221702302556e-06,
      "loss": 0.0889,
      "step": 45450
    },
    {
      "epoch": 1.703195833801656,
      "grad_norm": 0.08658923953771591,
      "learning_rate": 5.9026136225999065e-06,
      "loss": 0.086,
      "step": 45460
    },
    {
      "epoch": 1.7035704919261174,
      "grad_norm": 0.07869413495063782,
      "learning_rate": 5.8880224657656135e-06,
      "loss": 0.087,
      "step": 45470
    },
    {
      "epoch": 1.703945150050579,
      "grad_norm": 0.08545326441526413,
      "learning_rate": 5.873448237399193e-06,
      "loss": 0.0887,
      "step": 45480
    },
    {
      "epoch": 1.7043198081750401,
      "grad_norm": 0.12526173889636993,
      "learning_rate": 5.858890943093681e-06,
      "loss": 0.092,
      "step": 45490
    },
    {
      "epoch": 1.7046944662995016,
      "grad_norm": 0.11145778745412827,
      "learning_rate": 5.844350588435599e-06,
      "loss": 0.0901,
      "step": 45500
    },
    {
      "epoch": 1.705069124423963,
      "grad_norm": 0.10485552251338959,
      "learning_rate": 5.829827179004976e-06,
      "loss": 0.092,
      "step": 45510
    },
    {
      "epoch": 1.7054437825484245,
      "grad_norm": 0.0780816376209259,
      "learning_rate": 5.815320720375323e-06,
      "loss": 0.0861,
      "step": 45520
    },
    {
      "epoch": 1.705818440672886,
      "grad_norm": 0.06661409139633179,
      "learning_rate": 5.800831218113678e-06,
      "loss": 0.0866,
      "step": 45530
    },
    {
      "epoch": 1.7061930987973475,
      "grad_norm": 0.16728124022483826,
      "learning_rate": 5.786358677780512e-06,
      "loss": 0.0928,
      "step": 45540
    },
    {
      "epoch": 1.706567756921809,
      "grad_norm": 0.11130188405513763,
      "learning_rate": 5.77190310492986e-06,
      "loss": 0.0909,
      "step": 45550
    },
    {
      "epoch": 1.7069424150462704,
      "grad_norm": 0.06973718851804733,
      "learning_rate": 5.757464505109211e-06,
      "loss": 0.0891,
      "step": 45560
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 0.11684597283601761,
      "learning_rate": 5.743042883859523e-06,
      "loss": 0.0928,
      "step": 45570
    },
    {
      "epoch": 1.7076917312951931,
      "grad_norm": 0.0927751436829567,
      "learning_rate": 5.728638246715245e-06,
      "loss": 0.0917,
      "step": 45580
    },
    {
      "epoch": 1.7080663894196546,
      "grad_norm": 0.11500461399555206,
      "learning_rate": 5.7142505992043625e-06,
      "loss": 0.0886,
      "step": 45590
    },
    {
      "epoch": 1.7084410475441159,
      "grad_norm": 0.09348680824041367,
      "learning_rate": 5.699879946848252e-06,
      "loss": 0.0905,
      "step": 45600
    },
    {
      "epoch": 1.7088157056685773,
      "grad_norm": 0.14344185590744019,
      "learning_rate": 5.685526295161836e-06,
      "loss": 0.0869,
      "step": 45610
    },
    {
      "epoch": 1.7091903637930388,
      "grad_norm": 0.08070273697376251,
      "learning_rate": 5.671189649653485e-06,
      "loss": 0.089,
      "step": 45620
    },
    {
      "epoch": 1.7095650219175003,
      "grad_norm": 0.0731484666466713,
      "learning_rate": 5.65687001582505e-06,
      "loss": 0.09,
      "step": 45630
    },
    {
      "epoch": 1.7099396800419617,
      "grad_norm": 0.10109732300043106,
      "learning_rate": 5.642567399171844e-06,
      "loss": 0.0874,
      "step": 45640
    },
    {
      "epoch": 1.7103143381664232,
      "grad_norm": 0.09112702310085297,
      "learning_rate": 5.628281805182667e-06,
      "loss": 0.0867,
      "step": 45650
    },
    {
      "epoch": 1.7106889962908847,
      "grad_norm": 0.12153366953134537,
      "learning_rate": 5.614013239339771e-06,
      "loss": 0.0897,
      "step": 45660
    },
    {
      "epoch": 1.7110636544153461,
      "grad_norm": 0.09350240230560303,
      "learning_rate": 5.599761707118889e-06,
      "loss": 0.0893,
      "step": 45670
    },
    {
      "epoch": 1.7114383125398074,
      "grad_norm": 0.08089856058359146,
      "learning_rate": 5.585527213989178e-06,
      "loss": 0.0871,
      "step": 45680
    },
    {
      "epoch": 1.7118129706642689,
      "grad_norm": 0.08702124655246735,
      "learning_rate": 5.571309765413318e-06,
      "loss": 0.0886,
      "step": 45690
    },
    {
      "epoch": 1.7121876287887303,
      "grad_norm": 0.09535805881023407,
      "learning_rate": 5.557109366847407e-06,
      "loss": 0.0867,
      "step": 45700
    },
    {
      "epoch": 1.7125622869131916,
      "grad_norm": 0.06551714241504669,
      "learning_rate": 5.542926023740996e-06,
      "loss": 0.0896,
      "step": 45710
    },
    {
      "epoch": 1.712936945037653,
      "grad_norm": 0.061710864305496216,
      "learning_rate": 5.528759741537098e-06,
      "loss": 0.0882,
      "step": 45720
    },
    {
      "epoch": 1.7133116031621145,
      "grad_norm": 0.0807243064045906,
      "learning_rate": 5.514610525672214e-06,
      "loss": 0.0905,
      "step": 45730
    },
    {
      "epoch": 1.713686261286576,
      "grad_norm": 0.09137358516454697,
      "learning_rate": 5.50047838157624e-06,
      "loss": 0.0924,
      "step": 45740
    },
    {
      "epoch": 1.7140609194110374,
      "grad_norm": 0.09862000495195389,
      "learning_rate": 5.4863633146725495e-06,
      "loss": 0.0922,
      "step": 45750
    },
    {
      "epoch": 1.714435577535499,
      "grad_norm": 0.08765202760696411,
      "learning_rate": 5.4722653303779515e-06,
      "loss": 0.0904,
      "step": 45760
    },
    {
      "epoch": 1.7148102356599604,
      "grad_norm": 0.09346066415309906,
      "learning_rate": 5.45818443410272e-06,
      "loss": 0.089,
      "step": 45770
    },
    {
      "epoch": 1.7151848937844218,
      "grad_norm": 0.08480353653430939,
      "learning_rate": 5.444120631250554e-06,
      "loss": 0.0869,
      "step": 45780
    },
    {
      "epoch": 1.715559551908883,
      "grad_norm": 0.09278643876314163,
      "learning_rate": 5.430073927218588e-06,
      "loss": 0.0915,
      "step": 45790
    },
    {
      "epoch": 1.7159342100333446,
      "grad_norm": 0.09420501440763474,
      "learning_rate": 5.4160443273974235e-06,
      "loss": 0.0906,
      "step": 45800
    },
    {
      "epoch": 1.716308868157806,
      "grad_norm": 0.08592591434717178,
      "learning_rate": 5.40203183717104e-06,
      "loss": 0.0877,
      "step": 45810
    },
    {
      "epoch": 1.7166835262822673,
      "grad_norm": 0.09976450353860855,
      "learning_rate": 5.388036461916917e-06,
      "loss": 0.0894,
      "step": 45820
    },
    {
      "epoch": 1.7170581844067287,
      "grad_norm": 0.0827380046248436,
      "learning_rate": 5.374058207005944e-06,
      "loss": 0.0873,
      "step": 45830
    },
    {
      "epoch": 1.7174328425311902,
      "grad_norm": 0.07410291582345963,
      "learning_rate": 5.360097077802412e-06,
      "loss": 0.0875,
      "step": 45840
    },
    {
      "epoch": 1.7178075006556517,
      "grad_norm": 0.0850367322564125,
      "learning_rate": 5.346153079664068e-06,
      "loss": 0.0903,
      "step": 45850
    },
    {
      "epoch": 1.7181821587801132,
      "grad_norm": 0.1036248728632927,
      "learning_rate": 5.332226217942071e-06,
      "loss": 0.0902,
      "step": 45860
    },
    {
      "epoch": 1.7185568169045746,
      "grad_norm": 0.08249656856060028,
      "learning_rate": 5.318316497981041e-06,
      "loss": 0.0906,
      "step": 45870
    },
    {
      "epoch": 1.718931475029036,
      "grad_norm": 0.08115212619304657,
      "learning_rate": 5.304423925118956e-06,
      "loss": 0.0884,
      "step": 45880
    },
    {
      "epoch": 1.7193061331534976,
      "grad_norm": 0.07712630927562714,
      "learning_rate": 5.290548504687265e-06,
      "loss": 0.0884,
      "step": 45890
    },
    {
      "epoch": 1.7196807912779588,
      "grad_norm": 0.08041177690029144,
      "learning_rate": 5.276690242010806e-06,
      "loss": 0.0909,
      "step": 45900
    },
    {
      "epoch": 1.7200554494024203,
      "grad_norm": 0.09063726663589478,
      "learning_rate": 5.262849142407855e-06,
      "loss": 0.091,
      "step": 45910
    },
    {
      "epoch": 1.7204301075268817,
      "grad_norm": 0.08933417499065399,
      "learning_rate": 5.2490252111900786e-06,
      "loss": 0.0857,
      "step": 45920
    },
    {
      "epoch": 1.720804765651343,
      "grad_norm": 0.09887184947729111,
      "learning_rate": 5.235218453662571e-06,
      "loss": 0.0922,
      "step": 45930
    },
    {
      "epoch": 1.7211794237758045,
      "grad_norm": 0.08890412747859955,
      "learning_rate": 5.221428875123846e-06,
      "loss": 0.09,
      "step": 45940
    },
    {
      "epoch": 1.721554081900266,
      "grad_norm": 0.08750100433826447,
      "learning_rate": 5.207656480865769e-06,
      "loss": 0.0898,
      "step": 45950
    },
    {
      "epoch": 1.7219287400247274,
      "grad_norm": 0.10249058157205582,
      "learning_rate": 5.19390127617369e-06,
      "loss": 0.0893,
      "step": 45960
    },
    {
      "epoch": 1.7223033981491889,
      "grad_norm": 0.07737468928098679,
      "learning_rate": 5.1801632663263215e-06,
      "loss": 0.0871,
      "step": 45970
    },
    {
      "epoch": 1.7226780562736503,
      "grad_norm": 0.08000363409519196,
      "learning_rate": 5.166442456595755e-06,
      "loss": 0.09,
      "step": 45980
    },
    {
      "epoch": 1.7230527143981118,
      "grad_norm": 0.1312858760356903,
      "learning_rate": 5.152738852247524e-06,
      "loss": 0.092,
      "step": 45990
    },
    {
      "epoch": 1.7234273725225733,
      "grad_norm": 0.10273418575525284,
      "learning_rate": 5.139052458540533e-06,
      "loss": 0.0933,
      "step": 46000
    },
    {
      "epoch": 1.7238020306470347,
      "grad_norm": 0.07653718441724777,
      "learning_rate": 5.125383280727092e-06,
      "loss": 0.0868,
      "step": 46010
    },
    {
      "epoch": 1.724176688771496,
      "grad_norm": 0.09123679995536804,
      "learning_rate": 5.111731324052904e-06,
      "loss": 0.0908,
      "step": 46020
    },
    {
      "epoch": 1.7245513468959575,
      "grad_norm": 0.09043118357658386,
      "learning_rate": 5.098096593757057e-06,
      "loss": 0.0896,
      "step": 46030
    },
    {
      "epoch": 1.7249260050204187,
      "grad_norm": 0.09996865689754486,
      "learning_rate": 5.084479095072048e-06,
      "loss": 0.0898,
      "step": 46040
    },
    {
      "epoch": 1.7253006631448802,
      "grad_norm": 0.09351841360330582,
      "learning_rate": 5.070878833223708e-06,
      "loss": 0.0917,
      "step": 46050
    },
    {
      "epoch": 1.7256753212693416,
      "grad_norm": 0.08761259913444519,
      "learning_rate": 5.0572958134313285e-06,
      "loss": 0.0869,
      "step": 46060
    },
    {
      "epoch": 1.7260499793938031,
      "grad_norm": 0.10582087188959122,
      "learning_rate": 5.043730040907529e-06,
      "loss": 0.0866,
      "step": 46070
    },
    {
      "epoch": 1.7264246375182646,
      "grad_norm": 0.07703480869531631,
      "learning_rate": 5.0301815208583404e-06,
      "loss": 0.0885,
      "step": 46080
    },
    {
      "epoch": 1.726799295642726,
      "grad_norm": 0.11947879195213318,
      "learning_rate": 5.016650258483135e-06,
      "loss": 0.0884,
      "step": 46090
    },
    {
      "epoch": 1.7271739537671875,
      "grad_norm": 0.08600518852472305,
      "learning_rate": 5.003136258974711e-06,
      "loss": 0.0895,
      "step": 46100
    },
    {
      "epoch": 1.727548611891649,
      "grad_norm": 0.1107967421412468,
      "learning_rate": 4.989639527519213e-06,
      "loss": 0.0882,
      "step": 46110
    },
    {
      "epoch": 1.7279232700161105,
      "grad_norm": 0.0895022377371788,
      "learning_rate": 4.976160069296154e-06,
      "loss": 0.0869,
      "step": 46120
    },
    {
      "epoch": 1.7282979281405717,
      "grad_norm": 0.08761519938707352,
      "learning_rate": 4.962697889478435e-06,
      "loss": 0.0937,
      "step": 46130
    },
    {
      "epoch": 1.7286725862650332,
      "grad_norm": 0.07233143597841263,
      "learning_rate": 4.949252993232318e-06,
      "loss": 0.0918,
      "step": 46140
    },
    {
      "epoch": 1.7290472443894944,
      "grad_norm": 0.08131228387355804,
      "learning_rate": 4.93582538571743e-06,
      "loss": 0.0885,
      "step": 46150
    },
    {
      "epoch": 1.7294219025139559,
      "grad_norm": 0.07402217388153076,
      "learning_rate": 4.922415072086767e-06,
      "loss": 0.0918,
      "step": 46160
    },
    {
      "epoch": 1.7297965606384174,
      "grad_norm": 0.09166990965604782,
      "learning_rate": 4.909022057486696e-06,
      "loss": 0.089,
      "step": 46170
    },
    {
      "epoch": 1.7301712187628788,
      "grad_norm": 0.06903646886348724,
      "learning_rate": 4.8956463470569366e-06,
      "loss": 0.0868,
      "step": 46180
    },
    {
      "epoch": 1.7305458768873403,
      "grad_norm": 0.1011204794049263,
      "learning_rate": 4.8822879459305415e-06,
      "loss": 0.0885,
      "step": 46190
    },
    {
      "epoch": 1.7309205350118018,
      "grad_norm": 0.07276330143213272,
      "learning_rate": 4.868946859233986e-06,
      "loss": 0.0903,
      "step": 46200
    },
    {
      "epoch": 1.7312951931362632,
      "grad_norm": 0.08019067347049713,
      "learning_rate": 4.855623092087047e-06,
      "loss": 0.0882,
      "step": 46210
    },
    {
      "epoch": 1.7316698512607247,
      "grad_norm": 0.12030835449695587,
      "learning_rate": 4.842316649602863e-06,
      "loss": 0.09,
      "step": 46220
    },
    {
      "epoch": 1.7320445093851862,
      "grad_norm": 0.10463777184486389,
      "learning_rate": 4.829027536887926e-06,
      "loss": 0.0901,
      "step": 46230
    },
    {
      "epoch": 1.7324191675096474,
      "grad_norm": 0.07575280219316483,
      "learning_rate": 4.815755759042112e-06,
      "loss": 0.0888,
      "step": 46240
    },
    {
      "epoch": 1.7327938256341089,
      "grad_norm": 0.09472168982028961,
      "learning_rate": 4.802501321158581e-06,
      "loss": 0.0922,
      "step": 46250
    },
    {
      "epoch": 1.7331684837585704,
      "grad_norm": 0.1001213937997818,
      "learning_rate": 4.789264228323892e-06,
      "loss": 0.0857,
      "step": 46260
    },
    {
      "epoch": 1.7335431418830316,
      "grad_norm": 0.08353292942047119,
      "learning_rate": 4.776044485617926e-06,
      "loss": 0.0919,
      "step": 46270
    },
    {
      "epoch": 1.733917800007493,
      "grad_norm": 0.08315306156873703,
      "learning_rate": 4.762842098113907e-06,
      "loss": 0.0888,
      "step": 46280
    },
    {
      "epoch": 1.7342924581319545,
      "grad_norm": 0.09266349673271179,
      "learning_rate": 4.7496570708783934e-06,
      "loss": 0.0883,
      "step": 46290
    },
    {
      "epoch": 1.734667116256416,
      "grad_norm": 0.08435878902673721,
      "learning_rate": 4.736489408971295e-06,
      "loss": 0.0927,
      "step": 46300
    },
    {
      "epoch": 1.7350417743808775,
      "grad_norm": 0.07376956194639206,
      "learning_rate": 4.7233391174458515e-06,
      "loss": 0.0889,
      "step": 46310
    },
    {
      "epoch": 1.735416432505339,
      "grad_norm": 0.10463164001703262,
      "learning_rate": 4.710206201348638e-06,
      "loss": 0.0934,
      "step": 46320
    },
    {
      "epoch": 1.7357910906298004,
      "grad_norm": 0.10270270705223083,
      "learning_rate": 4.697090665719533e-06,
      "loss": 0.0892,
      "step": 46330
    },
    {
      "epoch": 1.7361657487542619,
      "grad_norm": 0.08179150521755219,
      "learning_rate": 4.683992515591806e-06,
      "loss": 0.0916,
      "step": 46340
    },
    {
      "epoch": 1.7365404068787231,
      "grad_norm": 0.1199856773018837,
      "learning_rate": 4.670911755992014e-06,
      "loss": 0.0885,
      "step": 46350
    },
    {
      "epoch": 1.7369150650031846,
      "grad_norm": 0.12498276680707932,
      "learning_rate": 4.657848391940023e-06,
      "loss": 0.0931,
      "step": 46360
    },
    {
      "epoch": 1.737289723127646,
      "grad_norm": 0.0859779417514801,
      "learning_rate": 4.644802428449052e-06,
      "loss": 0.0914,
      "step": 46370
    },
    {
      "epoch": 1.7376643812521073,
      "grad_norm": 0.0777370035648346,
      "learning_rate": 4.631773870525657e-06,
      "loss": 0.0886,
      "step": 46380
    },
    {
      "epoch": 1.7380390393765688,
      "grad_norm": 0.10202658176422119,
      "learning_rate": 4.618762723169679e-06,
      "loss": 0.0902,
      "step": 46390
    },
    {
      "epoch": 1.7384136975010303,
      "grad_norm": 0.08404310792684555,
      "learning_rate": 4.605768991374293e-06,
      "loss": 0.0894,
      "step": 46400
    },
    {
      "epoch": 1.7387883556254917,
      "grad_norm": 0.0766705572605133,
      "learning_rate": 4.592792680125984e-06,
      "loss": 0.0911,
      "step": 46410
    },
    {
      "epoch": 1.7391630137499532,
      "grad_norm": 0.09190426021814346,
      "learning_rate": 4.57983379440457e-06,
      "loss": 0.0886,
      "step": 46420
    },
    {
      "epoch": 1.7395376718744147,
      "grad_norm": 0.07673730701208115,
      "learning_rate": 4.5668923391831596e-06,
      "loss": 0.0874,
      "step": 46430
    },
    {
      "epoch": 1.7399123299988761,
      "grad_norm": 0.07459480315446854,
      "learning_rate": 4.553968319428187e-06,
      "loss": 0.0877,
      "step": 46440
    },
    {
      "epoch": 1.7402869881233376,
      "grad_norm": 0.0725565180182457,
      "learning_rate": 4.5410617400994e-06,
      "loss": 0.0913,
      "step": 46450
    },
    {
      "epoch": 1.7406616462477988,
      "grad_norm": 0.081167072057724,
      "learning_rate": 4.5281726061498216e-06,
      "loss": 0.0906,
      "step": 46460
    },
    {
      "epoch": 1.7410363043722603,
      "grad_norm": 0.10295813530683517,
      "learning_rate": 4.5153009225258e-06,
      "loss": 0.0887,
      "step": 46470
    },
    {
      "epoch": 1.7414109624967218,
      "grad_norm": 0.07037648558616638,
      "learning_rate": 4.502446694167029e-06,
      "loss": 0.0883,
      "step": 46480
    },
    {
      "epoch": 1.741785620621183,
      "grad_norm": 0.11173412948846817,
      "learning_rate": 4.489609926006416e-06,
      "loss": 0.0903,
      "step": 46490
    },
    {
      "epoch": 1.7421602787456445,
      "grad_norm": 0.13264161348342896,
      "learning_rate": 4.476790622970245e-06,
      "loss": 0.0887,
      "step": 46500
    },
    {
      "epoch": 1.742534936870106,
      "grad_norm": 0.0905161201953888,
      "learning_rate": 4.463988789978041e-06,
      "loss": 0.0913,
      "step": 46510
    },
    {
      "epoch": 1.7429095949945674,
      "grad_norm": 0.08167087286710739,
      "learning_rate": 4.45120443194269e-06,
      "loss": 0.0894,
      "step": 46520
    },
    {
      "epoch": 1.743284253119029,
      "grad_norm": 0.08774742484092712,
      "learning_rate": 4.438437553770309e-06,
      "loss": 0.0914,
      "step": 46530
    },
    {
      "epoch": 1.7436589112434904,
      "grad_norm": 0.113862544298172,
      "learning_rate": 4.425688160360325e-06,
      "loss": 0.091,
      "step": 46540
    },
    {
      "epoch": 1.7440335693679518,
      "grad_norm": 0.11220875382423401,
      "learning_rate": 4.4129562566054715e-06,
      "loss": 0.089,
      "step": 46550
    },
    {
      "epoch": 1.7444082274924133,
      "grad_norm": 0.11088445037603378,
      "learning_rate": 4.400241847391762e-06,
      "loss": 0.0889,
      "step": 46560
    },
    {
      "epoch": 1.7447828856168746,
      "grad_norm": 0.08773218095302582,
      "learning_rate": 4.387544937598492e-06,
      "loss": 0.0913,
      "step": 46570
    },
    {
      "epoch": 1.745157543741336,
      "grad_norm": 0.12137085944414139,
      "learning_rate": 4.374865532098238e-06,
      "loss": 0.0905,
      "step": 46580
    },
    {
      "epoch": 1.7455322018657975,
      "grad_norm": 0.08629510551691055,
      "learning_rate": 4.362203635756878e-06,
      "loss": 0.089,
      "step": 46590
    },
    {
      "epoch": 1.7459068599902587,
      "grad_norm": 0.0959547758102417,
      "learning_rate": 4.349559253433533e-06,
      "loss": 0.0918,
      "step": 46600
    },
    {
      "epoch": 1.7462815181147202,
      "grad_norm": 0.10807203501462936,
      "learning_rate": 4.336932389980653e-06,
      "loss": 0.0916,
      "step": 46610
    },
    {
      "epoch": 1.7466561762391817,
      "grad_norm": 0.07936222851276398,
      "learning_rate": 4.3243230502439334e-06,
      "loss": 0.0879,
      "step": 46620
    },
    {
      "epoch": 1.7470308343636431,
      "grad_norm": 0.0802270695567131,
      "learning_rate": 4.311731239062339e-06,
      "loss": 0.0884,
      "step": 46630
    },
    {
      "epoch": 1.7474054924881046,
      "grad_norm": 0.09800884127616882,
      "learning_rate": 4.299156961268119e-06,
      "loss": 0.0866,
      "step": 46640
    },
    {
      "epoch": 1.747780150612566,
      "grad_norm": 0.09585138410329819,
      "learning_rate": 4.2866002216868036e-06,
      "loss": 0.092,
      "step": 46650
    },
    {
      "epoch": 1.7481548087370276,
      "grad_norm": 0.07003336399793625,
      "learning_rate": 4.274061025137183e-06,
      "loss": 0.0901,
      "step": 46660
    },
    {
      "epoch": 1.748529466861489,
      "grad_norm": 0.08619499951601028,
      "learning_rate": 4.261539376431306e-06,
      "loss": 0.0894,
      "step": 46670
    },
    {
      "epoch": 1.7489041249859503,
      "grad_norm": 0.09254918247461319,
      "learning_rate": 4.249035280374508e-06,
      "loss": 0.0924,
      "step": 46680
    },
    {
      "epoch": 1.7492787831104117,
      "grad_norm": 0.09005812555551529,
      "learning_rate": 4.236548741765373e-06,
      "loss": 0.0884,
      "step": 46690
    },
    {
      "epoch": 1.7496534412348732,
      "grad_norm": 0.08558817952871323,
      "learning_rate": 4.224079765395733e-06,
      "loss": 0.0879,
      "step": 46700
    },
    {
      "epoch": 1.7500280993593345,
      "grad_norm": 0.09465640038251877,
      "learning_rate": 4.211628356050723e-06,
      "loss": 0.0885,
      "step": 46710
    },
    {
      "epoch": 1.750402757483796,
      "grad_norm": 0.09578218311071396,
      "learning_rate": 4.199194518508709e-06,
      "loss": 0.0924,
      "step": 46720
    },
    {
      "epoch": 1.7507774156082574,
      "grad_norm": 0.15267205238342285,
      "learning_rate": 4.186778257541296e-06,
      "loss": 0.0896,
      "step": 46730
    },
    {
      "epoch": 1.7511520737327189,
      "grad_norm": 0.10830408334732056,
      "learning_rate": 4.1743795779133736e-06,
      "loss": 0.0915,
      "step": 46740
    },
    {
      "epoch": 1.7515267318571803,
      "grad_norm": 0.09363158792257309,
      "learning_rate": 4.161998484383084e-06,
      "loss": 0.0878,
      "step": 46750
    },
    {
      "epoch": 1.7519013899816418,
      "grad_norm": 0.0701741874217987,
      "learning_rate": 4.1496349817018166e-06,
      "loss": 0.0916,
      "step": 46760
    },
    {
      "epoch": 1.7522760481061033,
      "grad_norm": 0.07898836582899094,
      "learning_rate": 4.137289074614181e-06,
      "loss": 0.0877,
      "step": 46770
    },
    {
      "epoch": 1.7526507062305647,
      "grad_norm": 0.08053863793611526,
      "learning_rate": 4.1249607678580655e-06,
      "loss": 0.0899,
      "step": 46780
    },
    {
      "epoch": 1.753025364355026,
      "grad_norm": 0.09592155367136002,
      "learning_rate": 4.1126500661646085e-06,
      "loss": 0.0883,
      "step": 46790
    },
    {
      "epoch": 1.7534000224794875,
      "grad_norm": 0.07607554644346237,
      "learning_rate": 4.100356974258169e-06,
      "loss": 0.0881,
      "step": 46800
    },
    {
      "epoch": 1.753774680603949,
      "grad_norm": 0.08150479197502136,
      "learning_rate": 4.088081496856366e-06,
      "loss": 0.0885,
      "step": 46810
    },
    {
      "epoch": 1.7541493387284102,
      "grad_norm": 0.07144075632095337,
      "learning_rate": 4.075823638670046e-06,
      "loss": 0.0906,
      "step": 46820
    },
    {
      "epoch": 1.7545239968528716,
      "grad_norm": 0.10342466086149216,
      "learning_rate": 4.063583404403315e-06,
      "loss": 0.0898,
      "step": 46830
    },
    {
      "epoch": 1.754898654977333,
      "grad_norm": 0.20914489030838013,
      "learning_rate": 4.051360798753473e-06,
      "loss": 0.0879,
      "step": 46840
    },
    {
      "epoch": 1.7552733131017946,
      "grad_norm": 0.08359792083501816,
      "learning_rate": 4.0391558264111065e-06,
      "loss": 0.0878,
      "step": 46850
    },
    {
      "epoch": 1.755647971226256,
      "grad_norm": 0.0931815356016159,
      "learning_rate": 4.026968492060007e-06,
      "loss": 0.0899,
      "step": 46860
    },
    {
      "epoch": 1.7560226293507175,
      "grad_norm": 0.09634122997522354,
      "learning_rate": 4.01479880037719e-06,
      "loss": 0.09,
      "step": 46870
    },
    {
      "epoch": 1.756397287475179,
      "grad_norm": 0.09384883940219879,
      "learning_rate": 4.0026467560329125e-06,
      "loss": 0.0864,
      "step": 46880
    },
    {
      "epoch": 1.7567719455996404,
      "grad_norm": 0.07048125565052032,
      "learning_rate": 3.990512363690685e-06,
      "loss": 0.0875,
      "step": 46890
    },
    {
      "epoch": 1.757146603724102,
      "grad_norm": 0.08861418813467026,
      "learning_rate": 3.978395628007181e-06,
      "loss": 0.092,
      "step": 46900
    },
    {
      "epoch": 1.7575212618485632,
      "grad_norm": 0.0734960287809372,
      "learning_rate": 3.9662965536323534e-06,
      "loss": 0.091,
      "step": 46910
    },
    {
      "epoch": 1.7578959199730246,
      "grad_norm": 0.08818808197975159,
      "learning_rate": 3.954215145209356e-06,
      "loss": 0.0888,
      "step": 46920
    },
    {
      "epoch": 1.7582705780974859,
      "grad_norm": 0.15934453904628754,
      "learning_rate": 3.942151407374567e-06,
      "loss": 0.0882,
      "step": 46930
    },
    {
      "epoch": 1.7586452362219473,
      "grad_norm": 0.09478729218244553,
      "learning_rate": 3.930105344757573e-06,
      "loss": 0.0883,
      "step": 46940
    },
    {
      "epoch": 1.7590198943464088,
      "grad_norm": 0.08339517563581467,
      "learning_rate": 3.918076961981193e-06,
      "loss": 0.0901,
      "step": 46950
    },
    {
      "epoch": 1.7593945524708703,
      "grad_norm": 0.08524052053689957,
      "learning_rate": 3.906066263661462e-06,
      "loss": 0.091,
      "step": 46960
    },
    {
      "epoch": 1.7597692105953318,
      "grad_norm": 0.14475968480110168,
      "learning_rate": 3.894073254407604e-06,
      "loss": 0.0899,
      "step": 46970
    },
    {
      "epoch": 1.7601438687197932,
      "grad_norm": 0.06349033117294312,
      "learning_rate": 3.882097938822071e-06,
      "loss": 0.0888,
      "step": 46980
    },
    {
      "epoch": 1.7605185268442547,
      "grad_norm": 0.09355738013982773,
      "learning_rate": 3.870140321500543e-06,
      "loss": 0.0908,
      "step": 46990
    },
    {
      "epoch": 1.7608931849687162,
      "grad_norm": 0.10039607435464859,
      "learning_rate": 3.858200407031887e-06,
      "loss": 0.0899,
      "step": 47000
    },
    {
      "epoch": 1.7612678430931776,
      "grad_norm": 0.0924166813492775,
      "learning_rate": 3.84627819999816e-06,
      "loss": 0.0864,
      "step": 47010
    },
    {
      "epoch": 1.7616425012176389,
      "grad_norm": 0.06874362379312515,
      "learning_rate": 3.834373704974653e-06,
      "loss": 0.0891,
      "step": 47020
    },
    {
      "epoch": 1.7620171593421003,
      "grad_norm": 0.11317746341228485,
      "learning_rate": 3.822486926529872e-06,
      "loss": 0.0923,
      "step": 47030
    },
    {
      "epoch": 1.7623918174665618,
      "grad_norm": 0.13228212296962738,
      "learning_rate": 3.810617869225469e-06,
      "loss": 0.0897,
      "step": 47040
    },
    {
      "epoch": 1.762766475591023,
      "grad_norm": 0.08926358819007874,
      "learning_rate": 3.798766537616344e-06,
      "loss": 0.0844,
      "step": 47050
    },
    {
      "epoch": 1.7631411337154845,
      "grad_norm": 0.06942026317119598,
      "learning_rate": 3.7869329362505813e-06,
      "loss": 0.089,
      "step": 47060
    },
    {
      "epoch": 1.763515791839946,
      "grad_norm": 0.10740158706903458,
      "learning_rate": 3.7751170696694483e-06,
      "loss": 0.0857,
      "step": 47070
    },
    {
      "epoch": 1.7638904499644075,
      "grad_norm": 0.08951640874147415,
      "learning_rate": 3.7633189424074287e-06,
      "loss": 0.0909,
      "step": 47080
    },
    {
      "epoch": 1.764265108088869,
      "grad_norm": 0.08919387310743332,
      "learning_rate": 3.751538558992179e-06,
      "loss": 0.0905,
      "step": 47090
    },
    {
      "epoch": 1.7646397662133304,
      "grad_norm": 0.09385070949792862,
      "learning_rate": 3.739775923944566e-06,
      "loss": 0.0881,
      "step": 47100
    },
    {
      "epoch": 1.7650144243377919,
      "grad_norm": 0.07440418750047684,
      "learning_rate": 3.7280310417786126e-06,
      "loss": 0.0938,
      "step": 47110
    },
    {
      "epoch": 1.7653890824622533,
      "grad_norm": 0.08179093897342682,
      "learning_rate": 3.7163039170015514e-06,
      "loss": 0.0902,
      "step": 47120
    },
    {
      "epoch": 1.7657637405867146,
      "grad_norm": 0.12249074131250381,
      "learning_rate": 3.7045945541138273e-06,
      "loss": 0.0896,
      "step": 47130
    },
    {
      "epoch": 1.766138398711176,
      "grad_norm": 0.1097121313214302,
      "learning_rate": 3.692902957609007e-06,
      "loss": 0.091,
      "step": 47140
    },
    {
      "epoch": 1.7665130568356375,
      "grad_norm": 0.09179223328828812,
      "learning_rate": 3.68122913197389e-06,
      "loss": 0.0917,
      "step": 47150
    },
    {
      "epoch": 1.7668877149600988,
      "grad_norm": 0.10405297577381134,
      "learning_rate": 3.669573081688432e-06,
      "loss": 0.0863,
      "step": 47160
    },
    {
      "epoch": 1.7672623730845602,
      "grad_norm": 0.10099554806947708,
      "learning_rate": 3.6579348112257715e-06,
      "loss": 0.0934,
      "step": 47170
    },
    {
      "epoch": 1.7676370312090217,
      "grad_norm": 0.0893561914563179,
      "learning_rate": 3.646314325052236e-06,
      "loss": 0.0908,
      "step": 47180
    },
    {
      "epoch": 1.7680116893334832,
      "grad_norm": 0.1064867302775383,
      "learning_rate": 3.6347116276273076e-06,
      "loss": 0.0907,
      "step": 47190
    },
    {
      "epoch": 1.7683863474579447,
      "grad_norm": 0.08690061420202255,
      "learning_rate": 3.623126723403664e-06,
      "loss": 0.087,
      "step": 47200
    },
    {
      "epoch": 1.7687610055824061,
      "grad_norm": 0.10189219564199448,
      "learning_rate": 3.6115596168271204e-06,
      "loss": 0.0922,
      "step": 47210
    },
    {
      "epoch": 1.7691356637068676,
      "grad_norm": 0.09425466507673264,
      "learning_rate": 3.6000103123367045e-06,
      "loss": 0.0889,
      "step": 47220
    },
    {
      "epoch": 1.769510321831329,
      "grad_norm": 0.08313138782978058,
      "learning_rate": 3.588478814364582e-06,
      "loss": 0.0868,
      "step": 47230
    },
    {
      "epoch": 1.7698849799557903,
      "grad_norm": 0.08281580358743668,
      "learning_rate": 3.5769651273361125e-06,
      "loss": 0.0905,
      "step": 47240
    },
    {
      "epoch": 1.7702596380802518,
      "grad_norm": 0.08165984600782394,
      "learning_rate": 3.565469255669779e-06,
      "loss": 0.0905,
      "step": 47250
    },
    {
      "epoch": 1.7706342962047132,
      "grad_norm": 0.08059220761060715,
      "learning_rate": 3.5539912037772516e-06,
      "loss": 0.0935,
      "step": 47260
    },
    {
      "epoch": 1.7710089543291745,
      "grad_norm": 0.11366155743598938,
      "learning_rate": 3.5425309760633908e-06,
      "loss": 0.0903,
      "step": 47270
    },
    {
      "epoch": 1.771383612453636,
      "grad_norm": 0.11283766478300095,
      "learning_rate": 3.531088576926156e-06,
      "loss": 0.0873,
      "step": 47280
    },
    {
      "epoch": 1.7717582705780974,
      "grad_norm": 0.08927726745605469,
      "learning_rate": 3.519664010756718e-06,
      "loss": 0.0911,
      "step": 47290
    },
    {
      "epoch": 1.772132928702559,
      "grad_norm": 0.10751117765903473,
      "learning_rate": 3.50825728193937e-06,
      "loss": 0.0908,
      "step": 47300
    },
    {
      "epoch": 1.7725075868270204,
      "grad_norm": 0.09667197614908218,
      "learning_rate": 3.4968683948515767e-06,
      "loss": 0.0889,
      "step": 47310
    },
    {
      "epoch": 1.7728822449514818,
      "grad_norm": 0.1217857152223587,
      "learning_rate": 3.485497353863948e-06,
      "loss": 0.0883,
      "step": 47320
    },
    {
      "epoch": 1.7732569030759433,
      "grad_norm": 0.15363284945487976,
      "learning_rate": 3.474144163340259e-06,
      "loss": 0.09,
      "step": 47330
    },
    {
      "epoch": 1.7736315612004048,
      "grad_norm": 0.1200852319598198,
      "learning_rate": 3.462808827637426e-06,
      "loss": 0.0905,
      "step": 47340
    },
    {
      "epoch": 1.774006219324866,
      "grad_norm": 0.10755051672458649,
      "learning_rate": 3.451491351105485e-06,
      "loss": 0.093,
      "step": 47350
    },
    {
      "epoch": 1.7743808774493275,
      "grad_norm": 0.08254623413085938,
      "learning_rate": 3.4401917380876726e-06,
      "loss": 0.0919,
      "step": 47360
    },
    {
      "epoch": 1.774755535573789,
      "grad_norm": 0.08209729939699173,
      "learning_rate": 3.428909992920343e-06,
      "loss": 0.0886,
      "step": 47370
    },
    {
      "epoch": 1.7751301936982502,
      "grad_norm": 0.09202922880649567,
      "learning_rate": 3.417646119932977e-06,
      "loss": 0.0909,
      "step": 47380
    },
    {
      "epoch": 1.7755048518227117,
      "grad_norm": 0.08901572972536087,
      "learning_rate": 3.4064001234482056e-06,
      "loss": 0.0899,
      "step": 47390
    },
    {
      "epoch": 1.7758795099471731,
      "grad_norm": 0.1535765826702118,
      "learning_rate": 3.395172007781844e-06,
      "loss": 0.0889,
      "step": 47400
    },
    {
      "epoch": 1.7762541680716346,
      "grad_norm": 0.11364798992872238,
      "learning_rate": 3.383961777242772e-06,
      "loss": 0.0919,
      "step": 47410
    },
    {
      "epoch": 1.776628826196096,
      "grad_norm": 0.12940147519111633,
      "learning_rate": 3.372769436133055e-06,
      "loss": 0.0907,
      "step": 47420
    },
    {
      "epoch": 1.7770034843205575,
      "grad_norm": 0.12643380463123322,
      "learning_rate": 3.3615949887478735e-06,
      "loss": 0.0895,
      "step": 47430
    },
    {
      "epoch": 1.777378142445019,
      "grad_norm": 0.10353127121925354,
      "learning_rate": 3.3504384393755582e-06,
      "loss": 0.091,
      "step": 47440
    },
    {
      "epoch": 1.7777528005694805,
      "grad_norm": 0.1313442885875702,
      "learning_rate": 3.3392997922975568e-06,
      "loss": 0.0921,
      "step": 47450
    },
    {
      "epoch": 1.7781274586939417,
      "grad_norm": 0.09709260612726212,
      "learning_rate": 3.3281790517884438e-06,
      "loss": 0.0897,
      "step": 47460
    },
    {
      "epoch": 1.7785021168184032,
      "grad_norm": 0.08458956331014633,
      "learning_rate": 3.3170762221159392e-06,
      "loss": 0.0885,
      "step": 47470
    },
    {
      "epoch": 1.7788767749428647,
      "grad_norm": 0.09742115437984467,
      "learning_rate": 3.3059913075408734e-06,
      "loss": 0.0869,
      "step": 47480
    },
    {
      "epoch": 1.779251433067326,
      "grad_norm": 0.09367817640304565,
      "learning_rate": 3.294924312317199e-06,
      "loss": 0.0884,
      "step": 47490
    },
    {
      "epoch": 1.7796260911917874,
      "grad_norm": 0.07959200441837311,
      "learning_rate": 3.2838752406920136e-06,
      "loss": 0.0889,
      "step": 47500
    },
    {
      "epoch": 1.7800007493162489,
      "grad_norm": 0.07952257245779037,
      "learning_rate": 3.27284409690552e-06,
      "loss": 0.0878,
      "step": 47510
    },
    {
      "epoch": 1.7803754074407103,
      "grad_norm": 0.06562527269124985,
      "learning_rate": 3.2618308851910364e-06,
      "loss": 0.0861,
      "step": 47520
    },
    {
      "epoch": 1.7807500655651718,
      "grad_norm": 0.1294773370027542,
      "learning_rate": 3.250835609775005e-06,
      "loss": 0.0908,
      "step": 47530
    },
    {
      "epoch": 1.7811247236896333,
      "grad_norm": 0.08071468025445938,
      "learning_rate": 3.2398582748770058e-06,
      "loss": 0.0879,
      "step": 47540
    },
    {
      "epoch": 1.7814993818140947,
      "grad_norm": 0.06992997229099274,
      "learning_rate": 3.2288988847096966e-06,
      "loss": 0.0889,
      "step": 47550
    },
    {
      "epoch": 1.7818740399385562,
      "grad_norm": 0.09310821443796158,
      "learning_rate": 3.2179574434788695e-06,
      "loss": 0.0928,
      "step": 47560
    },
    {
      "epoch": 1.7822486980630174,
      "grad_norm": 0.0824059396982193,
      "learning_rate": 3.207033955383426e-06,
      "loss": 0.0863,
      "step": 47570
    },
    {
      "epoch": 1.782623356187479,
      "grad_norm": 0.12226643413305283,
      "learning_rate": 3.1961284246153855e-06,
      "loss": 0.0912,
      "step": 47580
    },
    {
      "epoch": 1.7829980143119404,
      "grad_norm": 0.10395687818527222,
      "learning_rate": 3.1852408553598557e-06,
      "loss": 0.0884,
      "step": 47590
    },
    {
      "epoch": 1.7833726724364016,
      "grad_norm": 0.07588890194892883,
      "learning_rate": 3.1743712517950718e-06,
      "loss": 0.0869,
      "step": 47600
    },
    {
      "epoch": 1.783747330560863,
      "grad_norm": 0.07996102422475815,
      "learning_rate": 3.1635196180923754e-06,
      "loss": 0.0857,
      "step": 47610
    },
    {
      "epoch": 1.7841219886853246,
      "grad_norm": 0.1787087619304657,
      "learning_rate": 3.152685958416185e-06,
      "loss": 0.0939,
      "step": 47620
    },
    {
      "epoch": 1.784496646809786,
      "grad_norm": 0.09064168483018875,
      "learning_rate": 3.141870276924036e-06,
      "loss": 0.0889,
      "step": 47630
    },
    {
      "epoch": 1.7848713049342475,
      "grad_norm": 0.08122380822896957,
      "learning_rate": 3.131072577766592e-06,
      "loss": 0.0884,
      "step": 47640
    },
    {
      "epoch": 1.785245963058709,
      "grad_norm": 0.10198253393173218,
      "learning_rate": 3.120292865087582e-06,
      "loss": 0.0904,
      "step": 47650
    },
    {
      "epoch": 1.7856206211831704,
      "grad_norm": 0.0859362855553627,
      "learning_rate": 3.1095311430238305e-06,
      "loss": 0.0898,
      "step": 47660
    },
    {
      "epoch": 1.785995279307632,
      "grad_norm": 0.10411881655454636,
      "learning_rate": 3.098787415705273e-06,
      "loss": 0.0913,
      "step": 47670
    },
    {
      "epoch": 1.7863699374320934,
      "grad_norm": 0.11432132869958878,
      "learning_rate": 3.0880616872549562e-06,
      "loss": 0.0928,
      "step": 47680
    },
    {
      "epoch": 1.7867445955565546,
      "grad_norm": 0.10890371352434158,
      "learning_rate": 3.0773539617889702e-06,
      "loss": 0.0886,
      "step": 47690
    },
    {
      "epoch": 1.787119253681016,
      "grad_norm": 0.1020401269197464,
      "learning_rate": 3.0666642434165448e-06,
      "loss": 0.0889,
      "step": 47700
    },
    {
      "epoch": 1.7874939118054773,
      "grad_norm": 0.12882138788700104,
      "learning_rate": 3.055992536239971e-06,
      "loss": 0.0903,
      "step": 47710
    },
    {
      "epoch": 1.7878685699299388,
      "grad_norm": 0.0867944285273552,
      "learning_rate": 3.0453388443546393e-06,
      "loss": 0.0904,
      "step": 47720
    },
    {
      "epoch": 1.7882432280544003,
      "grad_norm": 0.09257014840841293,
      "learning_rate": 3.034703171849018e-06,
      "loss": 0.0906,
      "step": 47730
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 0.07254840433597565,
      "learning_rate": 3.024085522804676e-06,
      "loss": 0.0875,
      "step": 47740
    },
    {
      "epoch": 1.7889925443033232,
      "grad_norm": 0.08878811448812485,
      "learning_rate": 3.013485901296259e-06,
      "loss": 0.0895,
      "step": 47750
    },
    {
      "epoch": 1.7893672024277847,
      "grad_norm": 0.07770784944295883,
      "learning_rate": 3.00290431139148e-06,
      "loss": 0.0882,
      "step": 47760
    },
    {
      "epoch": 1.7897418605522462,
      "grad_norm": 0.0822996199131012,
      "learning_rate": 2.99234075715113e-06,
      "loss": 0.0925,
      "step": 47770
    },
    {
      "epoch": 1.7901165186767076,
      "grad_norm": 0.09212801605463028,
      "learning_rate": 2.981795242629132e-06,
      "loss": 0.0895,
      "step": 47780
    },
    {
      "epoch": 1.790491176801169,
      "grad_norm": 0.10654404759407043,
      "learning_rate": 2.9712677718724104e-06,
      "loss": 0.0937,
      "step": 47790
    },
    {
      "epoch": 1.7908658349256303,
      "grad_norm": 0.10034170746803284,
      "learning_rate": 2.9607583489210113e-06,
      "loss": 0.0902,
      "step": 47800
    },
    {
      "epoch": 1.7912404930500918,
      "grad_norm": 0.093370221555233,
      "learning_rate": 2.9502669778080473e-06,
      "loss": 0.0887,
      "step": 47810
    },
    {
      "epoch": 1.791615151174553,
      "grad_norm": 0.08476313203573227,
      "learning_rate": 2.939793662559692e-06,
      "loss": 0.0905,
      "step": 47820
    },
    {
      "epoch": 1.7919898092990145,
      "grad_norm": 0.10108380019664764,
      "learning_rate": 2.929338407195209e-06,
      "loss": 0.0902,
      "step": 47830
    },
    {
      "epoch": 1.792364467423476,
      "grad_norm": 0.08302557468414307,
      "learning_rate": 2.9189012157269156e-06,
      "loss": 0.0888,
      "step": 47840
    },
    {
      "epoch": 1.7927391255479375,
      "grad_norm": 0.1082543432712555,
      "learning_rate": 2.9084820921602087e-06,
      "loss": 0.0919,
      "step": 47850
    },
    {
      "epoch": 1.793113783672399,
      "grad_norm": 0.09216299653053284,
      "learning_rate": 2.898081040493522e-06,
      "loss": 0.0918,
      "step": 47860
    },
    {
      "epoch": 1.7934884417968604,
      "grad_norm": 0.08620108664035797,
      "learning_rate": 2.887698064718397e-06,
      "loss": 0.0904,
      "step": 47870
    },
    {
      "epoch": 1.7938630999213219,
      "grad_norm": 0.09151270240545273,
      "learning_rate": 2.877333168819413e-06,
      "loss": 0.0892,
      "step": 47880
    },
    {
      "epoch": 1.7942377580457833,
      "grad_norm": 0.09822604805231094,
      "learning_rate": 2.8669863567742206e-06,
      "loss": 0.0851,
      "step": 47890
    },
    {
      "epoch": 1.7946124161702448,
      "grad_norm": 0.12789249420166016,
      "learning_rate": 2.856657632553511e-06,
      "loss": 0.0875,
      "step": 47900
    },
    {
      "epoch": 1.794987074294706,
      "grad_norm": 0.08975360542535782,
      "learning_rate": 2.846347000121047e-06,
      "loss": 0.0879,
      "step": 47910
    },
    {
      "epoch": 1.7953617324191675,
      "grad_norm": 0.09135659784078598,
      "learning_rate": 2.836054463433674e-06,
      "loss": 0.0934,
      "step": 47920
    },
    {
      "epoch": 1.795736390543629,
      "grad_norm": 0.09125927835702896,
      "learning_rate": 2.8257800264412494e-06,
      "loss": 0.0881,
      "step": 47930
    },
    {
      "epoch": 1.7961110486680902,
      "grad_norm": 0.0990452840924263,
      "learning_rate": 2.8155236930867026e-06,
      "loss": 0.0878,
      "step": 47940
    },
    {
      "epoch": 1.7964857067925517,
      "grad_norm": 0.09999772906303406,
      "learning_rate": 2.8052854673060236e-06,
      "loss": 0.0911,
      "step": 47950
    },
    {
      "epoch": 1.7968603649170132,
      "grad_norm": 0.12173950672149658,
      "learning_rate": 2.795065353028248e-06,
      "loss": 0.0886,
      "step": 47960
    },
    {
      "epoch": 1.7972350230414746,
      "grad_norm": 0.102826789021492,
      "learning_rate": 2.78486335417546e-06,
      "loss": 0.0922,
      "step": 47970
    },
    {
      "epoch": 1.7976096811659361,
      "grad_norm": 0.11769266426563263,
      "learning_rate": 2.7746794746627835e-06,
      "loss": 0.0895,
      "step": 47980
    },
    {
      "epoch": 1.7979843392903976,
      "grad_norm": 0.1002531498670578,
      "learning_rate": 2.764513718398415e-06,
      "loss": 0.0887,
      "step": 47990
    },
    {
      "epoch": 1.798358997414859,
      "grad_norm": 0.10964382439851761,
      "learning_rate": 2.7543660892835442e-06,
      "loss": 0.0914,
      "step": 48000
    },
    {
      "epoch": 1.7987336555393205,
      "grad_norm": 0.22640231251716614,
      "learning_rate": 2.7442365912124733e-06,
      "loss": 0.0895,
      "step": 48010
    },
    {
      "epoch": 1.7991083136637818,
      "grad_norm": 0.07769378274679184,
      "learning_rate": 2.7341252280724982e-06,
      "loss": 0.0887,
      "step": 48020
    },
    {
      "epoch": 1.7994829717882432,
      "grad_norm": 0.0839703157544136,
      "learning_rate": 2.724032003743965e-06,
      "loss": 0.0875,
      "step": 48030
    },
    {
      "epoch": 1.7998576299127047,
      "grad_norm": 0.09462495148181915,
      "learning_rate": 2.7139569221002527e-06,
      "loss": 0.0895,
      "step": 48040
    },
    {
      "epoch": 1.800232288037166,
      "grad_norm": 0.08292774111032486,
      "learning_rate": 2.7038999870078187e-06,
      "loss": 0.0897,
      "step": 48050
    },
    {
      "epoch": 1.8006069461616274,
      "grad_norm": 0.08992326259613037,
      "learning_rate": 2.6938612023260923e-06,
      "loss": 0.0873,
      "step": 48060
    },
    {
      "epoch": 1.8009816042860889,
      "grad_norm": 0.1075783297419548,
      "learning_rate": 2.683840571907581e-06,
      "loss": 0.0868,
      "step": 48070
    },
    {
      "epoch": 1.8013562624105504,
      "grad_norm": 0.08511567860841751,
      "learning_rate": 2.6738380995978197e-06,
      "loss": 0.0907,
      "step": 48080
    },
    {
      "epoch": 1.8017309205350118,
      "grad_norm": 0.10646797716617584,
      "learning_rate": 2.663853789235371e-06,
      "loss": 0.0913,
      "step": 48090
    },
    {
      "epoch": 1.8021055786594733,
      "grad_norm": 0.0866287350654602,
      "learning_rate": 2.6538876446518145e-06,
      "loss": 0.087,
      "step": 48100
    },
    {
      "epoch": 1.8024802367839348,
      "grad_norm": 0.10510177910327911,
      "learning_rate": 2.64393966967178e-06,
      "loss": 0.0894,
      "step": 48110
    },
    {
      "epoch": 1.8028548949083962,
      "grad_norm": 0.0765712559223175,
      "learning_rate": 2.634009868112913e-06,
      "loss": 0.0909,
      "step": 48120
    },
    {
      "epoch": 1.8032295530328575,
      "grad_norm": 0.08379282802343369,
      "learning_rate": 2.624098243785894e-06,
      "loss": 0.0904,
      "step": 48130
    },
    {
      "epoch": 1.803604211157319,
      "grad_norm": 0.10100146383047104,
      "learning_rate": 2.614204800494402e-06,
      "loss": 0.0922,
      "step": 48140
    },
    {
      "epoch": 1.8039788692817804,
      "grad_norm": 0.12494032084941864,
      "learning_rate": 2.6043295420351667e-06,
      "loss": 0.0862,
      "step": 48150
    },
    {
      "epoch": 1.8043535274062417,
      "grad_norm": 0.10955896973609924,
      "learning_rate": 2.5944724721979452e-06,
      "loss": 0.0899,
      "step": 48160
    },
    {
      "epoch": 1.8047281855307031,
      "grad_norm": 0.10180287808179855,
      "learning_rate": 2.5846335947654733e-06,
      "loss": 0.0865,
      "step": 48170
    },
    {
      "epoch": 1.8051028436551646,
      "grad_norm": 0.09242914617061615,
      "learning_rate": 2.574812913513536e-06,
      "loss": 0.09,
      "step": 48180
    },
    {
      "epoch": 1.805477501779626,
      "grad_norm": 0.09468388557434082,
      "learning_rate": 2.5650104322109516e-06,
      "loss": 0.0926,
      "step": 48190
    },
    {
      "epoch": 1.8058521599040875,
      "grad_norm": 0.11366916447877884,
      "learning_rate": 2.5552261546195054e-06,
      "loss": 0.0884,
      "step": 48200
    },
    {
      "epoch": 1.806226818028549,
      "grad_norm": 0.0950125977396965,
      "learning_rate": 2.5454600844940392e-06,
      "loss": 0.0875,
      "step": 48210
    },
    {
      "epoch": 1.8066014761530105,
      "grad_norm": 0.09387260675430298,
      "learning_rate": 2.5357122255823927e-06,
      "loss": 0.0864,
      "step": 48220
    },
    {
      "epoch": 1.806976134277472,
      "grad_norm": 0.10923130810260773,
      "learning_rate": 2.525982581625408e-06,
      "loss": 0.0903,
      "step": 48230
    },
    {
      "epoch": 1.8073507924019332,
      "grad_norm": 0.08853069692850113,
      "learning_rate": 2.516271156356953e-06,
      "loss": 0.0894,
      "step": 48240
    },
    {
      "epoch": 1.8077254505263947,
      "grad_norm": 0.12187596410512924,
      "learning_rate": 2.5065779535038968e-06,
      "loss": 0.0858,
      "step": 48250
    },
    {
      "epoch": 1.8081001086508561,
      "grad_norm": 0.08812307566404343,
      "learning_rate": 2.496902976786114e-06,
      "loss": 0.0869,
      "step": 48260
    },
    {
      "epoch": 1.8084747667753174,
      "grad_norm": 0.18951907753944397,
      "learning_rate": 2.4872462299164845e-06,
      "loss": 0.0906,
      "step": 48270
    },
    {
      "epoch": 1.8088494248997788,
      "grad_norm": 0.0779094472527504,
      "learning_rate": 2.477607716600877e-06,
      "loss": 0.0896,
      "step": 48280
    },
    {
      "epoch": 1.8092240830242403,
      "grad_norm": 0.08710754662752151,
      "learning_rate": 2.4679874405382163e-06,
      "loss": 0.0865,
      "step": 48290
    },
    {
      "epoch": 1.8095987411487018,
      "grad_norm": 0.098320871591568,
      "learning_rate": 2.4583854054203604e-06,
      "loss": 0.0882,
      "step": 48300
    },
    {
      "epoch": 1.8099733992731633,
      "grad_norm": 0.09348910301923752,
      "learning_rate": 2.4488016149322113e-06,
      "loss": 0.0895,
      "step": 48310
    },
    {
      "epoch": 1.8103480573976247,
      "grad_norm": 0.1474665105342865,
      "learning_rate": 2.4392360727516438e-06,
      "loss": 0.0889,
      "step": 48320
    },
    {
      "epoch": 1.8107227155220862,
      "grad_norm": 0.1004432961344719,
      "learning_rate": 2.429688782549572e-06,
      "loss": 0.0906,
      "step": 48330
    },
    {
      "epoch": 1.8110973736465477,
      "grad_norm": 0.10130604356527328,
      "learning_rate": 2.4201597479898473e-06,
      "loss": 0.0876,
      "step": 48340
    },
    {
      "epoch": 1.811472031771009,
      "grad_norm": 0.12476179003715515,
      "learning_rate": 2.4106489727293513e-06,
      "loss": 0.0851,
      "step": 48350
    },
    {
      "epoch": 1.8118466898954704,
      "grad_norm": 0.09267134219408035,
      "learning_rate": 2.4011564604179527e-06,
      "loss": 0.0938,
      "step": 48360
    },
    {
      "epoch": 1.8122213480199318,
      "grad_norm": 0.10861486941576004,
      "learning_rate": 2.391682214698515e-06,
      "loss": 0.0914,
      "step": 48370
    },
    {
      "epoch": 1.812596006144393,
      "grad_norm": 0.10066679120063782,
      "learning_rate": 2.382226239206875e-06,
      "loss": 0.0911,
      "step": 48380
    },
    {
      "epoch": 1.8129706642688546,
      "grad_norm": 0.0851510688662529,
      "learning_rate": 2.372788537571885e-06,
      "loss": 0.0885,
      "step": 48390
    },
    {
      "epoch": 1.813345322393316,
      "grad_norm": 0.10953322798013687,
      "learning_rate": 2.363369113415359e-06,
      "loss": 0.0933,
      "step": 48400
    },
    {
      "epoch": 1.8137199805177775,
      "grad_norm": 0.08947794139385223,
      "learning_rate": 2.353967970352111e-06,
      "loss": 0.0906,
      "step": 48410
    },
    {
      "epoch": 1.814094638642239,
      "grad_norm": 0.08046185970306396,
      "learning_rate": 2.344585111989922e-06,
      "loss": 0.0884,
      "step": 48420
    },
    {
      "epoch": 1.8144692967667004,
      "grad_norm": 0.1728217601776123,
      "learning_rate": 2.335220541929595e-06,
      "loss": 0.0884,
      "step": 48430
    },
    {
      "epoch": 1.814843954891162,
      "grad_norm": 0.07876816391944885,
      "learning_rate": 2.3258742637648656e-06,
      "loss": 0.0902,
      "step": 48440
    },
    {
      "epoch": 1.8152186130156234,
      "grad_norm": 0.09761670976877213,
      "learning_rate": 2.3165462810824877e-06,
      "loss": 0.0916,
      "step": 48450
    },
    {
      "epoch": 1.8155932711400846,
      "grad_norm": 0.06982272863388062,
      "learning_rate": 2.30723659746217e-06,
      "loss": 0.0921,
      "step": 48460
    },
    {
      "epoch": 1.815967929264546,
      "grad_norm": 0.08052022010087967,
      "learning_rate": 2.297945216476616e-06,
      "loss": 0.0885,
      "step": 48470
    },
    {
      "epoch": 1.8163425873890076,
      "grad_norm": 0.11711125820875168,
      "learning_rate": 2.2886721416914947e-06,
      "loss": 0.0882,
      "step": 48480
    },
    {
      "epoch": 1.8167172455134688,
      "grad_norm": 0.11637216061353683,
      "learning_rate": 2.279417376665449e-06,
      "loss": 0.0931,
      "step": 48490
    },
    {
      "epoch": 1.8170919036379303,
      "grad_norm": 0.08658799529075623,
      "learning_rate": 2.270180924950116e-06,
      "loss": 0.0848,
      "step": 48500
    },
    {
      "epoch": 1.8174665617623917,
      "grad_norm": 0.1324453353881836,
      "learning_rate": 2.26096279009006e-06,
      "loss": 0.0919,
      "step": 48510
    },
    {
      "epoch": 1.8178412198868532,
      "grad_norm": 0.08722401410341263,
      "learning_rate": 2.251762975622862e-06,
      "loss": 0.0893,
      "step": 48520
    },
    {
      "epoch": 1.8182158780113147,
      "grad_norm": 0.11323914676904678,
      "learning_rate": 2.2425814850790595e-06,
      "loss": 0.0877,
      "step": 48530
    },
    {
      "epoch": 1.8185905361357761,
      "grad_norm": 0.0856134295463562,
      "learning_rate": 2.2334183219821393e-06,
      "loss": 0.0868,
      "step": 48540
    },
    {
      "epoch": 1.8189651942602376,
      "grad_norm": 0.10827900469303131,
      "learning_rate": 2.2242734898485774e-06,
      "loss": 0.0899,
      "step": 48550
    },
    {
      "epoch": 1.819339852384699,
      "grad_norm": 0.0946144387125969,
      "learning_rate": 2.2151469921877944e-06,
      "loss": 0.0849,
      "step": 48560
    },
    {
      "epoch": 1.8197145105091606,
      "grad_norm": 0.08581945300102234,
      "learning_rate": 2.2060388325022054e-06,
      "loss": 0.0887,
      "step": 48570
    },
    {
      "epoch": 1.8200891686336218,
      "grad_norm": 0.09959088265895844,
      "learning_rate": 2.196949014287153e-06,
      "loss": 0.0915,
      "step": 48580
    },
    {
      "epoch": 1.8204638267580833,
      "grad_norm": 0.09449920058250427,
      "learning_rate": 2.187877541030958e-06,
      "loss": 0.093,
      "step": 48590
    },
    {
      "epoch": 1.8208384848825445,
      "grad_norm": 0.08875297009944916,
      "learning_rate": 2.178824416214914e-06,
      "loss": 0.0874,
      "step": 48600
    },
    {
      "epoch": 1.821213143007006,
      "grad_norm": 0.08811061084270477,
      "learning_rate": 2.169789643313247e-06,
      "loss": 0.0898,
      "step": 48610
    },
    {
      "epoch": 1.8215878011314675,
      "grad_norm": 0.10237693786621094,
      "learning_rate": 2.1607732257931556e-06,
      "loss": 0.0876,
      "step": 48620
    },
    {
      "epoch": 1.821962459255929,
      "grad_norm": 0.07357499748468399,
      "learning_rate": 2.151775167114789e-06,
      "loss": 0.0879,
      "step": 48630
    },
    {
      "epoch": 1.8223371173803904,
      "grad_norm": 0.08666959404945374,
      "learning_rate": 2.142795470731268e-06,
      "loss": 0.0877,
      "step": 48640
    },
    {
      "epoch": 1.8227117755048519,
      "grad_norm": 0.1032358780503273,
      "learning_rate": 2.1338341400886255e-06,
      "loss": 0.088,
      "step": 48650
    },
    {
      "epoch": 1.8230864336293133,
      "grad_norm": 0.08398667722940445,
      "learning_rate": 2.1248911786258986e-06,
      "loss": 0.0891,
      "step": 48660
    },
    {
      "epoch": 1.8234610917537748,
      "grad_norm": 0.08712917566299438,
      "learning_rate": 2.1159665897750423e-06,
      "loss": 0.0901,
      "step": 48670
    },
    {
      "epoch": 1.8238357498782363,
      "grad_norm": 0.14268502593040466,
      "learning_rate": 2.1070603769609622e-06,
      "loss": 0.0897,
      "step": 48680
    },
    {
      "epoch": 1.8242104080026975,
      "grad_norm": 0.11067572236061096,
      "learning_rate": 2.0981725436015176e-06,
      "loss": 0.0889,
      "step": 48690
    },
    {
      "epoch": 1.824585066127159,
      "grad_norm": 0.08359027653932571,
      "learning_rate": 2.089303093107514e-06,
      "loss": 0.0863,
      "step": 48700
    },
    {
      "epoch": 1.8249597242516202,
      "grad_norm": 0.12452830374240875,
      "learning_rate": 2.0804520288827124e-06,
      "loss": 0.092,
      "step": 48710
    },
    {
      "epoch": 1.8253343823760817,
      "grad_norm": 0.07187039405107498,
      "learning_rate": 2.0716193543238007e-06,
      "loss": 0.0911,
      "step": 48720
    },
    {
      "epoch": 1.8257090405005432,
      "grad_norm": 0.1038702130317688,
      "learning_rate": 2.062805072820417e-06,
      "loss": 0.0893,
      "step": 48730
    },
    {
      "epoch": 1.8260836986250046,
      "grad_norm": 0.1253400444984436,
      "learning_rate": 2.054009187755146e-06,
      "loss": 0.0897,
      "step": 48740
    },
    {
      "epoch": 1.826458356749466,
      "grad_norm": 0.0916571319103241,
      "learning_rate": 2.045231702503503e-06,
      "loss": 0.0889,
      "step": 48750
    },
    {
      "epoch": 1.8268330148739276,
      "grad_norm": 0.084450863301754,
      "learning_rate": 2.036472620433949e-06,
      "loss": 0.088,
      "step": 48760
    },
    {
      "epoch": 1.827207672998389,
      "grad_norm": 0.11709509044885635,
      "learning_rate": 2.0277319449078846e-06,
      "loss": 0.0893,
      "step": 48770
    },
    {
      "epoch": 1.8275823311228505,
      "grad_norm": 0.11851169914007187,
      "learning_rate": 2.019009679279632e-06,
      "loss": 0.091,
      "step": 48780
    },
    {
      "epoch": 1.827956989247312,
      "grad_norm": 0.08596430718898773,
      "learning_rate": 2.0103058268964525e-06,
      "loss": 0.0911,
      "step": 48790
    },
    {
      "epoch": 1.8283316473717732,
      "grad_norm": 0.09127996116876602,
      "learning_rate": 2.0016203910985743e-06,
      "loss": 0.0882,
      "step": 48800
    },
    {
      "epoch": 1.8287063054962347,
      "grad_norm": 0.08588463813066483,
      "learning_rate": 1.9929533752191145e-06,
      "loss": 0.09,
      "step": 48810
    },
    {
      "epoch": 1.8290809636206962,
      "grad_norm": 0.08968133479356766,
      "learning_rate": 1.9843047825841342e-06,
      "loss": 0.089,
      "step": 48820
    },
    {
      "epoch": 1.8294556217451574,
      "grad_norm": 0.11921261250972748,
      "learning_rate": 1.9756746165126292e-06,
      "loss": 0.0912,
      "step": 48830
    },
    {
      "epoch": 1.8298302798696189,
      "grad_norm": 0.0847911462187767,
      "learning_rate": 1.967062880316545e-06,
      "loss": 0.0862,
      "step": 48840
    },
    {
      "epoch": 1.8302049379940803,
      "grad_norm": 0.10271763801574707,
      "learning_rate": 1.958469577300698e-06,
      "loss": 0.0856,
      "step": 48850
    },
    {
      "epoch": 1.8305795961185418,
      "grad_norm": 0.13773739337921143,
      "learning_rate": 1.9498947107628894e-06,
      "loss": 0.0919,
      "step": 48860
    },
    {
      "epoch": 1.8309542542430033,
      "grad_norm": 0.0826941728591919,
      "learning_rate": 1.94133828399381e-06,
      "loss": 0.0868,
      "step": 48870
    },
    {
      "epoch": 1.8313289123674648,
      "grad_norm": 0.11333724111318588,
      "learning_rate": 1.9328003002770824e-06,
      "loss": 0.0919,
      "step": 48880
    },
    {
      "epoch": 1.8317035704919262,
      "grad_norm": 0.09012894332408905,
      "learning_rate": 1.9242807628892633e-06,
      "loss": 0.0909,
      "step": 48890
    },
    {
      "epoch": 1.8320782286163877,
      "grad_norm": 0.11137912422418594,
      "learning_rate": 1.9157796750998156e-06,
      "loss": 0.092,
      "step": 48900
    },
    {
      "epoch": 1.832452886740849,
      "grad_norm": 0.08887340873479843,
      "learning_rate": 1.9072970401711409e-06,
      "loss": 0.0871,
      "step": 48910
    },
    {
      "epoch": 1.8328275448653104,
      "grad_norm": 0.10863006860017776,
      "learning_rate": 1.8988328613585181e-06,
      "loss": 0.093,
      "step": 48920
    },
    {
      "epoch": 1.8332022029897719,
      "grad_norm": 0.10916201770305634,
      "learning_rate": 1.8903871419101881e-06,
      "loss": 0.0874,
      "step": 48930
    },
    {
      "epoch": 1.8335768611142331,
      "grad_norm": 0.07925710827112198,
      "learning_rate": 1.881959885067297e-06,
      "loss": 0.0886,
      "step": 48940
    },
    {
      "epoch": 1.8339515192386946,
      "grad_norm": 0.09108205139636993,
      "learning_rate": 1.8735510940638855e-06,
      "loss": 0.0892,
      "step": 48950
    },
    {
      "epoch": 1.834326177363156,
      "grad_norm": 0.12627604603767395,
      "learning_rate": 1.8651607721269338e-06,
      "loss": 0.0893,
      "step": 48960
    },
    {
      "epoch": 1.8347008354876175,
      "grad_norm": 0.10371799767017365,
      "learning_rate": 1.8567889224763101e-06,
      "loss": 0.0905,
      "step": 48970
    },
    {
      "epoch": 1.835075493612079,
      "grad_norm": 0.08203744888305664,
      "learning_rate": 1.8484355483248117e-06,
      "loss": 0.0879,
      "step": 48980
    },
    {
      "epoch": 1.8354501517365405,
      "grad_norm": 0.09839721769094467,
      "learning_rate": 1.8401006528781462e-06,
      "loss": 0.0902,
      "step": 48990
    },
    {
      "epoch": 1.835824809861002,
      "grad_norm": 0.09602488577365875,
      "learning_rate": 1.831784239334916e-06,
      "loss": 0.0889,
      "step": 49000
    },
    {
      "epoch": 1.8361994679854634,
      "grad_norm": 0.09492215514183044,
      "learning_rate": 1.8234863108866462e-06,
      "loss": 0.0898,
      "step": 49010
    },
    {
      "epoch": 1.8365741261099247,
      "grad_norm": 0.08817087113857269,
      "learning_rate": 1.8152068707177394e-06,
      "loss": 0.0866,
      "step": 49020
    },
    {
      "epoch": 1.8369487842343861,
      "grad_norm": 0.08397002518177032,
      "learning_rate": 1.8069459220055484e-06,
      "loss": 0.0898,
      "step": 49030
    },
    {
      "epoch": 1.8373234423588476,
      "grad_norm": 0.09085017442703247,
      "learning_rate": 1.798703467920293e-06,
      "loss": 0.0868,
      "step": 49040
    },
    {
      "epoch": 1.8376981004833088,
      "grad_norm": 0.09957228600978851,
      "learning_rate": 1.7904795116251204e-06,
      "loss": 0.0894,
      "step": 49050
    },
    {
      "epoch": 1.8380727586077703,
      "grad_norm": 0.08526908606290817,
      "learning_rate": 1.782274056276051e-06,
      "loss": 0.0888,
      "step": 49060
    },
    {
      "epoch": 1.8384474167322318,
      "grad_norm": 0.09513819217681885,
      "learning_rate": 1.7740871050220155e-06,
      "loss": 0.0857,
      "step": 49070
    },
    {
      "epoch": 1.8388220748566932,
      "grad_norm": 0.08138539642095566,
      "learning_rate": 1.7659186610048727e-06,
      "loss": 0.0859,
      "step": 49080
    },
    {
      "epoch": 1.8391967329811547,
      "grad_norm": 0.09321682155132294,
      "learning_rate": 1.757768727359338e-06,
      "loss": 0.0879,
      "step": 49090
    },
    {
      "epoch": 1.8395713911056162,
      "grad_norm": 0.06821495294570923,
      "learning_rate": 1.7496373072130424e-06,
      "loss": 0.0895,
      "step": 49100
    },
    {
      "epoch": 1.8399460492300777,
      "grad_norm": 0.1162412092089653,
      "learning_rate": 1.741524403686512e-06,
      "loss": 0.0871,
      "step": 49110
    },
    {
      "epoch": 1.8403207073545391,
      "grad_norm": 0.10176704078912735,
      "learning_rate": 1.7334300198931674e-06,
      "loss": 0.0861,
      "step": 49120
    },
    {
      "epoch": 1.8406953654790004,
      "grad_norm": 0.10643524676561356,
      "learning_rate": 1.7253541589393074e-06,
      "loss": 0.0906,
      "step": 49130
    },
    {
      "epoch": 1.8410700236034618,
      "grad_norm": 0.09823483973741531,
      "learning_rate": 1.7172968239241526e-06,
      "loss": 0.0897,
      "step": 49140
    },
    {
      "epoch": 1.8414446817279233,
      "grad_norm": 0.09913403540849686,
      "learning_rate": 1.7092580179397855e-06,
      "loss": 0.0909,
      "step": 49150
    },
    {
      "epoch": 1.8418193398523846,
      "grad_norm": 0.07761040329933167,
      "learning_rate": 1.7012377440711823e-06,
      "loss": 0.0884,
      "step": 49160
    },
    {
      "epoch": 1.842193997976846,
      "grad_norm": 0.0900128036737442,
      "learning_rate": 1.693236005396226e-06,
      "loss": 0.0882,
      "step": 49170
    },
    {
      "epoch": 1.8425686561013075,
      "grad_norm": 0.1168501079082489,
      "learning_rate": 1.685252804985671e-06,
      "loss": 0.0892,
      "step": 49180
    },
    {
      "epoch": 1.842943314225769,
      "grad_norm": 0.10084951668977737,
      "learning_rate": 1.6772881459031553e-06,
      "loss": 0.0903,
      "step": 49190
    },
    {
      "epoch": 1.8433179723502304,
      "grad_norm": 0.09574808925390244,
      "learning_rate": 1.6693420312052066e-06,
      "loss": 0.0928,
      "step": 49200
    },
    {
      "epoch": 1.843692630474692,
      "grad_norm": 0.16426271200180054,
      "learning_rate": 1.661414463941241e-06,
      "loss": 0.0905,
      "step": 49210
    },
    {
      "epoch": 1.8440672885991534,
      "grad_norm": 0.09057767689228058,
      "learning_rate": 1.6535054471535473e-06,
      "loss": 0.0882,
      "step": 49220
    },
    {
      "epoch": 1.8444419467236148,
      "grad_norm": 0.09507646411657333,
      "learning_rate": 1.6456149838772972e-06,
      "loss": 0.0894,
      "step": 49230
    },
    {
      "epoch": 1.844816604848076,
      "grad_norm": 0.09298834949731827,
      "learning_rate": 1.6377430771405578e-06,
      "loss": 0.0896,
      "step": 49240
    },
    {
      "epoch": 1.8451912629725375,
      "grad_norm": 0.07333352416753769,
      "learning_rate": 1.6298897299642458e-06,
      "loss": 0.0921,
      "step": 49250
    },
    {
      "epoch": 1.845565921096999,
      "grad_norm": 0.08854031562805176,
      "learning_rate": 1.6220549453621837e-06,
      "loss": 0.0886,
      "step": 49260
    },
    {
      "epoch": 1.8459405792214603,
      "grad_norm": 0.0839991569519043,
      "learning_rate": 1.6142387263410496e-06,
      "loss": 0.0853,
      "step": 49270
    },
    {
      "epoch": 1.8463152373459217,
      "grad_norm": 0.1364804059267044,
      "learning_rate": 1.606441075900411e-06,
      "loss": 0.0915,
      "step": 49280
    },
    {
      "epoch": 1.8466898954703832,
      "grad_norm": 0.07688961178064346,
      "learning_rate": 1.5986619970327077e-06,
      "loss": 0.0862,
      "step": 49290
    },
    {
      "epoch": 1.8470645535948447,
      "grad_norm": 0.11819881945848465,
      "learning_rate": 1.5909014927232291e-06,
      "loss": 0.0956,
      "step": 49300
    },
    {
      "epoch": 1.8474392117193061,
      "grad_norm": 0.08972767740488052,
      "learning_rate": 1.583159565950182e-06,
      "loss": 0.0893,
      "step": 49310
    },
    {
      "epoch": 1.8478138698437676,
      "grad_norm": 0.09216819703578949,
      "learning_rate": 1.575436219684606e-06,
      "loss": 0.0904,
      "step": 49320
    },
    {
      "epoch": 1.848188527968229,
      "grad_norm": 0.09593460708856583,
      "learning_rate": 1.56773145689042e-06,
      "loss": 0.093,
      "step": 49330
    },
    {
      "epoch": 1.8485631860926905,
      "grad_norm": 0.0837760791182518,
      "learning_rate": 1.5600452805244193e-06,
      "loss": 0.0869,
      "step": 49340
    },
    {
      "epoch": 1.8489378442171518,
      "grad_norm": 0.07127374410629272,
      "learning_rate": 1.5523776935362555e-06,
      "loss": 0.09,
      "step": 49350
    },
    {
      "epoch": 1.8493125023416133,
      "grad_norm": 0.09389468282461166,
      "learning_rate": 1.5447286988684473e-06,
      "loss": 0.0879,
      "step": 49360
    },
    {
      "epoch": 1.8496871604660747,
      "grad_norm": 0.09954804927110672,
      "learning_rate": 1.5370982994563965e-06,
      "loss": 0.0902,
      "step": 49370
    },
    {
      "epoch": 1.850061818590536,
      "grad_norm": 0.09025193750858307,
      "learning_rate": 1.5294864982283386e-06,
      "loss": 0.0875,
      "step": 49380
    },
    {
      "epoch": 1.8504364767149974,
      "grad_norm": 0.09729383140802383,
      "learning_rate": 1.5218932981054035e-06,
      "loss": 0.0883,
      "step": 49390
    },
    {
      "epoch": 1.850811134839459,
      "grad_norm": 0.07652891427278519,
      "learning_rate": 1.5143187020015549e-06,
      "loss": 0.0891,
      "step": 49400
    },
    {
      "epoch": 1.8511857929639204,
      "grad_norm": 0.09410272538661957,
      "learning_rate": 1.5067627128236338e-06,
      "loss": 0.0855,
      "step": 49410
    },
    {
      "epoch": 1.8515604510883819,
      "grad_norm": 0.07794934511184692,
      "learning_rate": 1.4992253334713436e-06,
      "loss": 0.0897,
      "step": 49420
    },
    {
      "epoch": 1.8519351092128433,
      "grad_norm": 0.08393314480781555,
      "learning_rate": 1.4917065668372255e-06,
      "loss": 0.0916,
      "step": 49430
    },
    {
      "epoch": 1.8523097673373048,
      "grad_norm": 0.11495533585548401,
      "learning_rate": 1.4842064158066936e-06,
      "loss": 0.0877,
      "step": 49440
    },
    {
      "epoch": 1.8526844254617663,
      "grad_norm": 0.08423098921775818,
      "learning_rate": 1.4767248832580182e-06,
      "loss": 0.0906,
      "step": 49450
    },
    {
      "epoch": 1.8530590835862277,
      "grad_norm": 0.08630818873643875,
      "learning_rate": 1.4692619720623302e-06,
      "loss": 0.0893,
      "step": 49460
    },
    {
      "epoch": 1.853433741710689,
      "grad_norm": 0.10782907158136368,
      "learning_rate": 1.4618176850835941e-06,
      "loss": 0.0916,
      "step": 49470
    },
    {
      "epoch": 1.8538083998351504,
      "grad_norm": 0.10383797436952591,
      "learning_rate": 1.4543920251786413e-06,
      "loss": 0.0894,
      "step": 49480
    },
    {
      "epoch": 1.8541830579596117,
      "grad_norm": 0.0978984609246254,
      "learning_rate": 1.4469849951971482e-06,
      "loss": 0.0895,
      "step": 49490
    },
    {
      "epoch": 1.8545577160840732,
      "grad_norm": 0.09030115604400635,
      "learning_rate": 1.4395965979816572e-06,
      "loss": 0.0901,
      "step": 49500
    },
    {
      "epoch": 1.8549323742085346,
      "grad_norm": 0.07429682463407516,
      "learning_rate": 1.4322268363675394e-06,
      "loss": 0.0885,
      "step": 49510
    },
    {
      "epoch": 1.855307032332996,
      "grad_norm": 0.08641448616981506,
      "learning_rate": 1.424875713183027e-06,
      "loss": 0.0886,
      "step": 49520
    },
    {
      "epoch": 1.8556816904574576,
      "grad_norm": 0.11964316666126251,
      "learning_rate": 1.4175432312491965e-06,
      "loss": 0.0905,
      "step": 49530
    },
    {
      "epoch": 1.856056348581919,
      "grad_norm": 0.0817989632487297,
      "learning_rate": 1.4102293933799693e-06,
      "loss": 0.092,
      "step": 49540
    },
    {
      "epoch": 1.8564310067063805,
      "grad_norm": 0.13984419405460358,
      "learning_rate": 1.4029342023821113e-06,
      "loss": 0.0924,
      "step": 49550
    },
    {
      "epoch": 1.856805664830842,
      "grad_norm": 0.07499640434980392,
      "learning_rate": 1.395657661055244e-06,
      "loss": 0.0887,
      "step": 49560
    },
    {
      "epoch": 1.8571803229553034,
      "grad_norm": 0.08995556086301804,
      "learning_rate": 1.3883997721918118e-06,
      "loss": 0.091,
      "step": 49570
    },
    {
      "epoch": 1.8575549810797647,
      "grad_norm": 0.09225167334079742,
      "learning_rate": 1.3811605385771032e-06,
      "loss": 0.0905,
      "step": 49580
    },
    {
      "epoch": 1.8579296392042262,
      "grad_norm": 0.10710101574659348,
      "learning_rate": 1.3739399629892847e-06,
      "loss": 0.0929,
      "step": 49590
    },
    {
      "epoch": 1.8583042973286876,
      "grad_norm": 0.1087961196899414,
      "learning_rate": 1.3667380481993009e-06,
      "loss": 0.089,
      "step": 49600
    },
    {
      "epoch": 1.8586789554531489,
      "grad_norm": 0.08850352466106415,
      "learning_rate": 1.3595547969709854e-06,
      "loss": 0.088,
      "step": 49610
    },
    {
      "epoch": 1.8590536135776103,
      "grad_norm": 0.07188531756401062,
      "learning_rate": 1.3523902120609944e-06,
      "loss": 0.0861,
      "step": 49620
    },
    {
      "epoch": 1.8594282717020718,
      "grad_norm": 0.08161387592554092,
      "learning_rate": 1.3452442962188062e-06,
      "loss": 0.088,
      "step": 49630
    },
    {
      "epoch": 1.8598029298265333,
      "grad_norm": 0.0924200788140297,
      "learning_rate": 1.3381170521867491e-06,
      "loss": 0.0935,
      "step": 49640
    },
    {
      "epoch": 1.8601775879509947,
      "grad_norm": 0.0909084677696228,
      "learning_rate": 1.3310084826999913e-06,
      "loss": 0.089,
      "step": 49650
    },
    {
      "epoch": 1.8605522460754562,
      "grad_norm": 0.09053511917591095,
      "learning_rate": 1.3239185904865226e-06,
      "loss": 0.0924,
      "step": 49660
    },
    {
      "epoch": 1.8609269041999177,
      "grad_norm": 0.08298235386610031,
      "learning_rate": 1.316847378267161e-06,
      "loss": 0.0874,
      "step": 49670
    },
    {
      "epoch": 1.8613015623243792,
      "grad_norm": 0.10261934250593185,
      "learning_rate": 1.3097948487555744e-06,
      "loss": 0.0896,
      "step": 49680
    },
    {
      "epoch": 1.8616762204488404,
      "grad_norm": 0.18399468064308167,
      "learning_rate": 1.302761004658243e-06,
      "loss": 0.0899,
      "step": 49690
    },
    {
      "epoch": 1.8620508785733019,
      "grad_norm": 0.10303803533315659,
      "learning_rate": 1.2957458486744955e-06,
      "loss": 0.0897,
      "step": 49700
    },
    {
      "epoch": 1.8624255366977633,
      "grad_norm": 0.08354229480028152,
      "learning_rate": 1.2887493834964627e-06,
      "loss": 0.0907,
      "step": 49710
    },
    {
      "epoch": 1.8628001948222246,
      "grad_norm": 0.08408110588788986,
      "learning_rate": 1.2817716118091183e-06,
      "loss": 0.0909,
      "step": 49720
    },
    {
      "epoch": 1.863174852946686,
      "grad_norm": 0.11613834649324417,
      "learning_rate": 1.2748125362902708e-06,
      "loss": 0.0935,
      "step": 49730
    },
    {
      "epoch": 1.8635495110711475,
      "grad_norm": 0.09454792737960815,
      "learning_rate": 1.267872159610539e-06,
      "loss": 0.0933,
      "step": 49740
    },
    {
      "epoch": 1.863924169195609,
      "grad_norm": 0.07346329838037491,
      "learning_rate": 1.2609504844333652e-06,
      "loss": 0.0876,
      "step": 49750
    },
    {
      "epoch": 1.8642988273200705,
      "grad_norm": 0.10204312205314636,
      "learning_rate": 1.2540475134150242e-06,
      "loss": 0.0902,
      "step": 49760
    },
    {
      "epoch": 1.864673485444532,
      "grad_norm": 0.07273460924625397,
      "learning_rate": 1.2471632492046136e-06,
      "loss": 0.0883,
      "step": 49770
    },
    {
      "epoch": 1.8650481435689934,
      "grad_norm": 0.12944500148296356,
      "learning_rate": 1.240297694444037e-06,
      "loss": 0.0888,
      "step": 49780
    },
    {
      "epoch": 1.8654228016934549,
      "grad_norm": 0.09098092466592789,
      "learning_rate": 1.233450851768042e-06,
      "loss": 0.0902,
      "step": 49790
    },
    {
      "epoch": 1.8657974598179161,
      "grad_norm": 0.10377024114131927,
      "learning_rate": 1.2266227238041716e-06,
      "loss": 0.0917,
      "step": 49800
    },
    {
      "epoch": 1.8661721179423776,
      "grad_norm": 0.12232506275177002,
      "learning_rate": 1.2198133131727908e-06,
      "loss": 0.0889,
      "step": 49810
    },
    {
      "epoch": 1.866546776066839,
      "grad_norm": 0.08192024379968643,
      "learning_rate": 1.2130226224871032e-06,
      "loss": 0.0922,
      "step": 49820
    },
    {
      "epoch": 1.8669214341913003,
      "grad_norm": 0.10413564741611481,
      "learning_rate": 1.2062506543531028e-06,
      "loss": 0.0895,
      "step": 49830
    },
    {
      "epoch": 1.8672960923157618,
      "grad_norm": 0.06420115381479263,
      "learning_rate": 1.1994974113696155e-06,
      "loss": 0.0932,
      "step": 49840
    },
    {
      "epoch": 1.8676707504402232,
      "grad_norm": 0.08282867819070816,
      "learning_rate": 1.1927628961282634e-06,
      "loss": 0.0882,
      "step": 49850
    },
    {
      "epoch": 1.8680454085646847,
      "grad_norm": 0.08990131318569183,
      "learning_rate": 1.1860471112135009e-06,
      "loss": 0.089,
      "step": 49860
    },
    {
      "epoch": 1.8684200666891462,
      "grad_norm": 0.0890229120850563,
      "learning_rate": 1.179350059202583e-06,
      "loss": 0.0898,
      "step": 49870
    },
    {
      "epoch": 1.8687947248136076,
      "grad_norm": 0.1008019894361496,
      "learning_rate": 1.172671742665582e-06,
      "loss": 0.0881,
      "step": 49880
    },
    {
      "epoch": 1.8691693829380691,
      "grad_norm": 0.07190409302711487,
      "learning_rate": 1.1660121641653699e-06,
      "loss": 0.0891,
      "step": 49890
    },
    {
      "epoch": 1.8695440410625306,
      "grad_norm": 0.1107495129108429,
      "learning_rate": 1.1593713262576467e-06,
      "loss": 0.0906,
      "step": 49900
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 0.09279945492744446,
      "learning_rate": 1.152749231490896e-06,
      "loss": 0.0899,
      "step": 49910
    },
    {
      "epoch": 1.8702933573114533,
      "grad_norm": 0.06886489689350128,
      "learning_rate": 1.1461458824064298e-06,
      "loss": 0.0883,
      "step": 49920
    },
    {
      "epoch": 1.8706680154359148,
      "grad_norm": 0.10263235121965408,
      "learning_rate": 1.139561281538354e-06,
      "loss": 0.0897,
      "step": 49930
    },
    {
      "epoch": 1.871042673560376,
      "grad_norm": 0.08327250927686691,
      "learning_rate": 1.1329954314135916e-06,
      "loss": 0.0883,
      "step": 49940
    },
    {
      "epoch": 1.8714173316848375,
      "grad_norm": 0.09185252338647842,
      "learning_rate": 1.126448334551844e-06,
      "loss": 0.0877,
      "step": 49950
    },
    {
      "epoch": 1.871791989809299,
      "grad_norm": 0.08883562684059143,
      "learning_rate": 1.1199199934656456e-06,
      "loss": 0.0903,
      "step": 49960
    },
    {
      "epoch": 1.8721666479337604,
      "grad_norm": 0.08343131840229034,
      "learning_rate": 1.1134104106603205e-06,
      "loss": 0.087,
      "step": 49970
    },
    {
      "epoch": 1.8725413060582219,
      "grad_norm": 0.09706086665391922,
      "learning_rate": 1.1069195886339924e-06,
      "loss": 0.0906,
      "step": 49980
    },
    {
      "epoch": 1.8729159641826834,
      "grad_norm": 0.09124325960874557,
      "learning_rate": 1.1004475298775852e-06,
      "loss": 0.0873,
      "step": 49990
    },
    {
      "epoch": 1.8732906223071448,
      "grad_norm": 0.09193524718284607,
      "learning_rate": 1.093994236874829e-06,
      "loss": 0.0882,
      "step": 50000
    },
    {
      "epoch": 1.8736652804316063,
      "grad_norm": 0.0907001793384552,
      "learning_rate": 1.087559712102243e-06,
      "loss": 0.087,
      "step": 50010
    },
    {
      "epoch": 1.8740399385560675,
      "grad_norm": 0.08850307762622833,
      "learning_rate": 1.0811439580291516e-06,
      "loss": 0.0886,
      "step": 50020
    },
    {
      "epoch": 1.874414596680529,
      "grad_norm": 0.09717050194740295,
      "learning_rate": 1.0747469771176744e-06,
      "loss": 0.0898,
      "step": 50030
    },
    {
      "epoch": 1.8747892548049905,
      "grad_norm": 0.08594846725463867,
      "learning_rate": 1.0683687718227198e-06,
      "loss": 0.0888,
      "step": 50040
    },
    {
      "epoch": 1.8751639129294517,
      "grad_norm": 0.12078165262937546,
      "learning_rate": 1.0620093445919966e-06,
      "loss": 0.0864,
      "step": 50050
    },
    {
      "epoch": 1.8755385710539132,
      "grad_norm": 0.12210015952587128,
      "learning_rate": 1.055668697866008e-06,
      "loss": 0.0921,
      "step": 50060
    },
    {
      "epoch": 1.8759132291783747,
      "grad_norm": 0.07564523816108704,
      "learning_rate": 1.049346834078052e-06,
      "loss": 0.0903,
      "step": 50070
    },
    {
      "epoch": 1.8762878873028361,
      "grad_norm": 0.10427114367485046,
      "learning_rate": 1.0430437556542106e-06,
      "loss": 0.0895,
      "step": 50080
    },
    {
      "epoch": 1.8766625454272976,
      "grad_norm": 0.09669709950685501,
      "learning_rate": 1.0367594650133539e-06,
      "loss": 0.0903,
      "step": 50090
    },
    {
      "epoch": 1.877037203551759,
      "grad_norm": 0.13268445432186127,
      "learning_rate": 1.0304939645671697e-06,
      "loss": 0.0914,
      "step": 50100
    },
    {
      "epoch": 1.8774118616762205,
      "grad_norm": 0.08744855970144272,
      "learning_rate": 1.0242472567200955e-06,
      "loss": 0.0874,
      "step": 50110
    },
    {
      "epoch": 1.877786519800682,
      "grad_norm": 0.09953465312719345,
      "learning_rate": 1.0180193438693809e-06,
      "loss": 0.0883,
      "step": 50120
    },
    {
      "epoch": 1.8781611779251433,
      "grad_norm": 0.08697853982448578,
      "learning_rate": 1.0118102284050635e-06,
      "loss": 0.0876,
      "step": 50130
    },
    {
      "epoch": 1.8785358360496047,
      "grad_norm": 0.11143667250871658,
      "learning_rate": 1.0056199127099542e-06,
      "loss": 0.0864,
      "step": 50140
    },
    {
      "epoch": 1.8789104941740662,
      "grad_norm": 0.10399828851222992,
      "learning_rate": 9.994483991596581e-07,
      "loss": 0.0929,
      "step": 50150
    },
    {
      "epoch": 1.8792851522985274,
      "grad_norm": 0.11191526055335999,
      "learning_rate": 9.932956901225643e-07,
      "loss": 0.0874,
      "step": 50160
    },
    {
      "epoch": 1.879659810422989,
      "grad_norm": 0.08814311772584915,
      "learning_rate": 9.871617879598448e-07,
      "loss": 0.0911,
      "step": 50170
    },
    {
      "epoch": 1.8800344685474504,
      "grad_norm": 0.09330576658248901,
      "learning_rate": 9.810466950254505e-07,
      "loss": 0.0897,
      "step": 50180
    },
    {
      "epoch": 1.8804091266719118,
      "grad_norm": 0.08707768470048904,
      "learning_rate": 9.749504136661203e-07,
      "loss": 0.0854,
      "step": 50190
    },
    {
      "epoch": 1.8807837847963733,
      "grad_norm": 0.09225139021873474,
      "learning_rate": 9.688729462213664e-07,
      "loss": 0.0954,
      "step": 50200
    },
    {
      "epoch": 1.8811584429208348,
      "grad_norm": 0.15656164288520813,
      "learning_rate": 9.628142950234952e-07,
      "loss": 0.0894,
      "step": 50210
    },
    {
      "epoch": 1.8815331010452963,
      "grad_norm": 0.10857614874839783,
      "learning_rate": 9.567744623975693e-07,
      "loss": 0.0898,
      "step": 50220
    },
    {
      "epoch": 1.8819077591697577,
      "grad_norm": 0.09179241955280304,
      "learning_rate": 9.507534506614457e-07,
      "loss": 0.0897,
      "step": 50230
    },
    {
      "epoch": 1.8822824172942192,
      "grad_norm": 0.10128957778215408,
      "learning_rate": 9.447512621257648e-07,
      "loss": 0.0909,
      "step": 50240
    },
    {
      "epoch": 1.8826570754186804,
      "grad_norm": 0.10790256410837173,
      "learning_rate": 9.387678990939286e-07,
      "loss": 0.0886,
      "step": 50250
    },
    {
      "epoch": 1.883031733543142,
      "grad_norm": 0.13083770871162415,
      "learning_rate": 9.328033638621114e-07,
      "loss": 0.0898,
      "step": 50260
    },
    {
      "epoch": 1.8834063916676032,
      "grad_norm": 0.09607961773872375,
      "learning_rate": 9.268576587192823e-07,
      "loss": 0.0877,
      "step": 50270
    },
    {
      "epoch": 1.8837810497920646,
      "grad_norm": 0.10955622792243958,
      "learning_rate": 9.209307859471716e-07,
      "loss": 0.0896,
      "step": 50280
    },
    {
      "epoch": 1.884155707916526,
      "grad_norm": 0.17777515947818756,
      "learning_rate": 9.150227478202766e-07,
      "loss": 0.0867,
      "step": 50290
    },
    {
      "epoch": 1.8845303660409876,
      "grad_norm": 0.08304791152477264,
      "learning_rate": 9.091335466058782e-07,
      "loss": 0.0895,
      "step": 50300
    },
    {
      "epoch": 1.884905024165449,
      "grad_norm": 0.08397332578897476,
      "learning_rate": 9.032631845640238e-07,
      "loss": 0.0923,
      "step": 50310
    },
    {
      "epoch": 1.8852796822899105,
      "grad_norm": 0.14464198052883148,
      "learning_rate": 8.974116639475228e-07,
      "loss": 0.0878,
      "step": 50320
    },
    {
      "epoch": 1.885654340414372,
      "grad_norm": 0.10166986286640167,
      "learning_rate": 8.915789870019786e-07,
      "loss": 0.0858,
      "step": 50330
    },
    {
      "epoch": 1.8860289985388334,
      "grad_norm": 0.08924748003482819,
      "learning_rate": 8.857651559657343e-07,
      "loss": 0.0907,
      "step": 50340
    },
    {
      "epoch": 1.886403656663295,
      "grad_norm": 0.09852468222379684,
      "learning_rate": 8.799701730699162e-07,
      "loss": 0.0892,
      "step": 50350
    },
    {
      "epoch": 1.8867783147877562,
      "grad_norm": 0.07416611909866333,
      "learning_rate": 8.74194040538412e-07,
      "loss": 0.0892,
      "step": 50360
    },
    {
      "epoch": 1.8871529729122176,
      "grad_norm": 0.09711563587188721,
      "learning_rate": 8.684367605878763e-07,
      "loss": 0.0904,
      "step": 50370
    },
    {
      "epoch": 1.8875276310366789,
      "grad_norm": 0.1017746552824974,
      "learning_rate": 8.626983354277363e-07,
      "loss": 0.0934,
      "step": 50380
    },
    {
      "epoch": 1.8879022891611403,
      "grad_norm": 0.0861753597855568,
      "learning_rate": 8.569787672601748e-07,
      "loss": 0.0872,
      "step": 50390
    },
    {
      "epoch": 1.8882769472856018,
      "grad_norm": 0.09358279407024384,
      "learning_rate": 8.512780582801416e-07,
      "loss": 0.0939,
      "step": 50400
    },
    {
      "epoch": 1.8886516054100633,
      "grad_norm": 0.1422179788351059,
      "learning_rate": 8.455962106753424e-07,
      "loss": 0.0888,
      "step": 50410
    },
    {
      "epoch": 1.8890262635345247,
      "grad_norm": 0.08475176990032196,
      "learning_rate": 8.399332266262549e-07,
      "loss": 0.0889,
      "step": 50420
    },
    {
      "epoch": 1.8894009216589862,
      "grad_norm": 0.10110215842723846,
      "learning_rate": 8.342891083061132e-07,
      "loss": 0.0909,
      "step": 50430
    },
    {
      "epoch": 1.8897755797834477,
      "grad_norm": 0.15518438816070557,
      "learning_rate": 8.286638578809069e-07,
      "loss": 0.0913,
      "step": 50440
    },
    {
      "epoch": 1.8901502379079091,
      "grad_norm": 0.10516300052404404,
      "learning_rate": 8.23057477509398e-07,
      "loss": 0.0887,
      "step": 50450
    },
    {
      "epoch": 1.8905248960323706,
      "grad_norm": 0.10376179218292236,
      "learning_rate": 8.17469969343082e-07,
      "loss": 0.0886,
      "step": 50460
    },
    {
      "epoch": 1.8908995541568319,
      "grad_norm": 0.10002477467060089,
      "learning_rate": 8.119013355262439e-07,
      "loss": 0.0895,
      "step": 50470
    },
    {
      "epoch": 1.8912742122812933,
      "grad_norm": 0.08978593349456787,
      "learning_rate": 8.06351578195913e-07,
      "loss": 0.0951,
      "step": 50480
    },
    {
      "epoch": 1.8916488704057548,
      "grad_norm": 0.09088214486837387,
      "learning_rate": 8.008206994818523e-07,
      "loss": 0.0882,
      "step": 50490
    },
    {
      "epoch": 1.892023528530216,
      "grad_norm": 0.09482421725988388,
      "learning_rate": 7.953087015066085e-07,
      "loss": 0.0874,
      "step": 50500
    },
    {
      "epoch": 1.8923981866546775,
      "grad_norm": 0.09218873828649521,
      "learning_rate": 7.898155863854728e-07,
      "loss": 0.0863,
      "step": 50510
    },
    {
      "epoch": 1.892772844779139,
      "grad_norm": 0.09333862364292145,
      "learning_rate": 7.843413562264923e-07,
      "loss": 0.0868,
      "step": 50520
    },
    {
      "epoch": 1.8931475029036005,
      "grad_norm": 0.09092110395431519,
      "learning_rate": 7.788860131304643e-07,
      "loss": 0.0871,
      "step": 50530
    },
    {
      "epoch": 1.893522161028062,
      "grad_norm": 0.0691574364900589,
      "learning_rate": 7.734495591909363e-07,
      "loss": 0.0893,
      "step": 50540
    },
    {
      "epoch": 1.8938968191525234,
      "grad_norm": 0.12311305850744247,
      "learning_rate": 7.680319964942063e-07,
      "loss": 0.0903,
      "step": 50550
    },
    {
      "epoch": 1.8942714772769849,
      "grad_norm": 0.1020013615489006,
      "learning_rate": 7.62633327119322e-07,
      "loss": 0.0922,
      "step": 50560
    },
    {
      "epoch": 1.8946461354014463,
      "grad_norm": 0.1322009563446045,
      "learning_rate": 7.572535531380931e-07,
      "loss": 0.0911,
      "step": 50570
    },
    {
      "epoch": 1.8950207935259076,
      "grad_norm": 0.08728858828544617,
      "learning_rate": 7.518926766150736e-07,
      "loss": 0.0882,
      "step": 50580
    },
    {
      "epoch": 1.895395451650369,
      "grad_norm": 0.13973858952522278,
      "learning_rate": 7.4655069960754e-07,
      "loss": 0.0887,
      "step": 50590
    },
    {
      "epoch": 1.8957701097748305,
      "grad_norm": 0.08168022334575653,
      "learning_rate": 7.412276241655414e-07,
      "loss": 0.0898,
      "step": 50600
    },
    {
      "epoch": 1.8961447678992918,
      "grad_norm": 0.08369098603725433,
      "learning_rate": 7.359234523318825e-07,
      "loss": 0.0892,
      "step": 50610
    },
    {
      "epoch": 1.8965194260237532,
      "grad_norm": 0.13236182928085327,
      "learning_rate": 7.306381861420908e-07,
      "loss": 0.0893,
      "step": 50620
    },
    {
      "epoch": 1.8968940841482147,
      "grad_norm": 0.09418827295303345,
      "learning_rate": 7.253718276244436e-07,
      "loss": 0.0927,
      "step": 50630
    },
    {
      "epoch": 1.8972687422726762,
      "grad_norm": 0.09280287474393845,
      "learning_rate": 7.201243787999635e-07,
      "loss": 0.0883,
      "step": 50640
    },
    {
      "epoch": 1.8976434003971376,
      "grad_norm": 0.0947837382555008,
      "learning_rate": 7.148958416824226e-07,
      "loss": 0.0919,
      "step": 50650
    },
    {
      "epoch": 1.898018058521599,
      "grad_norm": 0.07592207938432693,
      "learning_rate": 7.096862182783326e-07,
      "loss": 0.0875,
      "step": 50660
    },
    {
      "epoch": 1.8983927166460606,
      "grad_norm": 0.09002245962619781,
      "learning_rate": 7.044955105869444e-07,
      "loss": 0.0864,
      "step": 50670
    },
    {
      "epoch": 1.898767374770522,
      "grad_norm": 0.09255055338144302,
      "learning_rate": 6.993237206002424e-07,
      "loss": 0.091,
      "step": 50680
    },
    {
      "epoch": 1.8991420328949833,
      "grad_norm": 0.10488730669021606,
      "learning_rate": 6.94170850302972e-07,
      "loss": 0.0904,
      "step": 50690
    },
    {
      "epoch": 1.8995166910194448,
      "grad_norm": 0.09405525773763657,
      "learning_rate": 6.890369016725907e-07,
      "loss": 0.0894,
      "step": 50700
    },
    {
      "epoch": 1.8998913491439062,
      "grad_norm": 0.0989413782954216,
      "learning_rate": 6.839218766793221e-07,
      "loss": 0.0911,
      "step": 50710
    },
    {
      "epoch": 1.9002660072683675,
      "grad_norm": 0.09931553900241852,
      "learning_rate": 6.788257772861129e-07,
      "loss": 0.0908,
      "step": 50720
    },
    {
      "epoch": 1.900640665392829,
      "grad_norm": 0.11017701774835587,
      "learning_rate": 6.73748605448643e-07,
      "loss": 0.0898,
      "step": 50730
    },
    {
      "epoch": 1.9010153235172904,
      "grad_norm": 0.09704374521970749,
      "learning_rate": 6.686903631153318e-07,
      "loss": 0.0911,
      "step": 50740
    },
    {
      "epoch": 1.9013899816417519,
      "grad_norm": 0.07364879548549652,
      "learning_rate": 6.636510522273487e-07,
      "loss": 0.0895,
      "step": 50750
    },
    {
      "epoch": 1.9017646397662133,
      "grad_norm": 0.09259860217571259,
      "learning_rate": 6.586306747185744e-07,
      "loss": 0.0882,
      "step": 50760
    },
    {
      "epoch": 1.9021392978906748,
      "grad_norm": 0.08018497377634048,
      "learning_rate": 6.536292325156401e-07,
      "loss": 0.0898,
      "step": 50770
    },
    {
      "epoch": 1.9025139560151363,
      "grad_norm": 0.09565222263336182,
      "learning_rate": 6.486467275379049e-07,
      "loss": 0.0856,
      "step": 50780
    },
    {
      "epoch": 1.9028886141395978,
      "grad_norm": 0.0814194604754448,
      "learning_rate": 6.436831616974614e-07,
      "loss": 0.0871,
      "step": 50790
    },
    {
      "epoch": 1.903263272264059,
      "grad_norm": 0.08926188945770264,
      "learning_rate": 6.387385368991305e-07,
      "loss": 0.092,
      "step": 50800
    },
    {
      "epoch": 1.9036379303885205,
      "grad_norm": 0.12993574142456055,
      "learning_rate": 6.338128550404721e-07,
      "loss": 0.0891,
      "step": 50810
    },
    {
      "epoch": 1.904012588512982,
      "grad_norm": 0.11267600953578949,
      "learning_rate": 6.28906118011774e-07,
      "loss": 0.0896,
      "step": 50820
    },
    {
      "epoch": 1.9043872466374432,
      "grad_norm": 0.09976467490196228,
      "learning_rate": 6.240183276960465e-07,
      "loss": 0.0927,
      "step": 50830
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.14374154806137085,
      "learning_rate": 6.191494859690394e-07,
      "loss": 0.0924,
      "step": 50840
    },
    {
      "epoch": 1.9051365628863661,
      "grad_norm": 0.12196024507284164,
      "learning_rate": 6.142995946992191e-07,
      "loss": 0.0881,
      "step": 50850
    },
    {
      "epoch": 1.9055112210108276,
      "grad_norm": 0.12514916062355042,
      "learning_rate": 6.094686557477913e-07,
      "loss": 0.0938,
      "step": 50860
    },
    {
      "epoch": 1.905885879135289,
      "grad_norm": 0.10047654062509537,
      "learning_rate": 6.046566709686785e-07,
      "loss": 0.088,
      "step": 50870
    },
    {
      "epoch": 1.9062605372597505,
      "grad_norm": 0.10702256858348846,
      "learning_rate": 5.998636422085369e-07,
      "loss": 0.0888,
      "step": 50880
    },
    {
      "epoch": 1.906635195384212,
      "grad_norm": 0.07285775244235992,
      "learning_rate": 5.95089571306745e-07,
      "loss": 0.0859,
      "step": 50890
    },
    {
      "epoch": 1.9070098535086735,
      "grad_norm": 0.09289877861738205,
      "learning_rate": 5.903344600954097e-07,
      "loss": 0.0942,
      "step": 50900
    },
    {
      "epoch": 1.9073845116331347,
      "grad_norm": 0.13411080837249756,
      "learning_rate": 5.85598310399349e-07,
      "loss": 0.0913,
      "step": 50910
    },
    {
      "epoch": 1.9077591697575962,
      "grad_norm": 0.11804269999265671,
      "learning_rate": 5.808811240361256e-07,
      "loss": 0.0893,
      "step": 50920
    },
    {
      "epoch": 1.9081338278820577,
      "grad_norm": 0.1267329454421997,
      "learning_rate": 5.761829028160026e-07,
      "loss": 0.0871,
      "step": 50930
    },
    {
      "epoch": 1.908508486006519,
      "grad_norm": 0.10838963091373444,
      "learning_rate": 5.715036485419822e-07,
      "loss": 0.0905,
      "step": 50940
    },
    {
      "epoch": 1.9088831441309804,
      "grad_norm": 0.08560948073863983,
      "learning_rate": 5.668433630097725e-07,
      "loss": 0.0931,
      "step": 50950
    },
    {
      "epoch": 1.9092578022554418,
      "grad_norm": 0.08307366818189621,
      "learning_rate": 5.622020480078261e-07,
      "loss": 0.0894,
      "step": 50960
    },
    {
      "epoch": 1.9096324603799033,
      "grad_norm": 0.08416098356246948,
      "learning_rate": 5.575797053172793e-07,
      "loss": 0.0877,
      "step": 50970
    },
    {
      "epoch": 1.9100071185043648,
      "grad_norm": 0.09031373262405396,
      "learning_rate": 5.529763367120244e-07,
      "loss": 0.087,
      "step": 50980
    },
    {
      "epoch": 1.9103817766288262,
      "grad_norm": 0.10318867117166519,
      "learning_rate": 5.483919439586538e-07,
      "loss": 0.0895,
      "step": 50990
    },
    {
      "epoch": 1.9107564347532877,
      "grad_norm": 0.08200875669717789,
      "learning_rate": 5.438265288164712e-07,
      "loss": 0.0913,
      "step": 51000
    },
    {
      "epoch": 1.9111310928777492,
      "grad_norm": 0.11714953929185867,
      "learning_rate": 5.392800930375141e-07,
      "loss": 0.0896,
      "step": 51010
    },
    {
      "epoch": 1.9115057510022104,
      "grad_norm": 0.10686523467302322,
      "learning_rate": 5.347526383665257e-07,
      "loss": 0.0879,
      "step": 51020
    },
    {
      "epoch": 1.911880409126672,
      "grad_norm": 0.1460961550474167,
      "learning_rate": 5.302441665409608e-07,
      "loss": 0.0902,
      "step": 51030
    },
    {
      "epoch": 1.9122550672511334,
      "grad_norm": 0.09467368572950363,
      "learning_rate": 5.257546792910128e-07,
      "loss": 0.0925,
      "step": 51040
    },
    {
      "epoch": 1.9126297253755946,
      "grad_norm": 0.0771559402346611,
      "learning_rate": 5.212841783395539e-07,
      "loss": 0.0894,
      "step": 51050
    },
    {
      "epoch": 1.913004383500056,
      "grad_norm": 0.0807969868183136,
      "learning_rate": 5.16832665402206e-07,
      "loss": 0.0884,
      "step": 51060
    },
    {
      "epoch": 1.9133790416245176,
      "grad_norm": 0.08697722852230072,
      "learning_rate": 5.124001421872748e-07,
      "loss": 0.0883,
      "step": 51070
    },
    {
      "epoch": 1.913753699748979,
      "grad_norm": 0.09580259025096893,
      "learning_rate": 5.079866103957942e-07,
      "loss": 0.0901,
      "step": 51080
    },
    {
      "epoch": 1.9141283578734405,
      "grad_norm": 0.09144458919763565,
      "learning_rate": 5.035920717215148e-07,
      "loss": 0.0909,
      "step": 51090
    },
    {
      "epoch": 1.914503015997902,
      "grad_norm": 0.11589506268501282,
      "learning_rate": 4.99216527850882e-07,
      "loss": 0.088,
      "step": 51100
    },
    {
      "epoch": 1.9148776741223634,
      "grad_norm": 0.06795729696750641,
      "learning_rate": 4.948599804630583e-07,
      "loss": 0.0853,
      "step": 51110
    },
    {
      "epoch": 1.915252332246825,
      "grad_norm": 0.15808941423892975,
      "learning_rate": 4.905224312299228e-07,
      "loss": 0.0899,
      "step": 51120
    },
    {
      "epoch": 1.9156269903712864,
      "grad_norm": 0.10309087485074997,
      "learning_rate": 4.862038818160608e-07,
      "loss": 0.0886,
      "step": 51130
    },
    {
      "epoch": 1.9160016484957476,
      "grad_norm": 0.09437308460474014,
      "learning_rate": 4.819043338787577e-07,
      "loss": 0.0906,
      "step": 51140
    },
    {
      "epoch": 1.916376306620209,
      "grad_norm": 0.11358781903982162,
      "learning_rate": 4.776237890680158e-07,
      "loss": 0.0893,
      "step": 51150
    },
    {
      "epoch": 1.9167509647446703,
      "grad_norm": 0.09745368361473083,
      "learning_rate": 4.7336224902654326e-07,
      "loss": 0.0867,
      "step": 51160
    },
    {
      "epoch": 1.9171256228691318,
      "grad_norm": 0.08491183072328568,
      "learning_rate": 4.691197153897542e-07,
      "loss": 0.0875,
      "step": 51170
    },
    {
      "epoch": 1.9175002809935933,
      "grad_norm": 0.07420013099908829,
      "learning_rate": 4.648961897857684e-07,
      "loss": 0.088,
      "step": 51180
    },
    {
      "epoch": 1.9178749391180547,
      "grad_norm": 0.1322527974843979,
      "learning_rate": 4.6069167383541167e-07,
      "loss": 0.09,
      "step": 51190
    },
    {
      "epoch": 1.9182495972425162,
      "grad_norm": 0.08890403807163239,
      "learning_rate": 4.565061691522099e-07,
      "loss": 0.0907,
      "step": 51200
    },
    {
      "epoch": 1.9186242553669777,
      "grad_norm": 0.08587248623371124,
      "learning_rate": 4.52339677342406e-07,
      "loss": 0.0892,
      "step": 51210
    },
    {
      "epoch": 1.9189989134914391,
      "grad_norm": 0.08047053962945938,
      "learning_rate": 4.481922000049321e-07,
      "loss": 0.0902,
      "step": 51220
    },
    {
      "epoch": 1.9193735716159006,
      "grad_norm": 0.09229061752557755,
      "learning_rate": 4.440637387314373e-07,
      "loss": 0.0896,
      "step": 51230
    },
    {
      "epoch": 1.919748229740362,
      "grad_norm": 0.1212875172495842,
      "learning_rate": 4.3995429510625407e-07,
      "loss": 0.0936,
      "step": 51240
    },
    {
      "epoch": 1.9201228878648233,
      "grad_norm": 0.1025790348649025,
      "learning_rate": 4.3586387070642664e-07,
      "loss": 0.0886,
      "step": 51250
    },
    {
      "epoch": 1.9204975459892848,
      "grad_norm": 0.12537457048892975,
      "learning_rate": 4.3179246710171037e-07,
      "loss": 0.0921,
      "step": 51260
    },
    {
      "epoch": 1.920872204113746,
      "grad_norm": 0.09143542498350143,
      "learning_rate": 4.277400858545555e-07,
      "loss": 0.087,
      "step": 51270
    },
    {
      "epoch": 1.9212468622382075,
      "grad_norm": 0.09113816916942596,
      "learning_rate": 4.237067285200957e-07,
      "loss": 0.0896,
      "step": 51280
    },
    {
      "epoch": 1.921621520362669,
      "grad_norm": 0.12793849408626556,
      "learning_rate": 4.196923966461819e-07,
      "loss": 0.0914,
      "step": 51290
    },
    {
      "epoch": 1.9219961784871304,
      "grad_norm": 0.0923716127872467,
      "learning_rate": 4.1569709177336494e-07,
      "loss": 0.088,
      "step": 51300
    },
    {
      "epoch": 1.922370836611592,
      "grad_norm": 0.0938301831483841,
      "learning_rate": 4.1172081543488506e-07,
      "loss": 0.0925,
      "step": 51310
    },
    {
      "epoch": 1.9227454947360534,
      "grad_norm": 0.11440993845462799,
      "learning_rate": 4.077635691566828e-07,
      "loss": 0.0892,
      "step": 51320
    },
    {
      "epoch": 1.9231201528605149,
      "grad_norm": 0.13732697069644928,
      "learning_rate": 4.038253544573933e-07,
      "loss": 0.0915,
      "step": 51330
    },
    {
      "epoch": 1.9234948109849763,
      "grad_norm": 0.09300815314054489,
      "learning_rate": 3.9990617284835217e-07,
      "loss": 0.0884,
      "step": 51340
    },
    {
      "epoch": 1.9238694691094378,
      "grad_norm": 0.1399930715560913,
      "learning_rate": 3.9600602583358957e-07,
      "loss": 0.088,
      "step": 51350
    },
    {
      "epoch": 1.924244127233899,
      "grad_norm": 0.10648486018180847,
      "learning_rate": 3.92124914909836e-07,
      "loss": 0.0866,
      "step": 51360
    },
    {
      "epoch": 1.9246187853583605,
      "grad_norm": 0.09445372223854065,
      "learning_rate": 3.8826284156650574e-07,
      "loss": 0.0881,
      "step": 51370
    },
    {
      "epoch": 1.924993443482822,
      "grad_norm": 0.07759369164705276,
      "learning_rate": 3.8441980728571304e-07,
      "loss": 0.0834,
      "step": 51380
    },
    {
      "epoch": 1.9253681016072832,
      "grad_norm": 0.10700477659702301,
      "learning_rate": 3.805958135422616e-07,
      "loss": 0.0888,
      "step": 51390
    },
    {
      "epoch": 1.9257427597317447,
      "grad_norm": 0.0948648527264595,
      "learning_rate": 3.7679086180366086e-07,
      "loss": 0.0885,
      "step": 51400
    },
    {
      "epoch": 1.9261174178562062,
      "grad_norm": 0.10785277932882309,
      "learning_rate": 3.730049535301039e-07,
      "loss": 0.0943,
      "step": 51410
    },
    {
      "epoch": 1.9264920759806676,
      "grad_norm": 0.08756789565086365,
      "learning_rate": 3.692380901744674e-07,
      "loss": 0.0889,
      "step": 51420
    },
    {
      "epoch": 1.926866734105129,
      "grad_norm": 0.08045276999473572,
      "learning_rate": 3.654902731823284e-07,
      "loss": 0.0901,
      "step": 51430
    },
    {
      "epoch": 1.9272413922295906,
      "grad_norm": 0.0955914631485939,
      "learning_rate": 3.617615039919642e-07,
      "loss": 0.0879,
      "step": 51440
    },
    {
      "epoch": 1.927616050354052,
      "grad_norm": 0.10819472372531891,
      "learning_rate": 3.5805178403431894e-07,
      "loss": 0.087,
      "step": 51450
    },
    {
      "epoch": 1.9279907084785135,
      "grad_norm": 0.15436063706874847,
      "learning_rate": 3.543611147330539e-07,
      "loss": 0.0896,
      "step": 51460
    },
    {
      "epoch": 1.9283653666029748,
      "grad_norm": 0.10333888977766037,
      "learning_rate": 3.5068949750449166e-07,
      "loss": 0.089,
      "step": 51470
    },
    {
      "epoch": 1.9287400247274362,
      "grad_norm": 0.11194904148578644,
      "learning_rate": 3.4703693375766066e-07,
      "loss": 0.088,
      "step": 51480
    },
    {
      "epoch": 1.9291146828518977,
      "grad_norm": 0.10436372458934784,
      "learning_rate": 3.4340342489427303e-07,
      "loss": 0.0901,
      "step": 51490
    },
    {
      "epoch": 1.929489340976359,
      "grad_norm": 0.07757464796304703,
      "learning_rate": 3.3978897230873555e-07,
      "loss": 0.0906,
      "step": 51500
    },
    {
      "epoch": 1.9298639991008204,
      "grad_norm": 0.09139669686555862,
      "learning_rate": 3.361935773881275e-07,
      "loss": 0.089,
      "step": 51510
    },
    {
      "epoch": 1.9302386572252819,
      "grad_norm": 0.1635793298482895,
      "learning_rate": 3.326172415122231e-07,
      "loss": 0.0896,
      "step": 51520
    },
    {
      "epoch": 1.9306133153497433,
      "grad_norm": 0.06918971240520477,
      "learning_rate": 3.290599660534799e-07,
      "loss": 0.0884,
      "step": 51530
    },
    {
      "epoch": 1.9309879734742048,
      "grad_norm": 0.11233299970626831,
      "learning_rate": 3.255217523770504e-07,
      "loss": 0.0885,
      "step": 51540
    },
    {
      "epoch": 1.9313626315986663,
      "grad_norm": 0.10457656532526016,
      "learning_rate": 3.2200260184075406e-07,
      "loss": 0.0915,
      "step": 51550
    },
    {
      "epoch": 1.9317372897231277,
      "grad_norm": 0.12373606115579605,
      "learning_rate": 3.185025157951105e-07,
      "loss": 0.0879,
      "step": 51560
    },
    {
      "epoch": 1.9321119478475892,
      "grad_norm": 0.11316762119531631,
      "learning_rate": 3.150214955833175e-07,
      "loss": 0.0875,
      "step": 51570
    },
    {
      "epoch": 1.9324866059720505,
      "grad_norm": 0.11782452464103699,
      "learning_rate": 3.11559542541251e-07,
      "loss": 0.0898,
      "step": 51580
    },
    {
      "epoch": 1.932861264096512,
      "grad_norm": 0.08798348158597946,
      "learning_rate": 3.0811665799748144e-07,
      "loss": 0.0882,
      "step": 51590
    },
    {
      "epoch": 1.9332359222209734,
      "grad_norm": 0.10847458988428116,
      "learning_rate": 3.0469284327324653e-07,
      "loss": 0.0898,
      "step": 51600
    },
    {
      "epoch": 1.9336105803454346,
      "grad_norm": 0.10097216814756393,
      "learning_rate": 3.0128809968248404e-07,
      "loss": 0.0901,
      "step": 51610
    },
    {
      "epoch": 1.9339852384698961,
      "grad_norm": 0.0738254263997078,
      "learning_rate": 2.979024285317877e-07,
      "loss": 0.0874,
      "step": 51620
    },
    {
      "epoch": 1.9343598965943576,
      "grad_norm": 0.09502828121185303,
      "learning_rate": 2.9453583112045714e-07,
      "loss": 0.0917,
      "step": 51630
    },
    {
      "epoch": 1.934734554718819,
      "grad_norm": 0.09298369288444519,
      "learning_rate": 2.9118830874046986e-07,
      "loss": 0.0891,
      "step": 51640
    },
    {
      "epoch": 1.9351092128432805,
      "grad_norm": 0.133816197514534,
      "learning_rate": 2.878598626764539e-07,
      "loss": 0.0886,
      "step": 51650
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 0.10731355100870132,
      "learning_rate": 2.845504942057542e-07,
      "loss": 0.0919,
      "step": 51660
    },
    {
      "epoch": 1.9358585290922035,
      "grad_norm": 0.08757587522268295,
      "learning_rate": 2.8126020459837697e-07,
      "loss": 0.0882,
      "step": 51670
    },
    {
      "epoch": 1.936233187216665,
      "grad_norm": 0.09906227141618729,
      "learning_rate": 2.7798899511700116e-07,
      "loss": 0.0897,
      "step": 51680
    },
    {
      "epoch": 1.9366078453411262,
      "grad_norm": 0.08178864419460297,
      "learning_rate": 2.747368670169892e-07,
      "loss": 0.0847,
      "step": 51690
    },
    {
      "epoch": 1.9369825034655876,
      "grad_norm": 0.11480341851711273,
      "learning_rate": 2.7150382154639274e-07,
      "loss": 0.0903,
      "step": 51700
    },
    {
      "epoch": 1.9373571615900491,
      "grad_norm": 0.12092670798301697,
      "learning_rate": 2.6828985994592494e-07,
      "loss": 0.0908,
      "step": 51710
    },
    {
      "epoch": 1.9377318197145104,
      "grad_norm": 0.0888240858912468,
      "learning_rate": 2.650949834489713e-07,
      "loss": 0.0862,
      "step": 51720
    },
    {
      "epoch": 1.9381064778389718,
      "grad_norm": 0.11964910477399826,
      "learning_rate": 2.6191919328160673e-07,
      "loss": 0.0923,
      "step": 51730
    },
    {
      "epoch": 1.9384811359634333,
      "grad_norm": 0.10840713232755661,
      "learning_rate": 2.5876249066258406e-07,
      "loss": 0.0876,
      "step": 51740
    },
    {
      "epoch": 1.9388557940878948,
      "grad_norm": 0.08222605288028717,
      "learning_rate": 2.556248768033176e-07,
      "loss": 0.0885,
      "step": 51750
    },
    {
      "epoch": 1.9392304522123562,
      "grad_norm": 0.09550786763429642,
      "learning_rate": 2.5250635290789415e-07,
      "loss": 0.0853,
      "step": 51760
    },
    {
      "epoch": 1.9396051103368177,
      "grad_norm": 0.07004225254058838,
      "learning_rate": 2.494069201730953e-07,
      "loss": 0.087,
      "step": 51770
    },
    {
      "epoch": 1.9399797684612792,
      "grad_norm": 0.12007524073123932,
      "learning_rate": 2.463265797883585e-07,
      "loss": 0.0921,
      "step": 51780
    },
    {
      "epoch": 1.9403544265857406,
      "grad_norm": 0.08793900907039642,
      "learning_rate": 2.4326533293579924e-07,
      "loss": 0.0896,
      "step": 51790
    },
    {
      "epoch": 1.940729084710202,
      "grad_norm": 0.15522919595241547,
      "learning_rate": 2.402231807902e-07,
      "loss": 0.0858,
      "step": 51800
    },
    {
      "epoch": 1.9411037428346634,
      "grad_norm": 0.09217407554388046,
      "learning_rate": 2.3720012451902696e-07,
      "loss": 0.0908,
      "step": 51810
    },
    {
      "epoch": 1.9414784009591248,
      "grad_norm": 0.09965529292821884,
      "learning_rate": 2.3419616528241318e-07,
      "loss": 0.0857,
      "step": 51820
    },
    {
      "epoch": 1.941853059083586,
      "grad_norm": 0.07349394261837006,
      "learning_rate": 2.3121130423315319e-07,
      "loss": 0.0878,
      "step": 51830
    },
    {
      "epoch": 1.9422277172080475,
      "grad_norm": 0.07733161747455597,
      "learning_rate": 2.282455425167307e-07,
      "loss": 0.0863,
      "step": 51840
    },
    {
      "epoch": 1.942602375332509,
      "grad_norm": 0.11650821566581726,
      "learning_rate": 2.2529888127128528e-07,
      "loss": 0.09,
      "step": 51850
    },
    {
      "epoch": 1.9429770334569705,
      "grad_norm": 0.08180662244558334,
      "learning_rate": 2.2237132162763463e-07,
      "loss": 0.0906,
      "step": 51860
    },
    {
      "epoch": 1.943351691581432,
      "grad_norm": 0.07138964533805847,
      "learning_rate": 2.1946286470925782e-07,
      "loss": 0.0878,
      "step": 51870
    },
    {
      "epoch": 1.9437263497058934,
      "grad_norm": 0.10035688430070877,
      "learning_rate": 2.16573511632312e-07,
      "loss": 0.0898,
      "step": 51880
    },
    {
      "epoch": 1.9441010078303549,
      "grad_norm": 0.10579434037208557,
      "learning_rate": 2.137032635056213e-07,
      "loss": 0.0903,
      "step": 51890
    },
    {
      "epoch": 1.9444756659548164,
      "grad_norm": 0.1123642772436142,
      "learning_rate": 2.108521214306658e-07,
      "loss": 0.0903,
      "step": 51900
    },
    {
      "epoch": 1.9448503240792776,
      "grad_norm": 0.08700790256261826,
      "learning_rate": 2.080200865016091e-07,
      "loss": 0.0939,
      "step": 51910
    },
    {
      "epoch": 1.945224982203739,
      "grad_norm": 0.09751785546541214,
      "learning_rate": 2.0520715980528183e-07,
      "loss": 0.0885,
      "step": 51920
    },
    {
      "epoch": 1.9455996403282005,
      "grad_norm": 0.09243113547563553,
      "learning_rate": 2.0241334242117048e-07,
      "loss": 0.0922,
      "step": 51930
    },
    {
      "epoch": 1.9459742984526618,
      "grad_norm": 0.12125565111637115,
      "learning_rate": 1.9963863542142857e-07,
      "loss": 0.0895,
      "step": 51940
    },
    {
      "epoch": 1.9463489565771233,
      "grad_norm": 0.11124400049448013,
      "learning_rate": 1.968830398708932e-07,
      "loss": 0.0893,
      "step": 51950
    },
    {
      "epoch": 1.9467236147015847,
      "grad_norm": 0.07546577602624893,
      "learning_rate": 1.9414655682705173e-07,
      "loss": 0.0877,
      "step": 51960
    },
    {
      "epoch": 1.9470982728260462,
      "grad_norm": 0.14208537340164185,
      "learning_rate": 1.914291873400531e-07,
      "loss": 0.0915,
      "step": 51970
    },
    {
      "epoch": 1.9474729309505077,
      "grad_norm": 0.13373924791812897,
      "learning_rate": 1.8873093245272422e-07,
      "loss": 0.0863,
      "step": 51980
    },
    {
      "epoch": 1.9478475890749691,
      "grad_norm": 0.06878571212291718,
      "learning_rate": 1.8605179320054788e-07,
      "loss": 0.0856,
      "step": 51990
    },
    {
      "epoch": 1.9482222471994306,
      "grad_norm": 0.07645071297883987,
      "learning_rate": 1.8339177061167943e-07,
      "loss": 0.088,
      "step": 52000
    },
    {
      "epoch": 1.948596905323892,
      "grad_norm": 0.09746409952640533,
      "learning_rate": 1.807508657069301e-07,
      "loss": 0.0854,
      "step": 52010
    },
    {
      "epoch": 1.9489715634483535,
      "grad_norm": 0.11846205592155457,
      "learning_rate": 1.7812907949977254e-07,
      "loss": 0.0908,
      "step": 52020
    },
    {
      "epoch": 1.9493462215728148,
      "grad_norm": 0.07773604243993759,
      "learning_rate": 1.7552641299635185e-07,
      "loss": 0.0915,
      "step": 52030
    },
    {
      "epoch": 1.9497208796972763,
      "grad_norm": 0.09107508510351181,
      "learning_rate": 1.729428671954636e-07,
      "loss": 0.0937,
      "step": 52040
    },
    {
      "epoch": 1.9500955378217375,
      "grad_norm": 0.0895731970667839,
      "learning_rate": 1.7037844308858685e-07,
      "loss": 0.092,
      "step": 52050
    },
    {
      "epoch": 1.950470195946199,
      "grad_norm": 0.08720667660236359,
      "learning_rate": 1.6783314165983443e-07,
      "loss": 0.0901,
      "step": 52060
    },
    {
      "epoch": 1.9508448540706604,
      "grad_norm": 0.10553672909736633,
      "learning_rate": 1.6530696388599721e-07,
      "loss": 0.0887,
      "step": 52070
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.1285509616136551,
      "learning_rate": 1.6279991073652746e-07,
      "loss": 0.0877,
      "step": 52080
    },
    {
      "epoch": 1.9515941703195834,
      "grad_norm": 0.09313304722309113,
      "learning_rate": 1.6031198317353336e-07,
      "loss": 0.0889,
      "step": 52090
    },
    {
      "epoch": 1.9519688284440448,
      "grad_norm": 0.08014358580112457,
      "learning_rate": 1.578431821517845e-07,
      "loss": 0.0884,
      "step": 52100
    },
    {
      "epoch": 1.9523434865685063,
      "grad_norm": 0.10356228053569794,
      "learning_rate": 1.5539350861871194e-07,
      "loss": 0.0874,
      "step": 52110
    },
    {
      "epoch": 1.9527181446929678,
      "grad_norm": 0.08214949816465378,
      "learning_rate": 1.5296296351440807e-07,
      "loss": 0.0851,
      "step": 52120
    },
    {
      "epoch": 1.9530928028174293,
      "grad_norm": 0.0890103206038475,
      "learning_rate": 1.5055154777161574e-07,
      "loss": 0.0885,
      "step": 52130
    },
    {
      "epoch": 1.9534674609418905,
      "grad_norm": 0.09176526218652725,
      "learning_rate": 1.4815926231574463e-07,
      "loss": 0.0957,
      "step": 52140
    },
    {
      "epoch": 1.953842119066352,
      "grad_norm": 0.10426337271928787,
      "learning_rate": 1.4578610806487147e-07,
      "loss": 0.0926,
      "step": 52150
    },
    {
      "epoch": 1.9542167771908134,
      "grad_norm": 0.14626607298851013,
      "learning_rate": 1.434320859297067e-07,
      "loss": 0.0938,
      "step": 52160
    },
    {
      "epoch": 1.9545914353152747,
      "grad_norm": 0.07468384504318237,
      "learning_rate": 1.4109719681363876e-07,
      "loss": 0.0904,
      "step": 52170
    },
    {
      "epoch": 1.9549660934397362,
      "grad_norm": 0.08777526021003723,
      "learning_rate": 1.3878144161270646e-07,
      "loss": 0.0861,
      "step": 52180
    },
    {
      "epoch": 1.9553407515641976,
      "grad_norm": 0.11728853732347488,
      "learning_rate": 1.3648482121561558e-07,
      "loss": 0.0892,
      "step": 52190
    },
    {
      "epoch": 1.955715409688659,
      "grad_norm": 0.09688294678926468,
      "learning_rate": 1.3420733650370554e-07,
      "loss": 0.0925,
      "step": 52200
    },
    {
      "epoch": 1.9560900678131206,
      "grad_norm": 0.10692868381738663,
      "learning_rate": 1.319489883509939e-07,
      "loss": 0.0909,
      "step": 52210
    },
    {
      "epoch": 1.956464725937582,
      "grad_norm": 0.10965267568826675,
      "learning_rate": 1.2970977762414848e-07,
      "loss": 0.0885,
      "step": 52220
    },
    {
      "epoch": 1.9568393840620435,
      "grad_norm": 0.0791437104344368,
      "learning_rate": 1.2748970518248748e-07,
      "loss": 0.0865,
      "step": 52230
    },
    {
      "epoch": 1.957214042186505,
      "grad_norm": 0.10074629634618759,
      "learning_rate": 1.2528877187799605e-07,
      "loss": 0.0885,
      "step": 52240
    },
    {
      "epoch": 1.9575887003109662,
      "grad_norm": 0.10383004695177078,
      "learning_rate": 1.231069785552985e-07,
      "loss": 0.087,
      "step": 52250
    },
    {
      "epoch": 1.9579633584354277,
      "grad_norm": 0.08581706881523132,
      "learning_rate": 1.2094432605169182e-07,
      "loss": 0.0887,
      "step": 52260
    },
    {
      "epoch": 1.9583380165598891,
      "grad_norm": 0.07373382151126862,
      "learning_rate": 1.1880081519710651e-07,
      "loss": 0.0875,
      "step": 52270
    },
    {
      "epoch": 1.9587126746843504,
      "grad_norm": 0.12111219763755798,
      "learning_rate": 1.1667644681414568e-07,
      "loss": 0.0912,
      "step": 52280
    },
    {
      "epoch": 1.9590873328088119,
      "grad_norm": 0.09764158725738525,
      "learning_rate": 1.1457122171806278e-07,
      "loss": 0.0905,
      "step": 52290
    },
    {
      "epoch": 1.9594619909332733,
      "grad_norm": 0.1001053899526596,
      "learning_rate": 1.1248514071675598e-07,
      "loss": 0.0878,
      "step": 52300
    },
    {
      "epoch": 1.9598366490577348,
      "grad_norm": 0.10097260773181915,
      "learning_rate": 1.104182046107849e-07,
      "loss": 0.093,
      "step": 52310
    },
    {
      "epoch": 1.9602113071821963,
      "grad_norm": 0.10389646887779236,
      "learning_rate": 1.0837041419335947e-07,
      "loss": 0.0911,
      "step": 52320
    },
    {
      "epoch": 1.9605859653066577,
      "grad_norm": 0.08630029112100601,
      "learning_rate": 1.0634177025033998e-07,
      "loss": 0.0862,
      "step": 52330
    },
    {
      "epoch": 1.9609606234311192,
      "grad_norm": 0.0990263894200325,
      "learning_rate": 1.0433227356024256e-07,
      "loss": 0.0897,
      "step": 52340
    },
    {
      "epoch": 1.9613352815555807,
      "grad_norm": 0.09589997678995132,
      "learning_rate": 1.0234192489423921e-07,
      "loss": 0.0871,
      "step": 52350
    },
    {
      "epoch": 1.961709939680042,
      "grad_norm": 0.10062319040298462,
      "learning_rate": 1.0037072501614119e-07,
      "loss": 0.0918,
      "step": 52360
    },
    {
      "epoch": 1.9620845978045034,
      "grad_norm": 0.09675919264554977,
      "learning_rate": 9.841867468242116e-08,
      "loss": 0.0896,
      "step": 52370
    },
    {
      "epoch": 1.9624592559289649,
      "grad_norm": 0.0872231125831604,
      "learning_rate": 9.648577464220764e-08,
      "loss": 0.0941,
      "step": 52380
    },
    {
      "epoch": 1.962833914053426,
      "grad_norm": 0.08225754648447037,
      "learning_rate": 9.457202563725732e-08,
      "loss": 0.0874,
      "step": 52390
    },
    {
      "epoch": 1.9632085721778876,
      "grad_norm": 0.11311152577400208,
      "learning_rate": 9.267742840201043e-08,
      "loss": 0.0906,
      "step": 52400
    },
    {
      "epoch": 1.963583230302349,
      "grad_norm": 0.09323307126760483,
      "learning_rate": 9.080198366352433e-08,
      "loss": 0.0868,
      "step": 52410
    },
    {
      "epoch": 1.9639578884268105,
      "grad_norm": 0.08494981378316879,
      "learning_rate": 8.894569214152882e-08,
      "loss": 0.0911,
      "step": 52420
    },
    {
      "epoch": 1.964332546551272,
      "grad_norm": 0.11468467116355896,
      "learning_rate": 8.710855454840406e-08,
      "loss": 0.0902,
      "step": 52430
    },
    {
      "epoch": 1.9647072046757335,
      "grad_norm": 0.08962853997945786,
      "learning_rate": 8.529057158915832e-08,
      "loss": 0.0898,
      "step": 52440
    },
    {
      "epoch": 1.965081862800195,
      "grad_norm": 0.09885896742343903,
      "learning_rate": 8.349174396147242e-08,
      "loss": 0.09,
      "step": 52450
    },
    {
      "epoch": 1.9654565209246564,
      "grad_norm": 0.0877348780632019,
      "learning_rate": 8.17120723556608e-08,
      "loss": 0.0874,
      "step": 52460
    },
    {
      "epoch": 1.9658311790491176,
      "grad_norm": 0.09430372714996338,
      "learning_rate": 7.99515574546994e-08,
      "loss": 0.0904,
      "step": 52470
    },
    {
      "epoch": 1.966205837173579,
      "grad_norm": 0.09101123362779617,
      "learning_rate": 7.821019993420331e-08,
      "loss": 0.0915,
      "step": 52480
    },
    {
      "epoch": 1.9665804952980406,
      "grad_norm": 0.08663149923086166,
      "learning_rate": 7.648800046243243e-08,
      "loss": 0.0944,
      "step": 52490
    },
    {
      "epoch": 1.9669551534225018,
      "grad_norm": 0.07942654192447662,
      "learning_rate": 7.478495970030807e-08,
      "loss": 0.0889,
      "step": 52500
    },
    {
      "epoch": 1.9673298115469633,
      "grad_norm": 0.11980374902486801,
      "learning_rate": 7.310107830138523e-08,
      "loss": 0.0903,
      "step": 52510
    },
    {
      "epoch": 1.9677044696714248,
      "grad_norm": 0.1018955260515213,
      "learning_rate": 7.143635691188033e-08,
      "loss": 0.0924,
      "step": 52520
    },
    {
      "epoch": 1.9680791277958862,
      "grad_norm": 0.09398837387561798,
      "learning_rate": 6.979079617063788e-08,
      "loss": 0.0903,
      "step": 52530
    },
    {
      "epoch": 1.9684537859203477,
      "grad_norm": 0.11469129472970963,
      "learning_rate": 6.816439670916941e-08,
      "loss": 0.0862,
      "step": 52540
    },
    {
      "epoch": 1.9688284440448092,
      "grad_norm": 0.0818176195025444,
      "learning_rate": 6.655715915161453e-08,
      "loss": 0.0912,
      "step": 52550
    },
    {
      "epoch": 1.9692031021692706,
      "grad_norm": 0.10377715528011322,
      "learning_rate": 6.496908411477986e-08,
      "loss": 0.0901,
      "step": 52560
    },
    {
      "epoch": 1.969577760293732,
      "grad_norm": 0.08475956320762634,
      "learning_rate": 6.340017220810013e-08,
      "loss": 0.0867,
      "step": 52570
    },
    {
      "epoch": 1.9699524184181934,
      "grad_norm": 0.1551242619752884,
      "learning_rate": 6.185042403366592e-08,
      "loss": 0.0903,
      "step": 52580
    },
    {
      "epoch": 1.9703270765426548,
      "grad_norm": 0.08085081726312637,
      "learning_rate": 6.031984018621261e-08,
      "loss": 0.0872,
      "step": 52590
    },
    {
      "epoch": 1.9707017346671163,
      "grad_norm": 0.12272094190120697,
      "learning_rate": 5.880842125310926e-08,
      "loss": 0.0905,
      "step": 52600
    },
    {
      "epoch": 1.9710763927915775,
      "grad_norm": 0.09142860770225525,
      "learning_rate": 5.731616781439186e-08,
      "loss": 0.0929,
      "step": 52610
    },
    {
      "epoch": 1.971451050916039,
      "grad_norm": 0.10733555257320404,
      "learning_rate": 5.584308044271902e-08,
      "loss": 0.0873,
      "step": 52620
    },
    {
      "epoch": 1.9718257090405005,
      "grad_norm": 0.0804465040564537,
      "learning_rate": 5.4389159703416293e-08,
      "loss": 0.0885,
      "step": 52630
    },
    {
      "epoch": 1.972200367164962,
      "grad_norm": 0.09968402236700058,
      "learning_rate": 5.295440615443181e-08,
      "loss": 0.0915,
      "step": 52640
    },
    {
      "epoch": 1.9725750252894234,
      "grad_norm": 0.09420815110206604,
      "learning_rate": 5.1538820346369584e-08,
      "loss": 0.0919,
      "step": 52650
    },
    {
      "epoch": 1.9729496834138849,
      "grad_norm": 0.0910072848200798,
      "learning_rate": 5.014240282248395e-08,
      "loss": 0.0908,
      "step": 52660
    },
    {
      "epoch": 1.9733243415383463,
      "grad_norm": 0.10922705382108688,
      "learning_rate": 4.876515411865734e-08,
      "loss": 0.0891,
      "step": 52670
    },
    {
      "epoch": 1.9736989996628078,
      "grad_norm": 0.08070645481348038,
      "learning_rate": 4.7407074763428095e-08,
      "loss": 0.0866,
      "step": 52680
    },
    {
      "epoch": 1.974073657787269,
      "grad_norm": 0.0882679894566536,
      "learning_rate": 4.606816527797375e-08,
      "loss": 0.0882,
      "step": 52690
    },
    {
      "epoch": 1.9744483159117305,
      "grad_norm": 0.07770992070436478,
      "learning_rate": 4.4748426176116634e-08,
      "loss": 0.0866,
      "step": 52700
    },
    {
      "epoch": 1.974822974036192,
      "grad_norm": 0.12221865355968475,
      "learning_rate": 4.3447857964323827e-08,
      "loss": 0.0898,
      "step": 52710
    },
    {
      "epoch": 1.9751976321606532,
      "grad_norm": 0.090472012758255,
      "learning_rate": 4.2166461141701644e-08,
      "loss": 0.0907,
      "step": 52720
    },
    {
      "epoch": 1.9755722902851147,
      "grad_norm": 0.09773654490709305,
      "learning_rate": 4.090423619999562e-08,
      "loss": 0.0879,
      "step": 52730
    },
    {
      "epoch": 1.9759469484095762,
      "grad_norm": 0.08551633358001709,
      "learning_rate": 3.966118362360716e-08,
      "loss": 0.0893,
      "step": 52740
    },
    {
      "epoch": 1.9763216065340377,
      "grad_norm": 0.08387123793363571,
      "learning_rate": 3.843730388956024e-08,
      "loss": 0.0926,
      "step": 52750
    },
    {
      "epoch": 1.9766962646584991,
      "grad_norm": 0.11347624659538269,
      "learning_rate": 3.7232597467545816e-08,
      "loss": 0.0904,
      "step": 52760
    },
    {
      "epoch": 1.9770709227829606,
      "grad_norm": 0.08209285140037537,
      "learning_rate": 3.604706481987741e-08,
      "loss": 0.0912,
      "step": 52770
    },
    {
      "epoch": 1.977445580907422,
      "grad_norm": 0.11010267585515976,
      "learning_rate": 3.488070640150776e-08,
      "loss": 0.0888,
      "step": 52780
    },
    {
      "epoch": 1.9778202390318835,
      "grad_norm": 0.0911489948630333,
      "learning_rate": 3.373352266005658e-08,
      "loss": 0.0888,
      "step": 52790
    },
    {
      "epoch": 1.978194897156345,
      "grad_norm": 0.07607107609510422,
      "learning_rate": 3.260551403576062e-08,
      "loss": 0.0922,
      "step": 52800
    },
    {
      "epoch": 1.9785695552808062,
      "grad_norm": 0.11624424159526825,
      "learning_rate": 3.149668096150138e-08,
      "loss": 0.0904,
      "step": 52810
    },
    {
      "epoch": 1.9789442134052677,
      "grad_norm": 0.08884866535663605,
      "learning_rate": 3.040702386281069e-08,
      "loss": 0.0903,
      "step": 52820
    },
    {
      "epoch": 1.979318871529729,
      "grad_norm": 0.10002056509256363,
      "learning_rate": 2.9336543157859608e-08,
      "loss": 0.0891,
      "step": 52830
    },
    {
      "epoch": 1.9796935296541904,
      "grad_norm": 0.07994082570075989,
      "learning_rate": 2.8285239257452857e-08,
      "loss": 0.0847,
      "step": 52840
    },
    {
      "epoch": 1.980068187778652,
      "grad_norm": 0.08959349989891052,
      "learning_rate": 2.7253112565039928e-08,
      "loss": 0.0861,
      "step": 52850
    },
    {
      "epoch": 1.9804428459031134,
      "grad_norm": 0.10192622244358063,
      "learning_rate": 2.6240163476715097e-08,
      "loss": 0.0879,
      "step": 52860
    },
    {
      "epoch": 1.9808175040275748,
      "grad_norm": 0.11603207886219025,
      "learning_rate": 2.5246392381206297e-08,
      "loss": 0.0909,
      "step": 52870
    },
    {
      "epoch": 1.9811921621520363,
      "grad_norm": 0.12109042704105377,
      "learning_rate": 2.427179965988069e-08,
      "loss": 0.0937,
      "step": 52880
    },
    {
      "epoch": 1.9815668202764978,
      "grad_norm": 0.102958545088768,
      "learning_rate": 2.331638568675576e-08,
      "loss": 0.0895,
      "step": 52890
    },
    {
      "epoch": 1.9819414784009592,
      "grad_norm": 0.09715429693460464,
      "learning_rate": 2.238015082847711e-08,
      "loss": 0.0908,
      "step": 52900
    },
    {
      "epoch": 1.9823161365254207,
      "grad_norm": 0.13568194210529327,
      "learning_rate": 2.1463095444340665e-08,
      "loss": 0.0914,
      "step": 52910
    },
    {
      "epoch": 1.982690794649882,
      "grad_norm": 0.08712529391050339,
      "learning_rate": 2.056521988627047e-08,
      "loss": 0.0913,
      "step": 52920
    },
    {
      "epoch": 1.9830654527743434,
      "grad_norm": 0.12264542281627655,
      "learning_rate": 1.9686524498840897e-08,
      "loss": 0.0882,
      "step": 52930
    },
    {
      "epoch": 1.9834401108988047,
      "grad_norm": 0.10135191679000854,
      "learning_rate": 1.8827009619265533e-08,
      "loss": 0.089,
      "step": 52940
    },
    {
      "epoch": 1.9838147690232661,
      "grad_norm": 0.07880397886037827,
      "learning_rate": 1.798667557738054e-08,
      "loss": 0.0881,
      "step": 52950
    },
    {
      "epoch": 1.9841894271477276,
      "grad_norm": 0.09540349245071411,
      "learning_rate": 1.71655226956835e-08,
      "loss": 0.0891,
      "step": 52960
    },
    {
      "epoch": 1.984564085272189,
      "grad_norm": 0.0942855253815651,
      "learning_rate": 1.6363551289294567e-08,
      "loss": 0.0892,
      "step": 52970
    },
    {
      "epoch": 1.9849387433966506,
      "grad_norm": 0.09370411932468414,
      "learning_rate": 1.5580761665989762e-08,
      "loss": 0.0919,
      "step": 52980
    },
    {
      "epoch": 1.985313401521112,
      "grad_norm": 0.10413806885480881,
      "learning_rate": 1.4817154126156585e-08,
      "loss": 0.0895,
      "step": 52990
    },
    {
      "epoch": 1.9856880596455735,
      "grad_norm": 0.10027476400136948,
      "learning_rate": 1.4072728962855054e-08,
      "loss": 0.0906,
      "step": 53000
    }
  ],
  "logging_steps": 10,
  "max_steps": 53382,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.687539474012856e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
